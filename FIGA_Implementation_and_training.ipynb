{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0oWEkRU2KNcB"
   },
   "outputs": [],
   "source": [
    "home = \"/home/ubuntu\"\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import collections\n",
    "import torch.utils.data as data_utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/FIGA\n"
     ]
    }
   ],
   "source": [
    "%cd $home/FIGA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5UYhfrdXtg8",
    "outputId": "24f1c87c-ece0-489f-f4b4-e52ae18a99ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1nzxq-AglpaPuIN90JY3zTW_wf4CcZhJR\n",
      "To: /home/ubuntu/FIGA/dataset_full_new.csv\n",
      "100%|███████████████████████████████████████| 84.7M/84.7M [00:00<00:00, 131MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown 1nzxq-AglpaPuIN90JY3zTW_wf4CcZhJR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "SlOJXOmye75F",
    "outputId": "188a7623-d18e-4c66-f091-17425ef58fde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0a0+gitb488e78'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8xrOWUDRKySF"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear_layers = nn.Sequential(\n",
    "                        nn.Linear(111,50),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.2),\n",
    "                        nn.Linear(50,200),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.2),\n",
    "                        nn.Linear(200,100),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.4),\n",
    "                        nn.Linear(100,20),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.8),\n",
    "                        nn.Linear(20,10),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.4),\n",
    "                        nn.Linear(10,2)\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ubxcnPAPpd_g"
   },
   "outputs": [],
   "source": [
    "model = Net()\n",
    "#model.load_state_dict(torch.load(\"/content/drive/MyDrive/FIGA/model_FIGA_trained.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-IUUDcPnYNP"
   },
   "source": [
    "###Original Dataset and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "HYqPOpmkKTRi"
   },
   "outputs": [],
   "source": [
    "# Reading Original Dataset\n",
    "data = pd.read_csv(home+'/FIGA/dataset_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "6xqTwiloQIWx",
    "outputId": "d3c9329c-763a-40ea-b0c2-d0822b1fb0c1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-89153224-8675-4425-9398-0dd8bdf56fbe\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qty_dot_url</th>\n",
       "      <th>qty_hyphen_url</th>\n",
       "      <th>qty_underline_url</th>\n",
       "      <th>qty_slash_url</th>\n",
       "      <th>qty_questionmark_url</th>\n",
       "      <th>qty_equal_url</th>\n",
       "      <th>qty_at_url</th>\n",
       "      <th>qty_and_url</th>\n",
       "      <th>qty_exclamation_url</th>\n",
       "      <th>qty_space_url</th>\n",
       "      <th>qty_tilde_url</th>\n",
       "      <th>qty_comma_url</th>\n",
       "      <th>qty_plus_url</th>\n",
       "      <th>qty_asterisk_url</th>\n",
       "      <th>qty_hashtag_url</th>\n",
       "      <th>qty_dollar_url</th>\n",
       "      <th>qty_percent_url</th>\n",
       "      <th>qty_tld_url</th>\n",
       "      <th>length_url</th>\n",
       "      <th>qty_dot_domain</th>\n",
       "      <th>qty_hyphen_domain</th>\n",
       "      <th>qty_underline_domain</th>\n",
       "      <th>qty_slash_domain</th>\n",
       "      <th>qty_questionmark_domain</th>\n",
       "      <th>qty_equal_domain</th>\n",
       "      <th>qty_at_domain</th>\n",
       "      <th>qty_and_domain</th>\n",
       "      <th>qty_exclamation_domain</th>\n",
       "      <th>qty_space_domain</th>\n",
       "      <th>qty_tilde_domain</th>\n",
       "      <th>qty_comma_domain</th>\n",
       "      <th>qty_plus_domain</th>\n",
       "      <th>qty_asterisk_domain</th>\n",
       "      <th>qty_hashtag_domain</th>\n",
       "      <th>qty_dollar_domain</th>\n",
       "      <th>qty_percent_domain</th>\n",
       "      <th>qty_vowels_domain</th>\n",
       "      <th>domain_length</th>\n",
       "      <th>domain_in_ip</th>\n",
       "      <th>server_client_domain</th>\n",
       "      <th>...</th>\n",
       "      <th>qty_hashtag_file</th>\n",
       "      <th>qty_dollar_file</th>\n",
       "      <th>qty_percent_file</th>\n",
       "      <th>file_length</th>\n",
       "      <th>qty_dot_params</th>\n",
       "      <th>qty_hyphen_params</th>\n",
       "      <th>qty_underline_params</th>\n",
       "      <th>qty_slash_params</th>\n",
       "      <th>qty_questionmark_params</th>\n",
       "      <th>qty_equal_params</th>\n",
       "      <th>qty_at_params</th>\n",
       "      <th>qty_and_params</th>\n",
       "      <th>qty_exclamation_params</th>\n",
       "      <th>qty_space_params</th>\n",
       "      <th>qty_tilde_params</th>\n",
       "      <th>qty_comma_params</th>\n",
       "      <th>qty_plus_params</th>\n",
       "      <th>qty_asterisk_params</th>\n",
       "      <th>qty_hashtag_params</th>\n",
       "      <th>qty_dollar_params</th>\n",
       "      <th>qty_percent_params</th>\n",
       "      <th>params_length</th>\n",
       "      <th>tld_present_params</th>\n",
       "      <th>qty_params</th>\n",
       "      <th>email_in_url</th>\n",
       "      <th>time_response</th>\n",
       "      <th>domain_spf</th>\n",
       "      <th>asn_ip</th>\n",
       "      <th>time_domain_activation</th>\n",
       "      <th>time_domain_expiration</th>\n",
       "      <th>qty_ip_resolved</th>\n",
       "      <th>qty_nameservers</th>\n",
       "      <th>qty_mx_servers</th>\n",
       "      <th>ttl_hostname</th>\n",
       "      <th>tls_ssl_certificate</th>\n",
       "      <th>qty_redirects</th>\n",
       "      <th>url_google_index</th>\n",
       "      <th>domain_google_index</th>\n",
       "      <th>url_shortened</th>\n",
       "      <th>phishing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.564355</td>\n",
       "      <td>0</td>\n",
       "      <td>63949</td>\n",
       "      <td>6684</td>\n",
       "      <td>985</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>21599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.253577</td>\n",
       "      <td>0</td>\n",
       "      <td>395082</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10799</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.905697</td>\n",
       "      <td>0</td>\n",
       "      <td>28299</td>\n",
       "      <td>7548</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3591</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.508409</td>\n",
       "      <td>0</td>\n",
       "      <td>6939</td>\n",
       "      <td>7752</td>\n",
       "      <td>281</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>38389</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.537179</td>\n",
       "      <td>0</td>\n",
       "      <td>6939</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14361</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88642</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.309043</td>\n",
       "      <td>0</td>\n",
       "      <td>47583</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88643</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.258456</td>\n",
       "      <td>0</td>\n",
       "      <td>18650</td>\n",
       "      <td>6693</td>\n",
       "      <td>611</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1769</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88644</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.495230</td>\n",
       "      <td>0</td>\n",
       "      <td>16125</td>\n",
       "      <td>5650</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88645</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.415314</td>\n",
       "      <td>0</td>\n",
       "      <td>26496</td>\n",
       "      <td>4259</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>591</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88646</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.269567</td>\n",
       "      <td>1</td>\n",
       "      <td>13414</td>\n",
       "      <td>6830</td>\n",
       "      <td>474</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>1150</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88647 rows × 112 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89153224-8675-4425-9398-0dd8bdf56fbe')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-89153224-8675-4425-9398-0dd8bdf56fbe button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-89153224-8675-4425-9398-0dd8bdf56fbe');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       qty_dot_url  qty_hyphen_url  ...  url_shortened  phishing\n",
       "0                2               0  ...              0         0\n",
       "1                3               0  ...              0         1\n",
       "2                4               0  ...              0         0\n",
       "3                2               0  ...              0         0\n",
       "4                2               0  ...              0         0\n",
       "...            ...             ...  ...            ...       ...\n",
       "88642            3               1  ...              0         1\n",
       "88643            2               0  ...              0         0\n",
       "88644            2               0  ...              0         0\n",
       "88645            2               0  ...              0         0\n",
       "88646            2               0  ...              0         0\n",
       "\n",
       "[88647 rows x 112 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "A79kmIxssCHB",
    "outputId": "53a22590-1795-4ed2-fbfd-e45b9d2d3821"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-a0e55016-9662-436e-abbc-fd5a2819855c\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qty_dot_url</th>\n",
       "      <th>qty_hyphen_url</th>\n",
       "      <th>qty_underline_url</th>\n",
       "      <th>qty_slash_url</th>\n",
       "      <th>qty_questionmark_url</th>\n",
       "      <th>qty_equal_url</th>\n",
       "      <th>qty_at_url</th>\n",
       "      <th>qty_and_url</th>\n",
       "      <th>qty_exclamation_url</th>\n",
       "      <th>qty_space_url</th>\n",
       "      <th>qty_tilde_url</th>\n",
       "      <th>qty_comma_url</th>\n",
       "      <th>qty_plus_url</th>\n",
       "      <th>qty_asterisk_url</th>\n",
       "      <th>qty_hashtag_url</th>\n",
       "      <th>qty_dollar_url</th>\n",
       "      <th>qty_percent_url</th>\n",
       "      <th>qty_tld_url</th>\n",
       "      <th>length_url</th>\n",
       "      <th>qty_dot_domain</th>\n",
       "      <th>qty_hyphen_domain</th>\n",
       "      <th>qty_underline_domain</th>\n",
       "      <th>qty_slash_domain</th>\n",
       "      <th>qty_questionmark_domain</th>\n",
       "      <th>qty_equal_domain</th>\n",
       "      <th>qty_at_domain</th>\n",
       "      <th>qty_and_domain</th>\n",
       "      <th>qty_exclamation_domain</th>\n",
       "      <th>qty_space_domain</th>\n",
       "      <th>qty_tilde_domain</th>\n",
       "      <th>qty_comma_domain</th>\n",
       "      <th>qty_plus_domain</th>\n",
       "      <th>qty_asterisk_domain</th>\n",
       "      <th>qty_hashtag_domain</th>\n",
       "      <th>qty_dollar_domain</th>\n",
       "      <th>qty_percent_domain</th>\n",
       "      <th>qty_vowels_domain</th>\n",
       "      <th>domain_length</th>\n",
       "      <th>domain_in_ip</th>\n",
       "      <th>server_client_domain</th>\n",
       "      <th>...</th>\n",
       "      <th>qty_hashtag_file</th>\n",
       "      <th>qty_dollar_file</th>\n",
       "      <th>qty_percent_file</th>\n",
       "      <th>file_length</th>\n",
       "      <th>qty_dot_params</th>\n",
       "      <th>qty_hyphen_params</th>\n",
       "      <th>qty_underline_params</th>\n",
       "      <th>qty_slash_params</th>\n",
       "      <th>qty_questionmark_params</th>\n",
       "      <th>qty_equal_params</th>\n",
       "      <th>qty_at_params</th>\n",
       "      <th>qty_and_params</th>\n",
       "      <th>qty_exclamation_params</th>\n",
       "      <th>qty_space_params</th>\n",
       "      <th>qty_tilde_params</th>\n",
       "      <th>qty_comma_params</th>\n",
       "      <th>qty_plus_params</th>\n",
       "      <th>qty_asterisk_params</th>\n",
       "      <th>qty_hashtag_params</th>\n",
       "      <th>qty_dollar_params</th>\n",
       "      <th>qty_percent_params</th>\n",
       "      <th>params_length</th>\n",
       "      <th>tld_present_params</th>\n",
       "      <th>qty_params</th>\n",
       "      <th>email_in_url</th>\n",
       "      <th>time_response</th>\n",
       "      <th>domain_spf</th>\n",
       "      <th>asn_ip</th>\n",
       "      <th>time_domain_activation</th>\n",
       "      <th>time_domain_expiration</th>\n",
       "      <th>qty_ip_resolved</th>\n",
       "      <th>qty_nameservers</th>\n",
       "      <th>qty_mx_servers</th>\n",
       "      <th>ttl_hostname</th>\n",
       "      <th>tls_ssl_certificate</th>\n",
       "      <th>qty_redirects</th>\n",
       "      <th>url_google_index</th>\n",
       "      <th>domain_google_index</th>\n",
       "      <th>url_shortened</th>\n",
       "      <th>phishing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.154490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.035299</td>\n",
       "      <td>0.566314</td>\n",
       "      <td>2.301408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.862745</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.336116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.374763</td>\n",
       "      <td>-0.474315</td>\n",
       "      <td>-0.476056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.834761</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.271984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.383842</td>\n",
       "      <td>0.700809</td>\n",
       "      <td>-0.476056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.148677</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.238095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.605642</td>\n",
       "      <td>0.732565</td>\n",
       "      <td>0.318310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.460879</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.605642</td>\n",
       "      <td>-0.474315</td>\n",
       "      <td>-0.476056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.173805</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88642</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.248555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.277158</td>\n",
       "      <td>-0.474315</td>\n",
       "      <td>-0.476056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.188273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88643</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.328414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.063140</td>\n",
       "      <td>0.567715</td>\n",
       "      <td>1.247887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.024748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88644</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.095238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.180108</td>\n",
       "      <td>0.405355</td>\n",
       "      <td>0.070423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.136113</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88645</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.080792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300320</td>\n",
       "      <td>0.188823</td>\n",
       "      <td>-0.126761</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.136874</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88646</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.310873</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.305693</td>\n",
       "      <td>0.589041</td>\n",
       "      <td>0.861972</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.083666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88647 rows × 112 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0e55016-9662-436e-abbc-fd5a2819855c')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-a0e55016-9662-436e-abbc-fd5a2819855c button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-a0e55016-9662-436e-abbc-fd5a2819855c');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       qty_dot_url  qty_hyphen_url  ...  url_shortened  phishing\n",
       "0              0.0             0.0  ...            0.0       0.0\n",
       "1              1.0             0.0  ...            0.0       1.0\n",
       "2              2.0             0.0  ...            0.0       0.0\n",
       "3              0.0             0.0  ...            0.0       0.0\n",
       "4              0.0             0.0  ...            0.0       0.0\n",
       "...            ...             ...  ...            ...       ...\n",
       "88642          1.0             1.0  ...            0.0       1.0\n",
       "88643          0.0             0.0  ...            0.0       0.0\n",
       "88644          0.0             0.0  ...            0.0       0.0\n",
       "88645          0.0             0.0  ...            0.0       0.0\n",
       "88646          0.0             0.0  ...            0.0       0.0\n",
       "\n",
       "[88647 rows x 112 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scaling the data\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "VY424ItHn8dr"
   },
   "outputs": [],
   "source": [
    "#Changing Target label type from float(Caused by scaling) to int\n",
    "df_scaled['phishing'] = df_scaled['phishing'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "SyKLbfFHXrve"
   },
   "outputs": [],
   "source": [
    "train_data = df_scaled.iloc[:int(88647*0.8)]\n",
    "val_data = df_scaled.iloc[int(88647*0.8):int(88647*0.8)+int(11055*0.1)]\n",
    "test_data = df_scaled.iloc[int(88647*0.8)+int(88647*0.1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "pyjuD9pPQSxx"
   },
   "outputs": [],
   "source": [
    "#Creating Dataloaders\n",
    "train_target = torch.from_numpy(train_data['phishing'].values)\n",
    "train = torch.from_numpy(train_data.drop('phishing', axis = 1).values,).float()\n",
    "train_tensor = data_utils.TensorDataset(train, train_target)\n",
    "train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = 128, shuffle = True, pin_memory=True)\n",
    "\n",
    "train_target = torch.from_numpy(val_data['phishing'].values)\n",
    "train = torch.from_numpy(val_data.drop('phishing', axis = 1).values).float()\n",
    "train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "valid_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = 128, shuffle = True)\n",
    "\n",
    "train_target = torch.from_numpy(test_data['phishing'].values)\n",
    "train = torch.from_numpy(test_data.drop('phishing', axis = 1).values).float()\n",
    "train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "test_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = 128, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Nvm8FnhcXthE"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"hpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kudJmmDKSQ69",
    "outputId": "e6e2c595-1a56-49d4-a230-39df72443092"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear_layers): Sequential(\n",
       "    (0): Linear(in_features=111, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=50, out_features=200, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.4, inplace=False)\n",
       "    (9): Linear(in_features=100, out_features=20, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.8, inplace=False)\n",
       "    (12): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Dropout(p=0.4, inplace=False)\n",
       "    (15): Linear(in_features=10, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "H35ItY78XthF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Habana modules from /home/ubuntu/.local/lib/python3.8/site-packages/habana_frameworks/torch/lib\n"
     ]
    }
   ],
   "source": [
    "from habana_frameworks.torch.utils.library_loader import load_habana_module\n",
    "import habana_frameworks.torch.core as htcore\n",
    "load_habana_module()\n",
    "def permute_params(model, to_filters_last, lazy_mode):\n",
    "    import habana_frameworks.torch.core as htcore\n",
    "    if htcore.is_enabled_weight_permute_pass() is True:\n",
    "        return\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if(param.ndim == 4):\n",
    "                if to_filters_last:\n",
    "                    param.data = param.data.permute((2, 3, 1, 0))\n",
    "                else:\n",
    "                    param.data = param.data.permute((3, 2, 0, 1))  # permute RSCK to KCRS\n",
    "    if lazy_mode:\n",
    "        import habana_frameworks.torch.core as htcore\n",
    "        htcore.mark_step()\n",
    "\n",
    "def permute_momentum(optimizer, to_filters_last, lazy_mode):\n",
    "    # Permute the momentum buffer before using for checkpoint\n",
    "    import habana_frameworks.torch.core as htcore\n",
    "    if htcore.is_enabled_weight_permute_pass() is True:\n",
    "        return\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            param_state = optimizer.state[p]\n",
    "            if 'momentum_buffer' in param_state:\n",
    "                buf = param_state['momentum_buffer']\n",
    "                if(buf.ndim == 4):\n",
    "                    if to_filters_last:\n",
    "                        buf = buf.permute((2,3,1,0))\n",
    "                    else:\n",
    "                        buf = buf.permute((3,2,0,1))\n",
    "                    param_state['momentum_buffer'] = buf\n",
    "\n",
    "    if lazy_mode:\n",
    "        import habana_frameworks.torch.core as htcore\n",
    "        htcore.mark_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mf5gnjlPS6uj"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer  = optim.SGD(model.parameters(),lr = 0.1)\n",
    "scheduler = StepLR(optimizer, step_size=8, gamma=0.35) #Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsG_ReypEBSh",
    "outputId": "edd842c4-1bc6-491d-c29c-fc28853d1021"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load previous model if any\n",
    "# model.load_state_dict(torch.load(home+\"/FIGA/model_normal.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "permute_params(model, True, True)\n",
    "permute_momentum(optimizer, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "gno0yiz1SxCE",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e7481f23-a55d-4f70-845c-7750ce524a06",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40.. Train loss: 0.004.. Validation loss: 0.367.. Validation accuracy: 0.860.. LR : [0.1]\n",
      "valid loss decreased (inf --> 0.367133).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.003.. Validation loss: 0.266.. Validation accuracy: 0.890.. LR : [0.1]\n",
      "valid loss decreased (0.367133 --> 0.266193).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.003.. Validation loss: 0.241.. Validation accuracy: 0.904.. LR : [0.1]\n",
      "valid loss decreased (0.266193 --> 0.241066).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.002.. Validation loss: 0.217.. Validation accuracy: 0.913.. LR : [0.1]\n",
      "valid loss decreased (0.241066 --> 0.217287).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.002.. Validation loss: 0.219.. Validation accuracy: 0.916.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.005.. Validation loss: 0.232.. Validation accuracy: 0.911.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.002.. Validation loss: 0.203.. Validation accuracy: 0.921.. LR : [0.1]\n",
      "valid loss decreased (0.217287 --> 0.203286).  Saving model ...\n",
      "Epoch 2/40.. Train loss: 0.002.. Validation loss: 0.198.. Validation accuracy: 0.920.. LR : [0.1]\n",
      "valid loss decreased (0.203286 --> 0.197637).  Saving model ...\n",
      "Epoch 2/40.. Train loss: 0.002.. Validation loss: 0.186.. Validation accuracy: 0.927.. LR : [0.1]\n",
      "valid loss decreased (0.197637 --> 0.185849).  Saving model ...\n",
      "Epoch 2/40.. Train loss: 0.002.. Validation loss: 0.214.. Validation accuracy: 0.911.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.001.. Validation loss: 0.232.. Validation accuracy: 0.919.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.003.. Validation loss: 0.223.. Validation accuracy: 0.926.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.002.. Validation loss: 0.179.. Validation accuracy: 0.932.. LR : [0.1]\n",
      "valid loss decreased (0.185849 --> 0.178712).  Saving model ...\n",
      "Epoch 3/40.. Train loss: 0.002.. Validation loss: 0.179.. Validation accuracy: 0.926.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.001.. Validation loss: 0.174.. Validation accuracy: 0.932.. LR : [0.1]\n",
      "valid loss decreased (0.178712 --> 0.173825).  Saving model ...\n",
      "Epoch 3/40.. Train loss: 0.001.. Validation loss: 0.190.. Validation accuracy: 0.924.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.005.. Validation loss: 0.195.. Validation accuracy: 0.913.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.002.. Validation loss: 0.160.. Validation accuracy: 0.938.. LR : [0.1]\n",
      "valid loss decreased (0.173825 --> 0.160142).  Saving model ...\n",
      "Epoch 4/40.. Train loss: 0.002.. Validation loss: 0.160.. Validation accuracy: 0.943.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.002.. Validation loss: 0.163.. Validation accuracy: 0.935.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.001.. Validation loss: 0.156.. Validation accuracy: 0.938.. LR : [0.1]\n",
      "valid loss decreased (0.160142 --> 0.156047).  Saving model ...\n",
      "Epoch 4/40.. Train loss: 0.001.. Validation loss: 0.200.. Validation accuracy: 0.919.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.003.. Validation loss: 0.256.. Validation accuracy: 0.901.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.001.. Validation loss: 0.162.. Validation accuracy: 0.938.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.002.. Validation loss: 0.170.. Validation accuracy: 0.934.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.002.. Validation loss: 0.213.. Validation accuracy: 0.920.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.001.. Validation loss: 0.190.. Validation accuracy: 0.927.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.003.. Validation loss: 0.177.. Validation accuracy: 0.936.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.001.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.002.. Validation loss: 0.158.. Validation accuracy: 0.941.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.002.. Validation loss: 0.144.. Validation accuracy: 0.947.. LR : [0.1]\n",
      "valid loss decreased (0.156047 --> 0.144450).  Saving model ...\n",
      "Epoch 6/40.. Train loss: 0.002.. Validation loss: 0.179.. Validation accuracy: 0.924.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.002.. Validation loss: 0.172.. Validation accuracy: 0.932.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.003.. Validation loss: 0.265.. Validation accuracy: 0.918.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.001.. Validation loss: 0.152.. Validation accuracy: 0.946.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.003.. Validation loss: 0.153.. Validation accuracy: 0.937.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.001.. Validation loss: 0.150.. Validation accuracy: 0.941.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.001.. Validation loss: 0.158.. Validation accuracy: 0.937.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.003.. Validation loss: 0.170.. Validation accuracy: 0.940.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.002.. Validation loss: 0.150.. Validation accuracy: 0.943.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.001.. Validation loss: 0.139.. Validation accuracy: 0.944.. LR : [0.1]\n",
      "valid loss decreased (0.144450 --> 0.138681).  Saving model ...\n",
      "Epoch 8/40.. Train loss: 0.002.. Validation loss: 0.167.. Validation accuracy: 0.937.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.001.. Validation loss: 0.151.. Validation accuracy: 0.943.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.001.. Validation loss: 0.141.. Validation accuracy: 0.944.. LR : [0.1]\n",
      "Epoch 9/40.. Train loss: 0.002.. Validation loss: 0.164.. Validation accuracy: 0.942.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.001.. Validation loss: 0.138.. Validation accuracy: 0.948.. LR : [0.012249999999999999]\n",
      "valid loss decreased (0.138681 --> 0.138329).  Saving model ...\n",
      "Epoch 9/40.. Train loss: 0.002.. Validation loss: 0.144.. Validation accuracy: 0.944.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.001.. Validation loss: 0.143.. Validation accuracy: 0.940.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.001.. Validation loss: 0.140.. Validation accuracy: 0.947.. LR : [0.012249999999999999]\n",
      "Epoch 10/40.. Train loss: 0.003.. Validation loss: 0.146.. Validation accuracy: 0.948.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.001.. Validation loss: 0.134.. Validation accuracy: 0.948.. LR : [0.034999999999999996]\n",
      "valid loss decreased (0.138329 --> 0.134088).  Saving model ...\n",
      "Epoch 10/40.. Train loss: 0.002.. Validation loss: 0.135.. Validation accuracy: 0.947.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.001.. Validation loss: 0.139.. Validation accuracy: 0.950.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.002.. Validation loss: 0.137.. Validation accuracy: 0.949.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.002.. Validation loss: 0.140.. Validation accuracy: 0.943.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.003.. Validation loss: 0.161.. Validation accuracy: 0.946.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.001.. Validation loss: 0.151.. Validation accuracy: 0.942.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.002.. Validation loss: 0.133.. Validation accuracy: 0.949.. LR : [0.034999999999999996]\n",
      "valid loss decreased (0.134088 --> 0.133362).  Saving model ...\n",
      "Epoch 11/40.. Train loss: 0.002.. Validation loss: 0.132.. Validation accuracy: 0.948.. LR : [0.034999999999999996]\n",
      "valid loss decreased (0.133362 --> 0.131530).  Saving model ...\n",
      "Epoch 11/40.. Train loss: 0.001.. Validation loss: 0.139.. Validation accuracy: 0.951.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.001.. Validation loss: 0.137.. Validation accuracy: 0.948.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.003.. Validation loss: 0.174.. Validation accuracy: 0.941.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.001.. Validation loss: 0.141.. Validation accuracy: 0.943.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.001.. Validation loss: 0.137.. Validation accuracy: 0.943.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.001.. Validation loss: 0.139.. Validation accuracy: 0.952.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.000.. Validation loss: 0.132.. Validation accuracy: 0.951.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.003.. Validation loss: 0.168.. Validation accuracy: 0.943.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.001.. Validation loss: 0.137.. Validation accuracy: 0.950.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.001.. Validation loss: 0.141.. Validation accuracy: 0.945.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.001.. Validation loss: 0.136.. Validation accuracy: 0.950.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.001.. Validation loss: 0.134.. Validation accuracy: 0.947.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.001.. Validation loss: 0.130.. Validation accuracy: 0.952.. LR : [0.034999999999999996]\n",
      "valid loss decreased (0.131530 --> 0.129893).  Saving model ...\n",
      "Epoch 14/40.. Train loss: 0.002.. Validation loss: 0.150.. Validation accuracy: 0.946.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.001.. Validation loss: 0.138.. Validation accuracy: 0.949.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.001.. Validation loss: 0.143.. Validation accuracy: 0.947.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.001.. Validation loss: 0.143.. Validation accuracy: 0.944.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.001.. Validation loss: 0.133.. Validation accuracy: 0.951.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.002.. Validation loss: 0.178.. Validation accuracy: 0.932.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.001.. Validation loss: 0.141.. Validation accuracy: 0.945.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.001.. Validation loss: 0.135.. Validation accuracy: 0.952.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.002.. Validation loss: 0.133.. Validation accuracy: 0.946.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.001.. Validation loss: 0.137.. Validation accuracy: 0.952.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.001.. Validation loss: 0.134.. Validation accuracy: 0.945.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.002.. Validation loss: 0.159.. Validation accuracy: 0.949.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.002.. Validation loss: 0.131.. Validation accuracy: 0.952.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.001.. Validation loss: 0.134.. Validation accuracy: 0.952.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.001.. Validation loss: 0.139.. Validation accuracy: 0.944.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.002.. Validation loss: 0.139.. Validation accuracy: 0.948.. LR : [0.034999999999999996]\n",
      "Epoch 17/40.. Train loss: 0.003.. Validation loss: 0.133.. Validation accuracy: 0.953.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.001.. Validation loss: 0.133.. Validation accuracy: 0.953.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.001.. Validation loss: 0.131.. Validation accuracy: 0.949.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.001.. Validation loss: 0.138.. Validation accuracy: 0.945.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.001.. Validation loss: 0.130.. Validation accuracy: 0.952.. LR : [0.0042875]\n",
      "valid loss decreased (0.129893 --> 0.129787).  Saving model ...\n",
      "Epoch 17/40.. Train loss: 0.001.. Validation loss: 0.131.. Validation accuracy: 0.951.. LR : [0.0042875]\n",
      "Epoch 18/40.. Train loss: 0.003.. Validation loss: 0.155.. Validation accuracy: 0.946.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.001.. Validation loss: 0.132.. Validation accuracy: 0.951.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.001.. Validation loss: 0.134.. Validation accuracy: 0.948.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.001.. Validation loss: 0.129.. Validation accuracy: 0.950.. LR : [0.012249999999999999]\n",
      "valid loss decreased (0.129787 --> 0.129412).  Saving model ...\n",
      "Epoch 18/40.. Train loss: 0.002.. Validation loss: 0.131.. Validation accuracy: 0.950.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.001.. Validation loss: 0.134.. Validation accuracy: 0.953.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.001.. Validation loss: 0.132.. Validation accuracy: 0.952.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.001.. Validation loss: 0.131.. Validation accuracy: 0.949.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.001.. Validation loss: 0.133.. Validation accuracy: 0.949.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.001.. Validation loss: 0.134.. Validation accuracy: 0.947.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.001.. Validation loss: 0.128.. Validation accuracy: 0.951.. LR : [0.012249999999999999]\n",
      "valid loss decreased (0.129412 --> 0.128492).  Saving model ...\n",
      "Epoch 20/40.. Train loss: 0.002.. Validation loss: 0.150.. Validation accuracy: 0.947.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.001.. Validation loss: 0.133.. Validation accuracy: 0.947.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.001.. Validation loss: 0.131.. Validation accuracy: 0.952.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.001.. Validation loss: 0.129.. Validation accuracy: 0.952.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.001.. Validation loss: 0.130.. Validation accuracy: 0.947.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.000.. Validation loss: 0.129.. Validation accuracy: 0.951.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.003.. Validation loss: 0.152.. Validation accuracy: 0.949.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.001.. Validation loss: 0.134.. Validation accuracy: 0.952.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.001.. Validation loss: 0.132.. Validation accuracy: 0.950.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.000.. Validation loss: 0.129.. Validation accuracy: 0.949.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.001.. Validation loss: 0.132.. Validation accuracy: 0.946.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.004.. Validation loss: 0.150.. Validation accuracy: 0.946.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.001.. Validation loss: 0.130.. Validation accuracy: 0.948.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.001.. Validation loss: 0.126.. Validation accuracy: 0.953.. LR : [0.012249999999999999]\n",
      "valid loss decreased (0.128492 --> 0.126069).  Saving model ...\n",
      "Epoch 22/40.. Train loss: 0.001.. Validation loss: 0.135.. Validation accuracy: 0.945.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.001.. Validation loss: 0.128.. Validation accuracy: 0.951.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.001.. Validation loss: 0.132.. Validation accuracy: 0.952.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.002.. Validation loss: 0.144.. Validation accuracy: 0.950.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.001.. Validation loss: 0.134.. Validation accuracy: 0.951.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.001.. Validation loss: 0.131.. Validation accuracy: 0.953.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.001.. Validation loss: 0.134.. Validation accuracy: 0.950.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.001.. Validation loss: 0.132.. Validation accuracy: 0.949.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.003.. Validation loss: 0.144.. Validation accuracy: 0.946.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.001.. Validation loss: 0.130.. Validation accuracy: 0.953.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.001.. Validation loss: 0.127.. Validation accuracy: 0.951.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.001.. Validation loss: 0.130.. Validation accuracy: 0.949.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.001.. Validation loss: 0.128.. Validation accuracy: 0.949.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.001.. Validation loss: 0.130.. Validation accuracy: 0.949.. LR : [0.012249999999999999]\n",
      "Epoch 25/40.. Train loss: 0.002.. Validation loss: 0.140.. Validation accuracy: 0.952.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.001.. Validation loss: 0.130.. Validation accuracy: 0.950.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.001.. Validation loss: 0.134.. Validation accuracy: 0.950.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.001.. Validation loss: 0.131.. Validation accuracy: 0.949.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.001.. Validation loss: 0.130.. Validation accuracy: 0.951.. LR : [0.0015006249999999998]\n",
      "Epoch 26/40.. Train loss: 0.002.. Validation loss: 0.135.. Validation accuracy: 0.949.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.001.. Validation loss: 0.127.. Validation accuracy: 0.950.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.001.. Validation loss: 0.131.. Validation accuracy: 0.947.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.001.. Validation loss: 0.127.. Validation accuracy: 0.949.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.001.. Validation loss: 0.132.. Validation accuracy: 0.947.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.001.. Validation loss: 0.128.. Validation accuracy: 0.950.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.006.. Validation loss: 0.142.. Validation accuracy: 0.947.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.001.. Validation loss: 0.129.. Validation accuracy: 0.951.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.002.. Validation loss: 0.129.. Validation accuracy: 0.950.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.001.. Validation loss: 0.131.. Validation accuracy: 0.946.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.001.. Validation loss: 0.131.. Validation accuracy: 0.949.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.005.. Validation loss: 0.136.. Validation accuracy: 0.953.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.001.. Validation loss: 0.130.. Validation accuracy: 0.949.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.000.. Validation loss: 0.132.. Validation accuracy: 0.949.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.001.. Validation loss: 0.128.. Validation accuracy: 0.948.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.001.. Validation loss: 0.127.. Validation accuracy: 0.949.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.001.. Validation loss: 0.126.. Validation accuracy: 0.950.. LR : [0.0042875]\n",
      "valid loss decreased (0.126069 --> 0.125899).  Saving model ...\n",
      "Epoch 29/40.. Train loss: 0.004.. Validation loss: 0.139.. Validation accuracy: 0.949.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.001.. Validation loss: 0.130.. Validation accuracy: 0.949.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.001.. Validation loss: 0.130.. Validation accuracy: 0.950.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.001.. Validation loss: 0.127.. Validation accuracy: 0.950.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.001.. Validation loss: 0.128.. Validation accuracy: 0.950.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.004.. Validation loss: 0.128.. Validation accuracy: 0.950.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.001.. Validation loss: 0.127.. Validation accuracy: 0.949.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.002.. Validation loss: 0.132.. Validation accuracy: 0.947.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.001.. Validation loss: 0.128.. Validation accuracy: 0.949.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.001.. Validation loss: 0.126.. Validation accuracy: 0.951.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.001.. Validation loss: 0.128.. Validation accuracy: 0.945.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.002.. Validation loss: 0.138.. Validation accuracy: 0.956.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.001.. Validation loss: 0.129.. Validation accuracy: 0.950.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.001.. Validation loss: 0.132.. Validation accuracy: 0.948.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.001.. Validation loss: 0.126.. Validation accuracy: 0.951.. LR : [0.0042875]\n",
      "valid loss decreased (0.125899 --> 0.125888).  Saving model ...\n",
      "Epoch 31/40.. Train loss: 0.001.. Validation loss: 0.133.. Validation accuracy: 0.946.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.001.. Validation loss: 0.127.. Validation accuracy: 0.949.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.003.. Validation loss: 0.140.. Validation accuracy: 0.952.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.001.. Validation loss: 0.129.. Validation accuracy: 0.950.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.001.. Validation loss: 0.130.. Validation accuracy: 0.951.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.001.. Validation loss: 0.128.. Validation accuracy: 0.948.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.002.. Validation loss: 0.128.. Validation accuracy: 0.951.. LR : [0.0042875]\n",
      "Epoch 33/40.. Train loss: 0.003.. Validation loss: 0.131.. Validation accuracy: 0.950.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.001.. Validation loss: 0.127.. Validation accuracy: 0.951.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.001.. Validation loss: 0.131.. Validation accuracy: 0.949.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.001.. Validation loss: 0.127.. Validation accuracy: 0.949.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.001.. Validation loss: 0.127.. Validation accuracy: 0.950.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.001.. Validation loss: 0.127.. Validation accuracy: 0.950.. LR : [0.0005252187499999999]\n",
      "Epoch 34/40.. Train loss: 0.003.. Validation loss: 0.136.. Validation accuracy: 0.952.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.001.. Validation loss: 0.128.. Validation accuracy: 0.951.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.002.. Validation loss: 0.131.. Validation accuracy: 0.949.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.001.. Validation loss: 0.126.. Validation accuracy: 0.951.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.001.. Validation loss: 0.132.. Validation accuracy: 0.945.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.004.. Validation loss: 0.126.. Validation accuracy: 0.956.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.002.. Validation loss: 0.130.. Validation accuracy: 0.947.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.001.. Validation loss: 0.125.. Validation accuracy: 0.953.. LR : [0.0015006249999999998]\n",
      "valid loss decreased (0.125888 --> 0.124928).  Saving model ...\n",
      "Epoch 35/40.. Train loss: 0.001.. Validation loss: 0.125.. Validation accuracy: 0.950.. LR : [0.0015006249999999998]\n",
      "valid loss decreased (0.124928 --> 0.124782).  Saving model ...\n",
      "Epoch 35/40.. Train loss: 0.002.. Validation loss: 0.125.. Validation accuracy: 0.950.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.001.. Validation loss: 0.126.. Validation accuracy: 0.949.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.002.. Validation loss: 0.131.. Validation accuracy: 0.954.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.001.. Validation loss: 0.132.. Validation accuracy: 0.949.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.001.. Validation loss: 0.130.. Validation accuracy: 0.951.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.001.. Validation loss: 0.128.. Validation accuracy: 0.951.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.001.. Validation loss: 0.128.. Validation accuracy: 0.948.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.002.. Validation loss: 0.129.. Validation accuracy: 0.947.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.001.. Validation loss: 0.130.. Validation accuracy: 0.948.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.001.. Validation loss: 0.126.. Validation accuracy: 0.950.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.001.. Validation loss: 0.125.. Validation accuracy: 0.950.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.001.. Validation loss: 0.127.. Validation accuracy: 0.950.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.001.. Validation loss: 0.123.. Validation accuracy: 0.952.. LR : [0.0015006249999999998]\n",
      "valid loss decreased (0.124782 --> 0.123258).  Saving model ...\n",
      "Epoch 38/40.. Train loss: 0.003.. Validation loss: 0.132.. Validation accuracy: 0.952.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.001.. Validation loss: 0.128.. Validation accuracy: 0.949.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.001.. Validation loss: 0.131.. Validation accuracy: 0.947.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.001.. Validation loss: 0.128.. Validation accuracy: 0.951.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.001.. Validation loss: 0.127.. Validation accuracy: 0.950.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.002.. Validation loss: 0.128.. Validation accuracy: 0.951.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.002.. Validation loss: 0.132.. Validation accuracy: 0.946.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.001.. Validation loss: 0.126.. Validation accuracy: 0.951.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.001.. Validation loss: 0.126.. Validation accuracy: 0.951.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.001.. Validation loss: 0.126.. Validation accuracy: 0.950.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.000.. Validation loss: 0.129.. Validation accuracy: 0.949.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.003.. Validation loss: 0.134.. Validation accuracy: 0.954.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.001.. Validation loss: 0.127.. Validation accuracy: 0.950.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.000.. Validation loss: 0.130.. Validation accuracy: 0.948.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.001.. Validation loss: 0.125.. Validation accuracy: 0.951.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.001.. Validation loss: 0.125.. Validation accuracy: 0.952.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.001.. Validation loss: 0.126.. Validation accuracy: 0.948.. LR : [0.0015006249999999998]\n"
     ]
    }
   ],
   "source": [
    "#TRAINING\n",
    "epochs = 40 # Number of epochs\n",
    "steps = 0\n",
    "print_every = 100\n",
    "train_losses, valid_losses,valid_acc = [], [], [] # List keeping track of losses and accuracy to plot later\n",
    "valid_loss_min = np.Inf # It will be used to save model whenever Vallidation loss decreases\n",
    "valid_acc_min = 0.0 \n",
    "\n",
    "for e in range(epochs):\n",
    "  \n",
    "  train_loss = 0 \n",
    "  model.train()\n",
    "  #train the model\n",
    "  for data, labels in train_loader:\n",
    "    steps+=1\n",
    "    # Move tensor to device('cuda' in case of GPU or 'cpu' in case of CPU)\n",
    "    data, labels = data.to(device), labels.to(device)\n",
    "    # Clearing all the previous gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Forward Pass\n",
    "    logits = model(data)\n",
    "    # Loss calculation\n",
    "    loss = criterion(logits,labels)\n",
    "    # Backward Pass\n",
    "    loss.backward()\n",
    "    # Update the parameters\n",
    "    htcore.mark_step()\n",
    "    optimizer.step()\n",
    "    #htcore.mark_step()\n",
    "    # Updating the losses list\n",
    "    train_loss += loss.item()\n",
    "\n",
    "    # Evaluating after specific amount of steps\n",
    "    if steps % print_every == 0:\n",
    "      valid_loss = 0\n",
    "      accuracy = 0\n",
    "      # Setting Model to Evaluation Mode\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        # Getting Validation loss\n",
    "        for data, labels in valid_loader:\n",
    "          data, labels = data.to(device), labels.to(device)\n",
    "          logits = model(data)\n",
    "          batch_loss = criterion(logits,labels)\n",
    "          valid_loss += batch_loss.item()\n",
    "          \n",
    "          # Calculating Accuracy\n",
    "          output = F.softmax(logits,dim=1)\n",
    "          top_p,top_class = output.topk(1,dim = 1)\n",
    "          equals = top_class == labels.view(*top_class.shape)\n",
    "          accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "      # Printing stats    \n",
    "      print(f\"Epoch {e+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {train_loss/print_every:.3f}.. \"\n",
    "                  f\"Validation loss: {valid_loss/len(valid_loader):.3f}.. \"\n",
    "                  f\"Validation accuracy: {accuracy/len(valid_loader):.3f}.. \"\n",
    "                  f\"LR : {scheduler.get_lr():}\"\n",
    "                  )\n",
    "      valid_loss = valid_loss/len(valid_loader)\n",
    "      train_losses.append(train_loss/print_every)\n",
    "      valid_losses.append(valid_loss)\n",
    "      valid_acc.append(accuracy/len(valid_loader))\n",
    "      \n",
    "      # Checking if Validation loss decreased\n",
    "      if valid_loss <= valid_loss_min:\n",
    "        \n",
    "        # if decreased, it will save the model\n",
    "        print('valid loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.cpu().state_dict(), 'model.pt')\n",
    "        model.to(device)\n",
    "        valid_loss_min = valid_loss\n",
    "      \n",
    "    train_loss = 0    \n",
    "  # Scheduler performing a step to change learning rate of Optimizer    \n",
    "  scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "Lt3xS4MjhubC",
    "outputId": "be464ace-2894-4345-a1be-e9f8c3b3fe9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f22d8af76d0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvUAAAHwCAYAAAAmZ5CjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxU9b3/8dc3CQkhQABZZVUQEFdARMEKYnGjqFfcrl4VrVWvtYqttirtBa2t0F4t2qr1uoDaiqL9CS5VwQou4BrUuqCyI7uABAIEsnx/f5yZzDkzZ2ZOkklmJryfj0cec+as30Ru72c+8/l+vsZai4iIiIiIZK+cdA9ARERERETqR0G9iIiIiEiWU1AvIiIiIpLlFNSLiIiIiGQ5BfUiIiIiIllOQb2IiIiISJZTUC8iIiIikuUU1IuIiIiIZDkF9SIiIiIiWU5BvYiIiIhIllNQLyIiIiKS5RTUi4iIiIhkubx0DyDTGWNWAq2BVWkeioiIiIg0bb2AHdbag2p7oYL65FoXFha2O/TQQ9uleyAiIiIi0nQtWbKEPXv21OlaBfXJrTr00EPblZSUpHscIiIiItKEDR48mMWLF6+qy7WqqRcRERERyXIK6kVEREREspyCehERERGRLKegXkREREQkyymoFxERERHJcgrqRURERESynIJ6EREREZEsp6BeRERERCTLKagXEREREclyCupFRERERLKcgnoRERERkSynoF5EREREJMspqBcRERERyXIK6kVEREREspyCehERERGRLKegPpNVV0HFHudVRERERCQOBfWZ6qET4Y528LvOsPnLdI9GREREGpgxhpEjR9b7PiNHjsQYU/8BpdCMGTMwxjBjxox0D6XJUlCfqUxuZLtyX/rGISIisp8wxtTqRwGqZJK8dA9A4sjNj2xXKagXERFpaJMmTYrZN23aNEpLS7nhhhto06aN59jRRx+d0ucvWbKEFi1a1Ps+TzzxBLt3707BiCSbKKjPVHnuoH5v+sYhIiKyn5g8eXLMvhkzZlBaWsqECRPo1atXgz6/f//+KblPjx49UnIfyS4qv8lUnkx9RfrGISIiIjHCdev79u3jjjvuoF+/fhQUFDB+/HgASktL+eMf/8ioUaPo1q0b+fn5dOjQgTPPPJN3333X955+NfWTJ0/GGMOCBQt47rnnOPbYY2nRogXt2rXjwgsvZN26dXHH5rZgwQKMMUyePJlPPvmEMWPG0KZNG1q0aMGIESNYtGiR75g2bNjA5ZdfTseOHSksLOToo4/m8ccf99yvvkpKShg3bhwdO3akoKCAnj17cu2117Jhw4aYczdt2sRNN91Ev379KCoqok2bNvTr14/x48ezYsWKmvOstTz++OMMGzaMDh060Lx5c7p3786pp57KM888U+8xZyJl6jNVbkFku1KZehERkUw0btw4PvzwQ04//XTOPvtsOnbsCDilNBMnTuTEE09kzJgxtG3bljVr1vDCCy/wyiuv8OKLL3LaaacFfs4DDzzACy+8wJlnnsmIESN4//33eeaZZ/j000/55JNPKCgoSH4T4KOPPuIPf/gDxx9/PFdeeSVr1qzhH//4ByeffDKffPIJ/fr1qzl38+bNHH/88axevZoTTzyRYcOGsXHjRq699lpOOeWU2v2h4njppZcYN24c1lrOPfdcevbsSUlJCQ8++CBz5szhnXfe4aCDDgJg9+7dDB8+nOXLlzN69GjGjh2LtZbVq1czZ84czj33XA4++GAAJk6cyF133cVBBx3E+eefT3FxMRs2bODDDz/k2Wef5YILLkjJ+DOJgvpMldsssq2aehERkYy0evVqPv/8c9q3b+/Zf+ihh7J+/fqY/WvXruXYY4/lxhtvrFVQ/+qrr/Lhhx9yxBFH1Oy76KKLmDlzJnPmzOH8888PdJ+XX36Z6dOn13yjAPDQQw9xzTXXcO+99/LAAw/U7L/11ltZvXo1v/zlL5k6dWrN/gkTJnDssccGHns8ZWVlXHbZZVRWVrJgwQJ+8IMf1BybOnUqt9xyC1dffTVz584F4F//+hfLly9nwoQJ/OlPf/Lca9++fezdG0mCPvTQQ3Tt2pXPP/88Zp7Cli1b6j32TKSgPlPluT5xK6gXEZE063XLy+keQmCrpoxptGf99re/jQncAYqLi33P79atG+eeey5//vOfWbNmTeD69+uvv94T0AP85Cc/YebMmXzwwQeBg/rhw4d7AnqAK664guuuu44PPvigZt++ffuYOXMmxcXF/PrXv/acf9RRR3HppZfyyCOPBHpmPHPmzGHbtm3853/+pyegB/jFL37BX//6V+bNmxfzdyosLIy5V35+Pvn5+Z59zZo1Izc3N+Zcv/9eTYFq6jOVMvUiIiIZL1HGeuHChZx//vl0796dgoKCmlaYf/7znwF86+HjOeaYY2L2de/eHYDvv/++Xvdp1qwZnTp18tzn66+/Zs+ePRx55JG0atUq5poTTjgh8DPjWbx4MQCjRo2KOZaXl8eJJ54IwMcffwzAiBEj6Nq1K1OmTOG0007jvvvuo6SkhKqq2EU6L774YlatWsWAAQO49dZbefXVVyktLa33mDOZMvWZSjX1IiIiGa9z586++59//nnOPfdcmjdvzujRo+nduzdFRUXk5OSwYMEC3nzzTU+5SDLR7TTBCXwB36C2NvcJ38t9n3AA3KlTJ9/z4+2vjfAzunTp4ns8vH/79u0AtG7dmvfee49Jkybxwgsv8NprrwFO5v3aa6/l17/+Nc2aOUnRP/3pTxx88MFMnz6dKVOmMGXKFPLy8jjjjDO4++676dOnT73Hn2kU1Gcqdb8REZEM0pglLdkk3sqtv/nNb8jPz+ejjz7i0EMP9Ry7+uqrefPNNxtjeHXWunVrwOk24yfe/toIlyht3LjR93i4+427lKlbt248+uijWGv58ssveeONN7j//vu54447qK6u5re//S0Aubm5TJgwgQkTJrB582beeecdnn76aZ599lm++OILvvjii8CTi7OFym8ylfrUi4iIZK1ly5YxYMCAmIC+urqad955J02jCq5///4UFhby73//m507d8YcT8XvMHDgQMBptxmtsrKSt99+G4BBgwbFHDfGcNhhh/Gzn/2MefPmATB79mzf53Ts2JFzzjmHWbNmMWrUKJYvX87nn39e7/FnGgX1mUqZehERkazVq1cvli5dyvr162v2WWuZPHkyX375ZRpHFkx+fj4XXHABpaWl3HnnnZ5jn376KU888US9n3H22WfTrl07Zs6cyXvvvec5Nm3aNFauXMkPf/jDmkmyX3zxhe83BOF94S43e/fuZeHChTHnVVRUsG3bNs+5TYnKbzKVaupFRESy1o033sg111zDwIEDGTduHM2aNWPhwoV8+eWXjB07lhdffDHdQ0xqypQpvPHGG/zhD3/g/fffZ9iwYWzYsIFZs2ZxxhlnMHv2bHJy6p4fbtmyJY899hjnnXceI0aM4LzzzqNHjx6UlJQwd+5cOnfuzEMPPVRz/rx587j55ps5/vjj6du3Lx07dmTt2rXMmTOHnJwcbr75ZgD27NnDCSecQJ8+fRg8eDA9e/akvLycefPmsWTJEs4888yYb1CagpQF9caYbsAdwGnAAcAGYDZwu7U20LRsY8zNwEnAAKA9UA2sBuYB91hr1/pcYxPc8n1r7XG1+T0yhrrfiIiIZK2rr76agoICpk2bxuOPP05hYSE/+MEPmD59Ov/4xz+yIqjv1KkTixYt4rbbbuOf//wn77//Pv369eOBBx6gqKiI2bNn19Te19VZZ53FwoUL+f3vf89rr71GaWkpnTt35pprruE3v/kNBx54YM25p556KmvWrOGtt95izpw57Nixgy5dujB69Gh+/vOfM2zYMACKioqYOnUq8+fPZ9GiRcyePZtWrVrRu3dvHnzwQa644op6jTlTGWsTxcQBb2JMb2AR0BGYA3wFHIsToH8NDLfWbg1wn2VAGfApsAloBgwERgA7gJHW2o+jrrE4gf8Mn1uutdbWq4mqMaZk0KBBg0pKSupzm9p770F49RZne+g1cPrUxOeLiIiINJKJEyfy+9//nldffZVTTz013cNpMgYPHszixYsXW2sH1/baVGXqH8AJ6K+31v45vNMYcw9wI/A74JoA9zncWlsevdMY8xPg/0L3OcPnulXW2sl1GHfmUqZeRERE0mz9+vWebDnAZ599xn333Ue7du0YMWJEmkYm0eod1Iey9KcAq4D7ow5PAq4CLjHG/MJauyvRvfwC+pBZOEH9IfUbbRbx1NQrqBcREZHGd8wxx9CnTx8OP/xwioqKWLp0KS+//DLV1dU89NBDNG/ePN1DlJBUZOpPCr3OtdZWuw9Ya3caYxbiBP3HAf+q4zPGhl7/Hed4G2PMFUBnoBQosda+F+fc7ODpfqOgXkRERBrf1VdfzezZs5k5cyY7d+6kTZs2nHrqqdx0002MHDky3cMTl1QE9f1Cr9/EOb4UJ6jvS8Cg3hhzJdANaAkcAfwQp27+ljiXHAU8GnWPT4FLrLWfBXxmvKL5/kGuTzn1qRcREZE0mzRpEpMmTUr3MCSAVAT14WW+SuMcD+/3X5fY35XAUNf7D4GLrLXLfM69B/gHzoeKcpwg/FfAucAbxpijrbXravHszKA+9SIiIiISUEYuPmWtPc5aa3DaWp4S2l1ijImZXm2t/YW1dpG1dou1tsxa+5G19jycQL89cFPAZw72+8Hp5NP43EG9+tSLiIiISAKpCOrDmfjiOMfD+7fX9sbW2q3W2nk4gf0e4EljTGHAy/8aej2xts/NCKqpFxEREZGAUhHUfx167RvneLhjTbya+6SstduBd4EOwGEBL/su9FpU1+emlYJ6EREREQkoFUH9/NDrKcYYz/2MMa2A4cBuoL7daLqGXisDnh9eSXZFPZ+bHnkK6kVEREQkmHoH9dba5cBcoBfw06jDt+Nkyp9096g3xvQ3xni6yhhjehhjOvk9wxhzNTAE+Bb4zLX/SGNMM5/zj8RZqArgb7X9nTKCp6ZeQb2IiIiIxJeqFWWvBRYB9xljTgaW4HSvOQmn7GZi1PlLQq/GtW8Q8Kwx5l1gGbAJOAAn434EUIbTorLKdc3PgbHGmLdxAv69ON1vTgNygYeBmSn6HRuXe/EpZepFREREJIGUBPXW2uXGmGOAO3AC6jOADcC9wO3W2u8D3GZx6PwfAGOAdjgtKlcAdwP3Wmu/jbpmNtAaOBIYBTQHtgKvAA9ba1+o56+WPrmuLyDUp15EREREEkhVpp5QwH15wHONz741BGw/6bpmNk5g3/TkuTP16lMvIiIiIvFlZJ96QX3qRUREmqjx48djjGHVqlU1+1atWoUxhvHjxwe+z4wZMzDGMGPGjJSP0c1vvOk2cuRIjInJEe/XFNRnKq0oKyIi0qguvvhijDE88MADSc895ZRTMMbw/PPPN8LIGtbkyZMxxrBgwYJ0D0XqQUF9pvIE9crUi4iINLSf/OQnADzyyCMJz1u1ahWvv/46Xbp0YezYsSl5dteuXVmyZAl33XVXSu6XSnfddRdLliyha9euyU+WtFFQn6miF5+yNn1jERER2Q+MHDmSvn378vHHH7N48eK45z366KNYa7n88svJy0vN9MRmzZrRv39/unTpkpL7pVKXLl3o378/zZrFdBGXDKKgPlPl5ECO638oqoOuuSUiIiJ1Fc7WP/zww77Hq6qqmD59OsYYrrzySgBmz57Nf/3Xf9G3b1+KioooKipi8ODB3HfffVRXVwd6bqKa+mXLlnHeeefRtm1bioqKGDZsGC+//HLce82fP5+rrrqKAQMG0Lp1awoLCzn88MO5/fbbKS8v95zbq1cvbr/9dgBOOukkjDE1P2GJaupnzZrFiSeeSHFxMYWFhRxxxBHcdddd7N0bW2XQq1cvevXqxa5du7j55pvp0aMHBQUF9OnTh6lTp2JTkMCsrq7mr3/9K0OGDKFly5YUFRUxZMgQHnzwQd//Fm+//TZjx46lW7duFBQU0LlzZ4477riav0nYpk2buOmmm+jXrx9FRUW0adOGfv36MX78eFasyIx1TlPW/UYaQG5BJJiv3OttcykiIiIpd9lllzFx4kRmzpzJ3XffTYsWLTzHX3nlFdatW8fo0aM56KCDALjlllvIyclh6NChdO3aldLSUt544w1uuOEGPvzwQ5588sk6j2fp0qUcf/zxbN26ldNPP52jjz6aZcuWcfbZZ3P66af7XjN16lS++uorhg0bxpgxYygvL2fhwoVMnjyZBQsW8Prrr5ObmwvAhAkTmD17Nm+++SaXXXYZvXr1Cjy22267jbvuuov27dtz0UUX0bJlS1555RVuu+02XnvtNebOnUt+fr7nmoqKCk499VTWr1/P6aefTl5eHrNnz+aWW26hvLycSZMm1flvBXDJJZfw1FNP0b17d6688sqaeQ/XXnst77zzDn//+99rzn311VcZM2YMrVu35swzz6Rr165s27aNJUuW8MADD9SMZffu3QwfPpzly5czevRoxo4di7WW1atXM2fOHM4991wOPvjgeo07Jay1+knwA5QMGjTIpsVdPayd1Nr52bU1PWMQERHZz5x//vkWsNOnT485duaZZ1rAPvvsszX7li1bFnNeVVWVvfTSSy1g33vvPc+xyy67zAJ25cqVNftWrlxpAXvZZZd5zh09erQF7LRp0zz7Z8+ebQHfcS5fvtxWV1fHjOnXv/61BezTTz/t2T9p0iQL2Pnz58dcE2+8ixYtsoDt3r273bBhQ83+iooK+6Mf/cgC9ne/+53nPj179rSAPf300+3u3btr9m/atMkWFxfb4uJiu2/fPt8xRBsxYoR1wtiIp556ygJ24MCBdufOnTX7y8rK7ODBgy1g//73v9fsP+eccyxgP/nkk5j7f/fddzXbL7zwggXshAkTYs7bu3ev3bFjR6AxBzFo0CALlNg6xKzK1GeyPK0qKyIiGWJycbpHENzk0npdftVVVzFr1iweeeQRTznMhg0b+Oc//0nHjh0566yzavb37t075h45OTnccMMNPPHEE7z22msMHTq01uNYu3Yt8+bN46CDDuK6667zHDvrrLMYMWIEb775Zsx18bLGN954I3feeSevvfYaF1xwQa3H4/bYY48B8Otf/5rOnTvX7M/Ly+Puu+/mn//8J4888gi33XZbzLX33XcfhYWFNe/Df88nnniCr7/+msMPP7xeY5oyZQotW7as2V9UVMTUqVP54Q9/yCOPPMJFF13kuc49lrD27dvH7PM7Lz8/P+bbiHRRTX0mi54sKyIiIg1u1KhR9O7dm4ULF7JkyZKa/dOnT6eyspLx48d7Jo1u3bqVW265hSOPPJKWLVvW1KQPHjwYgHXr1tVpHB9//DEAJ5xwQk25jNvIkSN9r9u1axe///3vGTJkCMXFxeTk5GCM4YADDqjXeNzCE4lHjRoVc6xv375069aNlStXUlrq/YBVXFxMnz59Yq7p3r07AN9//329xpSTk+P7dxkxYgS5ubk1f1NwWpgCDB06lGuuuYZnnnmGtWvX+l7btWtXpkyZwmmnncZ9991HSUkJVVVVdR5rQ1BQn8k8C1ApqBcREWkM7kmw4faW1loeffRRjDE1k2kBtm/fzpAhQ5g6dSqFhYVceumlTJw4kUmTJnHDDTcA+E4aDSIcEHfq1Mn3uDtDHlZRUcGoUaOYOHEi5eXlXHDBBdx6661MmjSppka8ruPxG1u8bj3h/du3b/fsb9Omje/54S5C9QmUS0tLadeunW/mPC8vj/bt23s+ZJxzzjm89NJLDBw4kMcee4wLL7yQ7t27c8wxxzBv3rya81q3bs17773H5ZdfTklJCTfccAPHHHMMnTt3ZtKkSVRUZMZ6Qiq/yWTK1IuISKaoZ0lLtrn88sv5n//5H5544gnuuusu3n77bVasWMGoUaM8meZHHnmElStXMmnSJCZPnuy5x7vvvsu9995b5zEUFzslT5s2bfI9vnHjxph9c+bM4YMPPmD8+PFMnz7dc2zDhg0xXV3qO7aNGzf6lh9t2LDBc15jKC4uZtu2bVRUVMS036ysrGTLli20bt3as3/MmDGMGTOGXbt28f777/PSSy/x4IMP8qMf/YiPP/6YAQMGANCtW7eaVqZffvklb7zxBvfffz933HEH1dXV/Pa3v2203zMeZeozWZ4WoBIREUmHTp06ceaZZ7JlyxZmz55dk7G/6qqrPOctW7YMgHHjxsXcw6/evTYGDhwIwDvvvOObwfZbATY8nnPOOSfweMKlPbXJkofHFm8Ma9eu5aCDDoqbmW8IAwcOpLq6mrfeeivm2FtvvUVVVRWDBg3yvbaoqIhRo0Zxzz33cNttt7Fv3z5eeeWVmPOMMRx22GH87Gc/q8nmz549O7W/SB0pqM9knkx9Zny1IyIisr8Il9ncfffdPP/887Rv357/+I//8JwTbgEZHdx+/PHH9V4dtlu3bowePZqVK1fyl7/8xXNszpw5vkF6vPGsWLGCX/3qV77PCdfar1mzJvDYrrjiCgDuvPNOvvvuu5r9VVVV3HTTTVRXV/PjH/848P1SITymW2+9ld27d9fs3717N7fccguAZ0xvvfUWlZWx6wCFvxkJtzP94osvfL8tiT4v3VR+k8lyXd1vKpWpFxERaUynnHIKvXr14oMPPgDguuuui6nXvvTSS/njH//IhAkTmD9/PocccghLly7lpZde4pxzzuGZZ56p1xjuv/9+jj/+eCZMmMDcuXM56qijWLZsGc8//zxjx47lxRdf9Jw/duxY+vTpwz333MNnn33GwIEDWbNmDS+99BJjxozxDdxPOukkcnJyuPXWW/n8889p27Yt4HS2iWfYsGH88pe/5A9/+AOHH3445557LkVFRbzyyit8/vnnnHDCCdx88831+t1r66KLLmLOnDnMmjWLww47jLPPPhtjDLNnz2blypVccMEFNZNjAa6//nrWrVvH8OHD6dWrF/n5+ZSUlPDGG2/Qs2dPLrzwQgDmzZvHzTffzPHHH0/fvn3p2LEja9euZc6cOeTk5DT67xlXXfpg7k8/pLNP/eNnRfrUL309PWMQERHZj9155501/eC/+uor33O++OILO3bsWNuhQwfbokULO2jQIPvwww/H7T1fmz711lq7dOlSO27cOFtcXGxbtGhhjzvuOPvSSy/Z6dOn+/apX7Nmjb3ooovsgQceaJs3b24HDBhgp06daisqKixgR4wYEfOMJ5980h511FG2efPmNb9vovGGzZw50w4fPty2bNnSFhQU2AEDBtg777zT7tmzJ+bcnj172p49e/r+DZP1yo/m16feWmd9gPvvv98OHjzYFhYW2sLCQjto0CD7l7/8xVZVVXnOfeaZZ+yFF15o+/TpY4uKimyrVq3sYYcdZm+77Ta7efPmmvO+/PJLe+ONN9rBgwfb9u3b2/z8fNuzZ087btw4u3DhwkDjDao+feqNtfVfkrcpM8aUDBo0aFBJSUnjP/ypC+CbV53t/3wa+vmvHCciIiIi2W/w4MEsXrx4sbV2cG2vVU19Jst1zdxW9xsRERERiUNBfSbz1NQrqBcRERERfwrqM5n61IuIiIhIAArqM5n61IuIiIhIAArqM5n61IuIiIhIAArqM5k7qFefehERERGJQ0F9JlNNvYiIiIgEoKA+kymoFxEREZEAFNRnsjwF9SIiIiKSnIL6TOapqVdQLyIiIiL+FNRnMvfiU8rUi4iIiEgcCuozWW6zyLb61IuIiIhIHArqM1meO1OvPvUiIiIi4k9BfSZTn3oRERERCUBBfSZTS0sRERERCUBBfSZTUC8iIiIiASioz2TqUy8iIiIiASioz2TqUy8iIiIiASioz2TqUy8iIiIiASioz2TqUy8iIiIiASioz2TqUy8iIiIiASioz2TqUy8iIiIiASioz2SelpbK1IuIiIiIv5QF9caYbsaYx4wx640xe40xq4wx04wxbWtxj5uNMf8MXVtmjNlhjPnMGHOPMaZbgusGGGNmGWM2G2PKjTFfG2NuN8YUpua3SxNPUK9MvYiIiIj4y0vFTYwxvYFFQEdgDvAVcCxwA3CaMWa4tXZrgFtdDZQBbwKbgGbAQOBG4MfGmJHW2o+jnj0UeCN07nPAt8Ao4H+Ak40xJ1trszMiVp96EREREQkgJUE98ABOQH+9tfbP4Z3GmHtwAvLfAdcEuM/h1try6J3GmJ8A/xe6zxmu/bnAdKAFcJa19oXQ/hxgFjAu9Pwpdfu10kx96kVEREQkgHqX34Sy9KcAq4D7ow5PAnYBlxhjipLdyy+gD5kVej0kav8I4FDgrXBAH7pPNfDL0NtrjDEm2bMzkvrUi4iIiEgAqaipPyn0OjcUTNew1u4EFuJk0o+rxzPGhl7/HbV/VOj11egLrLUrgG+AnsDB9Xh2+uTkAqHPI7YKqqvSOhwRERERyUypKL/pF3r9Js7xpTiZ/L7Av4Lc0BhzJdANaAkcAfwQWA3cUodn9w39LE/yzJI4h/oHGHLDMMbpVV8Z+gKjah/kZPfcXxERERFJvVQE9cWh19I4x8P729TinlcCQ13vPwQustYua4RnZ5bc/EhQX7kXmimoFxERERGvVE2UTSlr7XEAxpgDgEE4E2RLjDHnW2tfa6BnDvbbH8rgD2qIZwaiXvUiIiIikkQqaurD2fDiOMfD+7fX9sbW2q3W2nk45Tt7gCejes832LMzhnrVi4iIiEgSqQjqvw699o1zPNyxJl7de1LW2u3Au0AH4LDGfHbaqVe9iIiIiCSRiqB+fuj1lFB/+BrGmFbAcGA38F49n9M19Frp2vdG6PW06JONMQfjBPurgRX1fHb6qFe9iIiIiCRR76DeWrscmAv0An4adfh2oAh40lq7K7zTGNPfGOPpKmOM6WGM6eT3DGPM1cAQnNViP3MdehNYApxojDnTdX4OMDX09q/WWluHXy0zqFe9iIiIiCSRqomy1wKLgPuMMSfjBNpDcXrYfwNMjDp/SejVvSjUIOBZY8y7wDJgE3AATn/7I4Ay4BJrbU2zdmttlTHmcpyM/XPGmOeANcDJwDE4PfL/lKLfMT1ym0W2FeTjpwAAACAASURBVNSLiIiIiI9UlN+Es/XHADNwgvlfAL2Be4HjrLVbA9xmcej8AmAMcBPwn4AF7gYGWGvf9Hn2+zhZ/Dk4E2pvxJkgewcw2lqb3bNLc1VTLyIiIiKJpaylpbX2W+DygOcan31rcAL5ujz7S+C8ulyb8dwTZSuz+/OJiIiIiDSMlGTqpQGpT72IiIiIJKGgPtNpoqyIiIiIJKGgPtN5Jsqq/EZEREREYimoz3R57ky9ym9EREREJJaC+kznztRroqyIiIiI+FBQn+lUUy8iIiIiSSioz3TqUy8iIiIiSSioz3R5CupFREREJDEF9ZnOnamvVFAvIiIiIrEU1Gc61dSLiIiISBIK6jOd+tSLiIiISBIK6jOd+tSLiIiISBIK6jOd+tSLiIiISBIK6jNdrjL1IiIiIpKYgvpM5+lTr0y9iIiIiMRSUJ/p1KdeRERERJJQUJ/p1KdeRERERJJQUJ/p1KdeRERERJJQUJ/pPH3qFdSLiIiISCwF9ZkuT5l6EREREUlMQX2mU596EREREUlCQX2mU596EREREUlCQX2mU596EREREUlCQX2mU596EREREUlCQX2mU596EREREUlCQX2mU596EREREUlCQX2mU5/6hlexB75+BXZvS/dIREREROpEQX2ma9Yisr1vF1RVpm8sTdWc62DmhfDIyVBdne7RiIiIiNSagvpMl5sHRR1Cbyzs2pzW4TRJK99yXretgB3r0jsWERERkTpQUJ8NWnWObO/ckL5xNFXuVqHVWgtAREREso+C+mzQqktke+fG9I2jqXJ3FdICXyIiIpKFFNRnA2XqG5Y7U6/JyCIiIpKFFNRnA2XqG05VJVjX5Fhl6kVERCQLKajPBsrUNxx3lh4U1IuIiEhWUlCfDZSpbziV0UG9ym9EREQk+yiozwaeTL2C+pSKDuIV1IuIiEgWUlCfDTyZ+iZcfrP2I3j3fti1tfGeGZOpV/mNiIiIZJ+8dA9AAijqACbHmdC5e6sTiOYVpHtUqbXne3j8TKjYBRs/g//4a+M8Nzozrz71IiIikoWUqc8GObnQslPkfdmm9I2loWxZ6gT0ABs+bbznqqZeREREmoCUBfXGmG7GmMeMMeuNMXuNMauMMdOMMW0DXl9kjLnYGPOUMeYrY8wuY8xOY8xHxphfGGPy41xnE/y8l6rfL+2ael39vrLIdnSg3ZDU/UZERESagJSU3xhjegOLgI7AHOAr4FjgBuA0Y8xwa22yQukfAH8DtgHzgdlAW+BM4H+Bc4wxJ1try32uXQ3M8Nm/tva/TYZq1QX42NluinX1+3ZHthszW16pibIiIiKS/VJVU/8ATkB/vbX2z+Gdxph7gBuB3wHXJLnHRuC/gGettTWRlTHmJmABMAz4KXC3z7WrrLWT6zH+zNfkM/W7ItvK1IuIiIjUSr3Lb0JZ+lOAVcD9UYcnAbuAS4wxRYnuY639xFr7d3dAH9q/k0ggP7K+481aTb0Djrv8JjrQbkgxmXoF9SIiIpJ9UpGpPyn0OtdaW+0+YK3daYxZiBP0Hwf8q47PCEdalXGOtzHGXAF0BkqBEmtt06mnh/0sU9+IJTAxmXqV34iIiEj2SUVQ3y/0+k2c40txgvq+1D2ovyL0+mqc40cBj7p3GGM+BS6x1n4W5AHGmJI4h/oHGmFDa+qZ+gp3TX1jZuoV1IuIiEj2S0X3m+LQa2mc4+H9bepyc2PMdcBpwCfAYz6n3AMMBzoArYAhwHM4gf4bxpiudXluxmnymXpX+Y2thqp4X8qkWMyKsiq/ERERkeyT0YtPGWPOAabhTKIdZ62Nibistb+I2vURcJ4x5jlgHHATzmTdhKy1g+OMoQQYVMuhp15Tz9S7y2/AydbnNsI/z+hMvRafEhERkSyUikx9OBNfHOd4eP/22tzUGHM28DSwGRhprV1Ry3GFlyQ9sZbXZabCdpDTzNkuL/W2gGwKooP6xuqAE5OpV/mNiIiIZJ9UBPVfh177xjl+SOg1Xs19DGPMecCzwCZghLX26ySX+Pku9Jqw607WyMnxluCUNbESnJhMfSMF1zE19crUi4iISPZJRVA/P/R6ijHGcz9jTCucevfdQKBuNMaYi4GZwHqcgH5pHcd1XOi1thn+zNWU6+rTlqnXRFkRERHJfvUO6q21y4G5QC+cxaHcbsfJlD9pra2J2owx/Y0xMV1ljDGXAU8Aa4ATk5XcGGOONMY089uPs+AVOKvUNg2eoL6J1dWnLVOv8hsRERHJfqmaiXgtsAi4zxhzMrAEGIrTw/4bYGLU+UtCrya8wxhzEk53mxyc7P/lxpioy9hurZ3mev9zYKwx5m3gW2AvTgvK04Bc4GGcrH/T4Jksq0x9SsRk6hup646IiIhICqUkqLfWLjfGHAPcgRNQnwFsAO4FbrfWfh/gNj2JfHNwRZxzVuN0wwmbDbQGjgRGAc2BrcArwMPW2hdq+atktqacqa/w6X7TGJSpFxERkSYgZT0DrbXfApcHPDcmBW+tnQHMqOUzZ+ME9vuH/SpT30jBtWrqRUREpAlIxURZaSzuTP2O9ekbR0PImJp6db8RERGR7KOgPpu0di2O25SC+upqqIjqu99YQb0y9SIiItIEKKjPJtGrylqbvrGkUnRAD403UTZmRVlNlBUREZHso6A+mzRvDfmtnO3KctgTZP5xFoguvYHGmyirFWVFRESkCVBQn21au7L1TaUEZ19Z7L7Gmigbs6KsgnoRERHJPgrqs03rAyPbTSWo9yu/SVumXhNlRUREJPsoqM827smyO5tIUO9XfqNMvYiIiEhgCuqzTav9pPym0TL10UG9MvUiIiKSfRTUZ5tk5TdfzIYnzoavXq7dfa2FivL6ja2ufDP16VpRVkG9iIiIZB8F9dkmUVBfUQ5zfgor5sML1wdveVlVAY+Ohik94LPnUjfWoPb51dSrT72IiIhIUArqs407qN+5wXts85eRUpbdW2DvjmD3XPkWrP3QCXAXTEnNOGvDt/tNujL1CupFREQk+yiozzat3Jn6dd5jG//tfV/2XbB7bl3m2l4K21bUbWx15dunPl2ZepXfiIiISPZRUJ9tWhwAufnOdnmpNyDeEBXU7woa1C/3vl/6et3HVxeZVFNfraBeREREso+C+myTkwOtOkfe73CV4ERn6ndtDnZPd6YeYNm8uo2trtLap96npj7oXAQRERGRDKGgPhu5e9WHS3Cqq2DTF97zAmfqo4L6lW9BxZ66j6+20rWirLX+ZT7VlQ3/bBEREZEUUlCfjfwmy25dFpvxDlJTX7kXSr+N2lcOq96p3xhrw7emvhEy9fHq9jVZVkRERLKMgvps5FmAKpSpj66nh2DlN9+vAlsdu3/p3DoNrU7StaJsvLp9BfUiIiKSZRTUZyNP+U0oU7/x09jzgpTfuEtvCooj20vnNl5tecZl6lV+IyIiItlFQX02au3O1IcWoPLL1Acpv3EH9YedDfktne3vV8XW2jcUZepFRERE6kVBfTZyZ+p3rncy6hs/iz0vUKbe1c6y4wA4eGTk/bJ/1XWEtZO2TL2CehEREWkaFNRno1ZRmfod62DPNud9uIc91D6oP6APdB3suvfa+o0zqHT1qY/3bYAWoBIREZEso6A+G7XqDBhnu2wzrCuJHOt6DOTkOdt7d0BFeeJ7uUtsDjgYmrvq6st3pGS4SVWkaUVZZepFRESkiVBQn41ym0HLTqE3Fpa6FovqchS0aB95nyhbv7cMyjY62znNoLiHN6jf20hBfaZl6rWqrIiIiGQZBfXZyj1Z9vP/F9nuciS07BB5nyio3+YqvWl3EOTmQUHryL7GyNRXVzl98aM1RglM3Ey9gnoRERHJLgrqs5V7sqy7fKX7UCgKGNS7S2/a9XZem7uC+sbI1Ptl6aFxJsqq+42IiIg0EQrqs5V7siw4PebPeRgO6A1FHSP7yxIsQOWZJBsK6t2Z+r076z/OZNxBfW5BZLsxWlpqRVkRERFpIhTUZ6s+J0e2+/8Ifvo+HHm+874oYE19dOcbgIJWkX2NUX7jDupbtItspzVTr/IbERERyS556R6A1FG/0+HHr0NODhw4CIyJHGvpytQHLb8JB/WNXn5TFtkubAs7QyvkVu51+u+7f69Ui5upV1AvIiIi2UVBfTbrPsR/f11q6sPlN/mtcNplWifgrq6CnNz6jjS+it2R7YLWYHLBVjnPr650Ov24VVU6E3pTQTX1IiIi0kSo/KYpClJTv28XlG93tnPzIzX6OTneEpyGzta7y2/yiyDPXVcfFXQvvA/u6gYv35SaZ6v7jYiIiDQRCuqbIk9N/Rb/c9zBflFHb5lLY7a1dJff5LfwrogbnTFfOA0q98CHD8f/vWoj7oqyytSLiIhIdlFQ3xR5aurjZOrdZTnuvvbQuHX1nkx9y/iZ+upq2L0t8n77mvo/O16mXotPiYiISJZRUN8UuVeU3b3VqYuPFp2pd/OU3zRwW8t9rpr6/CJvW0t30L1vJ2Aj73esq/+z42bqFdSLiIhIdlFQ3xTl5UPzNs62jcpwh7kz+NGZ+rSV3xQ5Yw9zB93R49ixvv7PjltTr/IbERERyS4K6puqZCU4Za7ym+hMfTrLb+Jl6stLvdeVrq3/s93lPcbV4UdBvYiIiGQZBfVNVbK2lp5MfXT5jTtTHxVMp5o7qG/WIn6mPvrDRSrKb9zBe35L136V34iIiEh2UVDfVLmD+jKfoN5TU5/GibIVUS0t42bqo8ZRmoqaetf9CxTUi4iISPZSUN9UJc3Uu7vfJMrUN3b3G3emPkH5TYNm6lV+IyIiItlFQX1TlbSmPlH3G3emvqG730Rn6uP0qY8pv1nv39WnNpSpFxERkSYiZUG9MaabMeYxY8x6Y8xeY8wqY8w0Y0zbgNcXGWMuNsY8ZYz5yhizyxiz0xjzkTHmF8aY/ATXDjDGzDLGbDbGlBtjvjbG3G6MKUzV75d1PAtQ1TJTn7aJsi285TeJMvW2Cso21e/Znkx9kf9+ERERkSyQkqDeGNMbKAEuBz4A/gSsAG4A3jXGHBDgNj8A/gacCnwO/Bl4CugK/C8w3xjT3OfZQ4EPgbOB14F7gR3A/wDzjDEF0dfsF9zZ9+3feo9VlEeC9Zy8SPvLsEwpv3EH134TdutbV+/+0JDv6s2voF5ERESyTKoy9Q8AHYHrrbVnW2tvsdaOwgnu+wG/C3CPjcB/AV2steeG7nE10BdYDAwDfuq+wBiTC0wHWgDnWmsvstb+ChgK/AMYDtyYkt8w23Q8NLK98i3YtiLyflfUJNmcqH8GacvUF8XP1PuNY0c921pWxSm/qa6s331FREREGlm9g/pQlv4UYBVwf9ThScAu4BJjTBEJWGs/sdb+3Vq7L2r/TuDu0NuRUZeNAA4F3rLWvuC6phr4ZejtNcYYE/gXaioO6A29Tw69sfDuA5Fjnh71UZ1vII2Z+qjFpxJ1v4H6L0BVqYmyIiIi0jSkIlN/Uuh1biiYrhEKyBfiZNKPq8czwjMXo1Ooo0Kvr0ZfYK1dAXwD9AQOrsezs9ewn0W2P/5bZGXZRD3qIX0tLZtFZ+obuPzG/aFBNfUiIiKSxVIR1PcLvX4T5/jS0GvfejzjitBrdPCesmcbY0r8foD+tR9uhjh4JHQ6wtmu3AMfPupsJ+p8A42XqbfW212noCXkxelT3xDlN+4PDQXumnp1vxEREZHskoqgvjj0Gm/p0fD+NnGOJ2SMuQ44DfgEeKwxn531jPFm6z/4P2eSrCdT71d+4wpw9+5wgu+GULEnUr+eW+AE9LlxVpT1+3ARnanfWwbvPQhfvxLs+Z5MvcpvREREJHvlpXsAiRhjzgGm4UyiHWetbbAUqrV2cJwxlACDGuq5De7wc+D1ybBzvRPMf/F8VE29T6Y+txk0awEVuwEL+8q8gX6quLPv4ZKfeJl6v/Kb6AWoFk6Dt/7obP/3Iuh0WOLnezL16lMvIiIi2SsVmfpwtFUc53h4//ba3NQYczbwNLAZGBmqkW+UZzcpuc3gmCsi71e9nbymHhqnBMdTehP60BB08SmAnRu9AfjqdyPb6z9J/vy4mXoF9SIiIpJdUhHUfx16jVe3fkjoNV7dewxjzHnAs8AmYIS19us4p6b82U1SrxMi22s/TN79Buo2WbbsO6ekJij3h4UCn0x9OJNeVRH61gAwOdCyU+gECzs3RM4vXeO6d7yKLJdKld+IiIhI05CKoH5+6PUUY4znfsaYVji94ncD7wW5mTHmYmAmsB4noF+a4PQ3Qq+n+dznYJxgfzXOQlj7ry5HOYtMAWz5BrYuixxLVab+o+nwv33g/mOd2vYg9roC7/CHiFyflpae4L8VFHeLvA/X1VdVemvsywN8OVOl8hsRERFpGuod1FtrlwNzgV5ELQ4F3A4UAU9aa2t6Fxpj+htjYrrKGGMuA54A1gAnxim5cXsTWAKcaIw503WfHGBq6O1frW2omZ5ZIr8FdDo88r5sY2Tbr6YeYifLJrJzE7w20dnevgZWLww2Lk/5TYJMvSf4L4bWXSPvw3X1OzeArYrsV6ZeRERE9iOpmih7LbAIuM8YczJOoD0Up4f9N8DEqPOXhF5rFoUyxpyE090mByf7f7nPmlHbrbXTwm+stVXGmMtxMvbPGWOew/lAcDJwDE6P/D+l4hfMet2OgQ1RdeYmB1q08z+/NuU3C37v7Te/a0uwMfmV3+T6TJT1nBcnqN/uKr0B2JMkU19d5f0Q4O5TX61MvYiIiGSXlAT11trlxphjgDtwSmHOADYA9wK3W2u/D3CbnkS+ObgizjmrcbrhuJ/9vjFmCM63AqcArULn3QFMsdbujbnL/qjbEPjwEe++Fu0hJ9f//KDlN5uXwOInvPv2bAs2Jt/uN+6WluGgPqpMp9gV1IdLbkq/9d47WabenaXPLXAmFIep/EZERESyTMpaWlprvwUuD3huTAreWjsDmFHHZ38JnFeXa/cbXY+J3Revnh6cMpewRJn6ub8B70LCkZVrk0maqQ+X37iD/4CZ+mQ19e7ON9H98VV+IyIiIlkmFRNlJRsc0BuaR63BFa/zDQTL1K9aCMvmxe4PnKn3aWnpm6mPCv7dE2XDwXxMUJ8sU+8K3HPzFdSLiIhIVlNQv78wxqmrd0uYqQ9QU790bmTb/QEhaKbet/uNT6Y+uvzmgD6R91u+cTrf1LamPiZTr/IbERERyV4K6vcn3YZ439c3U7/F1W30kFMi23uCTKEgQJ/6UOAdXX7Toh20DmXrK8th69I61NQnytQrqBcREZHsoqB+f1KbTL2npeVO/3O2uNYE63FcZDtwpt5138B96kPndT4ism/9J7A9Kqiv2JU4OI/O1Oe4M/X7YD/vgioiIiLZRUH9/qTrYO/7eD3qIXn5TeU+2LYy8r770Mh2XbrfFPgE9ZVxym8AuhwZ2bdsnn8bykTZek/3m3zIyQET7gRknZaXIiIiIllCQf3+pLAtHHBI5H3CTL27/MYnON62ItLnvbi7tyPN7m3BMt3Jym/C2fToxacAOruC+qU+k3UhcV29ezJs+JmaLCsiIiJZSkH9/uaY0BIArbtCj+Pjn5espeWWbyLb7fs6izeFg+KqvVCxO/lY/PrU+2bqk5TfxJvIGzhTr6BeREREslvK+tRLljj+Wuh3GrTqAs0K45+XbKKsu56+fV+nu05hOyjb6Ozb8713lVY/npaWCTL15T6Z+jY9nO1EgXt5ggm7nkx9KJjPdf2fQ3Vl/GtFREREMowy9fujdgcnDughtqY+upzG3fmmQ1/ntUW7yL5kk2WrKiLZfJMT+QDgl6mP7n4DzocIdwmOH2XqRUREZD+hoF78uVdZra6Eij3e499FZerBydSHJZssG73wlDGR54Yl6n4D/kG9u01nwpp6d/ebcKY+qgOOiIiISJZQUC/xuQNodxBurTdT376f89qibWRfsky9O4te4Krfj158ylr/7jfg7YAT5q61T5ipd/ep98vUq1e9iIiIZA8F9RKfO4De/CV8v8oJsnesc/rAAzRvA0Xtne26Zurdz8nJgRxXbfveHZF2lbn5kNc8cswdwPvtK4/K1FsL38yFT5+GStc3DzWZepXfiIiISHbSRFmJz52pf/Js57XfGBjy48j+Dv0ipTOemvokq8p6etS38h7LLYhMVC37zjue8LPAKfvJLYiU0hQUOxNow6Iz9d++D0+d52y3Pcj7PPB+mFCmXkRERLKIMvUSX+sDY/d9/TK890DkfXtX3/vaZOrj1clDJHMOsMsV1DePOi+3GXQ8NPK+TXfnm4OaMURl6td+GNn+3rVwlm+fegX1Tcq2FTDrMlgwRasFi4hIk6SgXuI74efQ6Qhn5Vl3wL7s9ch2uJ4eatf9Jl75DXjr6j1BfTEx3HX1bXp4g/roTH28MeWq/KbJe/tu+HI2LLgL1n6U7tGIiIiknIJ6ia/7EPjvd+DmpXDtu9569rBw5xuoZU190Ez95vjnAXQ7NrLdob838I+uqd8TpySoJlOv7jdN1rZVru0VaRuGiIhIQ1FNvQTTqjMMuRLe/Yt3fwdXUF+bTL2n+41PTX3Yri2R7eiMPsBRF8K6EieAP+6/vfeNztTH+6Dhm6lX+U2T4v4QGe/DnYiISBZTUC/BnXAjfDQ90vkmtwDa9Iwcr2umPjpYz4sX1PuU3+Q2g7HT/J8RtPzGL1NfraA+oT3b4d+z4MCjofuxyc9PNwX1IiLSxKn8RoIrag/HXRN5f0AfyMmNvK9rTX10WU1uvPIbn6A+mjvw37PdOykyXjCXq/KbWnt9MrxyMzw+Fso2Jz097coV1IuISNOmoF5qZ9jPoF1vZ3vQJd5j0ZNUq6vi3ydh95talN9EyyuAvEJn21bBvl2RY3Ez9Sq/qbXwZNPKctj47/SOJRlrvR8iFdSLiEgTpPIbqZ3CtvDfi5zymuiWl7l5Tqa8vBSwTqa86AD/+yQqv8mN19IyQKYeoLAN7AwtLlW+HQpaOtvukqDCdpH3vivKKlOfkDswTvatTLpVlnvLqRTUi4hIE6RMvdRes+b+PewheF19ovIbd6a+LEn3Gz+eDjihuvp9u53gDiCnGRx7VeScLkc5r57ymwCZ+m8/cPqeb18TbFxNiSeo35q+cQTh/lYIFNSLiEiTpEy9pFaLdpGFnRJlcMsDZurdbSlbxMn6R/NbgMr9AaNFOzjxZmh3MBR3hU4DnP05tQjq92yHv50Le0th9UK47MVgY2sKKvdGJktD5gf17g+QENvqVEREpAlQUC+pFThT766pj25pmU8Mkws9hgYbg1+m3p2dLWznlAoddUH85yYrv1n7kRPQg5OxtxaMCTa+bBe9Um9tg/rqKu8E64a2N7q1qTL1IiLS9Kj8RlIraAecoBNlw3oNd+r5gyh0T9jdHjsW9xjdatP9Zv3iyHZlOZRtij1n2evwfyPhrf9NfK9sE/1hrTZB/fy74K7uMP/3qR1TIn7lN9XVjfd8ERGRRqCgXlIrSKbe2sQryvpl6vv/KPgYfDP17kmycT4c1Kb7zboS73u/uvp5k2H9x/DGb5tW3X10pntXwKC+qhLeuccp3Xn7nsbrMBRdfmOrYd9O/3NFRESylIJ6Sa0gmfp9ZUCof3yzIqcUxs0vU9/v9OBj8Kupr22mPtHiU9bCusXefd+vjj3v+1WR7c1L4t8v20QH9UEz9eWlkW9Aqisar2vO3h2x+1SCIyIiTYyCekktdxY8Xqa+PEE9PcRm6jsfAW16BB9D0kx9PctvStd6F8UC2L7K+75ijzcbvGVp7H2shTXvweav4j8rE9U1qI/+9+BuV9qQojP1oKBeRESaHAX1klpBMvXuIMtvQanoTH2/MbUbg29NvXuibD3Lb6JLbyA2Ux+9yupWn6D+37PgsVPhgaGw6Yv4z8s0fkG9e+XeoNc1VlAfXVPvNxYREZEsp+43klqemnpX4LR3p1NHXbkX+p4a2e/Xez43Kqjvf0btxpAsUx+3/CZg95v1i2P3bY8K6qMD1i3LYq9ZOjeyvfwN6HRY/GdmkuiA2FY5f2f3hyk/0R/y3KsFNySV34iIyH5AQb2kll+mfscGeOo82PiZ837V25FzfDP1ruC6uDt0PrJ2Y0hWU1/f8pvoenqIzdRHB/V+mfod6yPbOzfGf16m8QuId29NHtTHZPgV1IuIiKSKym8ktdwB867N8M1r8MjJkYAeYOO/I9t+NfWtu0a2Dx1b+/7vdc3UexafqvQ/p7oK1n8Su790rfea6PKbsk2xZSA71kW2Gzuo//pVeGda5O9TG75BfYBJr+mqqVf5jYiI7AeUqZfUcgfMu76Dp85PfL5f+U3/H8Hg8bC3DEb8qvZj8Kupj158yk+Q8pstSyMTYIs6Oh84yjY5JSg71kHbns6x6Im04GTruw52tqurvZl6vz73DeW7r2HmhYCF0m9hzN21uz5epr621zXaRFm/oF6ryoqISNOiTL2kVrMW/oF6fis45NTY/e6ses25LWDsvXDuo8lLOvz4ZepTtfiUu56+62Bo0zPy3l1XX+YTsG5dHtnevcXbNnPnBv/nNYRvP6CmpeiSl4JNcnXzDeoDlNKkraY+Rd1vXrkF7hvofMshIiKSYRTUS2oZA6N+A60OhLYHQffj4OiL4crX4Yw/xJ7vV35TX/mtwIT+ae8rg8p9kYw91K/7jbvzTddBkcw8eBeY8svUu9tauktvAHY2Yqa+9NvIdtlG2OoziTeRbMvUp6L8ZssyeP9B2LYCFjTiargiIiIBqfxGUm/oVc6Pn4NHwooFkfd+Wf36yslx7hsO5LetcFYRDT/PnZF3C1J+Ex3UV5ZH3rsny/plod2TZd2lN+CU9Owtg4KW/s9Npe3fet+vfBPaHxL8er/SlUBBfRZ3v3GvQxD99xMREckAytRL4xp0qfe9X/ebVGh3cGR79cLIdrwsPXhXtvVbUXbnxsgkWZMDV4STggAAIABJREFUBw5KUH7jl6l3ZcSjg3povLp69zcKACvf9j/PT1WFf5Bcp0x9FpXfuCcy79mWeB0DERGRNFBQL42r/4+8gXVDZOrBWYU2bOWbke2EQX2S8puvXqamFr3ncKc2311+48nU+02UXeZMkIXY8htovA44pVFB/ap3IuNKJl63nCDdb3ZHBdL7djor7zak6iqnBCtafYJ6aLzSIXA+DL420fsNl4iISBQF9dK48gpg2M9C24XQ47iGeY4nqHdlouNNkoXk5TdLXoxsHzrWefXL1FdVRIJGkxPpm1+5JxLMl/oE9WWNENRXV8V+S7B7C3y3JNj18YL3umTqoeGz9e5vFdyLmu35vnYThKODer9vYhrKizfAu3+Bpy+uWwtSERHZL6QsqDfGdDPGPGaMWW+M2WuMWWWMmWaMSZAajbnHaGPM3caYfxljthpjrDHmnSTX2AQ/79X/N5OUO+HncPmr8LOPoGXHhnmGe8GqPQEWngJvrf2e7bD4SeenqtIJAt2LZvUf47wWd4tMyt25ASrKvVncFgdAh36R9+G6er/ym8bI1O/cANU+PfiDluC4A3P3tyzJgvrKfZFWoG4NvQCVu/SmqAPkNXe2q/ZBxe7g94nuTtSYmfoNoZKvfWW1n9QsIiL7jZRMlDXG9AYWAR2BOcBXwLHADcBpxpjh1toAqTx+CpwFlAPLgAQRmMdqYIbP/rUBr5fGZAz0PL5hn9HpMMBQUy4TFjRTv205vHCds73mPTjoB5Fg+MBBTjAPzgeB1t0iJS2l33qDxaIOcMAh8O37zvuty6H3qPSV30TX04etehuOuyb59e6g/oDesP5jZztZUF8epy98Q2fq3Z1vClo56wmEA/Q930N+UbD7RM93aKxMffkOb/lQ6brIWgfZ4OtX4NOn4difQK8T0j0aEZEmLVXdbx7ACeivt9b+ObzTGHMPcCPwOyBAxMBUYCLOh4LuwMqAz19lrZ1cmwFLE1fQ0gk6ozObCTP1+f77P/kbrJgfeX/oj7zH2/SIBPXuunpwgvr2fSLvtyx1yj7SNVHW3bml4wDY/KWzveptpzQnJzfx9e6gvu1BoYnD1tlfVemdbOwWr2ynoTPe7vKb5q2dD5TuoD784SyZmJr6Rgrqo78h8PswmKkqyuH/XQ17S2HdYrjxs+TXiIhIndW7/CaUpT8FWAXcH3V4ErALuMQYkzQlZq1911r7hbW2qr7jEvHU1YclytS37eX8AOTkQXGPyDF3MHXomVHXuevqV3kDvpYdnUx92NalToBbtTf2+Y2xAJV7kmyfk6FlJ2e7vBQ2Bgi63EF9UQfv4mCJJp/GO9bgQb2r/KagtXeidNDJstamr6Y++t9EaRZ9+bhjnRPQg/PvLshkahERqbNU1NSfFHqda631tNCw1u4EFgItgAaaEQlAG2PMFcaY24wxPzXG1PpZxpgSvx+gfwOMVxqDX1CfKFOfkws/mQ/jX4abl8O1i6Bdb+857fvF9nRvE9UBxx3wFXX0nr95CexwBWY5rsx2YyxA5S6/Ke4BvX4Qeb/m3eTXuwPhwrbQon3kfaISnOge9WG1Der37YLZP4V//MR/Ualo0eU3dQnqd2+LbXHaWEH9juhMvc83PA1l9zZ44WdO550qn3kYyUR/IPk+6BevwDt/gmcv967CLCIiCaUiqA/PAvwmzvHwijt9U/CseI4CHsUp8/kL8K4x5hNjjE9UJ/uNzkfF7muRZN52i3ZO7W9hGycIPG+6tywn3PXGrd1Bke3vvvIGqi07OB8M8kMr5+7c4J2U2umwyHaj1NS7ym/a9HAW0Apzr3gbT0xQf0DkfaKg3p2lNa7/2dkVZKqNyydPOeVQn82Cd6O/GPQRXX4T9JsFN79vUBqt/CYqiG/M8psP/g8WP+F03vni/9X++ugPINsCBvXrSuD1yc4z5/+u9s8VEdlPpSKoLw69xuu1Ft7fJs7x+roHGA50AFoBQ4DncAL9N4wxXYPcxFo72O8Hp75fslFtM/V+uhwFY+52MuqF7WDw+Nhz3BMX137orY0v6uDUmbsnCX7yVGS70xGQE+q6s7e04fu2l7qD+u5wgKveP0hnlboG9Z5a/F6R7dpm6sNzAMC7qFg8e1OQqfdrNVrWSN1voj/o+bVCbSjucqx1i2t/fV2D+vDka4CNn9f+uSIi+6ms71Nvrf2FtXaRtXaLtbbMWvuRtfY84B9Ae+CmNA9R0qVVJ6f8xS3R4lPxDLoUJnwOEz5zAuFo7Q6OBLd7vne65YSFn3/wyMi+zV9Etou7ReraIVi2vqoS1rzvtN2sDWu9NdnF0UF9gFKHmKDe9SEpaPlNe9eXdrUN6t0Tkdctdib3JuIpvymuW1Dv99+ksTL10YHxzg3Jf+dUcX8A3BrgW5xodS2/cf873L46+MJoIiL7uVQE9eFMfHGc4+H9tYxA6u2vodcTG/m5kkmis/WJJsom0rqL01HHjzHQ7djIe3cw1LKD83rwyDj3PdD58BEWJKh/9RZ47BR4cDjsq0Wv9bLNUFnubDdv45SjtOkRqevfsdapWU8kFZl69xyD2ra03O4K6it2OXMUEnFPlG1ex4myfv9Ndm/zX3U41aIDY1vVOF2SwFuqFaQ0K1p0qVDQTL37G6PK8sb7fUVEslwqgvqvQ6/xaubD/x88Xs19QwmnAAM2opYmyR3U5+R5F0xKpe7H+u8PZ+o79IOWnWOPt+4KrbpE3idbVbbsOyiZ4WzvWAtrFgUfY3TpDTh99t3lMNtWJL5HwqA+QXcT97H2rsW4dm8JvrJrdXVsn/21Hya+JhXlN74ftGzD99iP9+zGKMHZt8v77cr2NU6LytqInuQbOFMfVQb2/araPTds56bG+eAlIpIhUhHUhxt4n2KM8dzPGNMKp959N9DYq7uGO+AkiVKkSeviWlm2sK2TVW8I3Yf67y8KdYcxxj9b3/rAqPKbTfCv38K0I+ClG2NLYj59ytuJZd3HBBbd+SasNnX1CYP6BEGu+7rirpBX6GxXlnsXV0qkbKOzEqzb2o8SX+Mpv4nO1Af88jBeq9GGLsGprvIP6nc0QltLd5YeAJv8A1+06L/bzg3Jv1mq3Be71sP21f7nJvLRY3B3X/jLENgb8N8XQOVeWP1u7T/AiIhkgHoH9dba5cBcoBfOirBut+Nkyp+01tZ8r2+M6W+MqXeryP/P3n3HyVWVfxz/nNneW7LpvQMJkITeW+giCIgKCgJKUVBQUdSfoqLY6IgUKQIKSAfpLQklBJKQ3uumb7LJ9jLl/P44Mzt3Zmezs5tNGfy+X699ze7M3Hun3Jl9znOec44xZpwxJiPR9biZcAAe39njSArrOx63siyxGeluP86BsdNTAmQXQXpW9O+hx7TdrqgfFHgy+Eteh6l/cQH4Zw/BXRPgqYtcsG+tm43Ea30Xg/riLgT1oaCbzz4ip7hr5Tc5pW4AcUSydfWJMraRTH0o5IKxj+9x010+exlULume8htv+UekMQK7frBsfaUrt4m3OzL11fFBPZ2rq2+vQdJR1n376rbPuSuZ+ukPhLddCUvfSH67J78OD58Cj52dfA+SiMheortWlL0K+Ai40xhzArAQOAQ3h/0S3CqxXpFC2Ji0qTHmSOCy8J+RAuYRxphHIvex1l7s2eQ64ExjzFSgAmjGzSt/CpAGPAD8eyeel6S60iFw8s0uWD72xl13nMxcV+rjDbLjB+kOiQvqM/Nd9tgb1HtXrgXAwsKXXHBywq/aBt3rOzErSaLyG3ADfSN2NFi2qdo9HnANFl9aF4P6EteDEVkIq35L7GNoT3wGF2DLYpdxf+FKWPxq7G01G6KLH0H3lN/02hfWhXsHdnWmvr0egt0xrWV8mRN0rq6+bnPiBsm2ldBrn/a3S9So7GxQ31LvppaNWDMN9vtKx9vVbYZlb4e3+ci9/oV9O3fsPcnfBFXL3UrRu6pHUkT2at0S1FtrlxtjJgK/wQXUpwEbgDuAm6y1Sf73ZDjwrbjryuOuu9jz+wtAITAOOB7IBrYCrwEPWGtf6twzkS+kw652P7vagENig/r8uKC+qJ+b+WVLeHhJYV/3zzdRrT3AwMOjNfMb58JTF7a9T+0GF7wW9ml7WzxvSUWRJ6hPNlMfH5hD8rPfeGvqc7uYqW+vDOO937cN6MEtpuV9fFmFboBwRDJBffxqsr3HRoP6Xb0AVXxNeuv1eyhT35mgPn5+/YiOBst2R1C/cS5410FcncSiatB2fMbGuakT1Ada4L6jXSP30KvhlN/v6UckIntAt01paa2tsNZeYq3tY63NtNYOstb+IFFAb6011to2qQRr7SOR29r7ibv/C9bac6y1w621heHj9rHWnqmAXna7+MGy3sA1Yuix0d8jAYN39puI/gfBt1+DM26PXhfw1Pl66/CTzdbvbPlNwqDek6mv25x4sKy/EQLh+fd96a6HIs+zEu26mfDIGfDw6bFTbsbzZuozPTMRTb8v+vugI6MNFhuMbTBkFbpsvUkLP66Gjlel9a4mm1UEJZ7Vgzs7HWdneQNj73u0O8pv2tTU07nym/ZWvu2oLj9hUN/Jmvr4OfU3zUtu/ETF9Ni/N87p3HEjajcm7unYldbPdAE9uMXZVDok8j8p5eepF9lrxA+WTRTUjzot+nuf8Iq3BQmy7Idc4S4nXgIHfyf2tj77w37nRv9OtDDQqg/cypwRoWBc+Y0nqC/oAxm57vfGbYkDc2sTB/XZRVAcDnQDTfDaDe73+q3u99d/Fpvpzil1vRPeoH7qX2DVVFj9AbxwVfsBiTdTP/qMtrdnFsD5j8a+xl7Zhe7YPT3DeRa8kPi+Ed4SmILesSVV8VMtVi5xz3fKX9yqwR1ND9oR7+vWb2L09z2WqV+WfLDo7WXwNkA7mgEnUflX7frODVxtM87Etg3YE4kfdO1dfCtZG+bAbfu6ge7L3+389l21ybNIV1N15wc176y6zam5nsBnD8Hfj4TZT+7pRyLSLRTUi3SXov5Q4Omujy+/ARh2HJx5Bxx5HRzxA3ddbo9o9hhckL3PWdG/T/5DbD3+xEuh3/jo3/GZ+mn3wiOnwwPHuwAT4M1fRmeZyS6OrS33+aB0WPRvb7bU3wSPfgn+NBQ+uC16fWR7Y+CUP0Svn/s0fHgHPHg8fPJ3mPY3ePe3bbdL1OABWDkZ5jyV+DZvxnbsuW1vP/IHrrEw+Ii2t6VlRQctH/D16PWR6UHb451itKBX7HvqLb8J+uGJc6PP99Ez4I+D4bOHd7x/cMH/hjltgyJvYNzPM+C7duOun6oxUaa+uTr53glvL8Ogw6O/d1R+4y3xSfMMMu9M5jtRz9WaDkpwgoG223VlNdtZj0Mo4H5P5r3vLpvmx/7dlRWAu2ryn+AvI+Chk3fvFKKhoFude8GLXdu+udYlHjbOhVeu04xH8oWgoF6kO3lLcBJl4AEmXAwn/ipa7+3zxQaLB13q5o+PSEuHrz4OR/0IJv3OrXAbE9TPimZQK5fAW7+K3vbub+Gxc2DaPdHrDr2q7UC6snaC+nnPuEC7sQpWfxi93tsoGH06jLsg+vdb/xdbBz3fkw2PPOdcT6Y+3hs3uky/V6DFk6E2MORol/WPKOjrnhfAoARBfVZB9Pf9vwZpme73dTPaZmSba+Hdm132zhtYF/SJfZ+8Ae7cZ9rW/Adb3GvhnYEnnr8J7j8O7jsKXo1b/NobGBcP8hzbJrdIWVcFWjw9FAbK943elmxdvbf8ZqAnqK+uaD/wa66NNqJ8GdDf0zuRbF19U3XiEp6OgvpN81w5llfVih2/d4msmhr7++7KXsc3QLy9dLuSta7xDrB2uush3F1mPOwGyD/9TVj8Wue33zAnOkWuv373vWYiu5CCepHudOQPXblB+b6wz5eS327wUe4ytwdMuKTt7dmFcMIv4fDvu4C8ZEg0sG7c5soaQkF48WoINsduu/yd6O+jz4Cjf9x2/+3V1c95OvHj9Qb1AKfe0n4jxjsLSnuZ+sO/D4X93e8NW+GtX8beXl1B68w7hX1d1t2bkT/+F24GInDZ+p5jYrfP9iw6llcGYzzvzYxHY+/73Hdhyp/g+e9GAxZw72tegkx9KOR6JyIGHBoda9Bc47KJ7Vn0SrQWetbjsXOqewP3wj6xgzZ3ZQlOzTpaX+uCPrGz1SRbV+8N6suGRs+NUCBxaQ/Elt6UDo2dESnZueo3zI7+HjmfwAVsO8rEJlzEzMKmBckdF9wsTps992/c1vW6/M4IhWKPC52bFSvQ4n66onpt7AD5JZ2YPnRnLfAMm5v/fOe33/B57N/eBplIilJQL9Kd+h4A1y2CKz909ebJ+tKdcO5DcNlbsfXm7THGzY0fsW6mK7tZG64d9qW7QaNevcfC2fe5noF4iYL62o2wckrkgLFBVvz4gZwS+NJd0b8L+0cbKjH3C2fXBx4SDXzHfRVO+i2c/pfo/T5/wpUORXogvEFdpIb/xJtcmdKxN7rsu9fguOfuzdSD6y2JmPNUdFGk1R/B4v9Gb/PWKhf0iX1vGra6so2lb0BleJbezAL4+lNwnGcW32n3tp+xneVZRiPYHNsA8wbGBX3c6sMROxpQvLO8QXdRfygbEf072Uy9dyxCYT/XCI1orwTH25gsGx67rkSymXpv2cnw46PndbBlx2s6eIN673oTnQnKEwWFK95Pbltr3Sw9XRlgu3112wXcNsxJrhRm+xq4eyLcMsCNA+ksbyMK3NTBu2OQbqAZKj6J/r3i/c4fN/586Mrz76rGbTD1r7u3EST/ExTUi3Q3n6/z80Rn5Li5tJOZrz2ir6cE593fwtuespujfwwXPR8dUFs0AL72JGTlk1BMUB/OmM57ltaM7eAj4erpbh8XvQDDT2y7jxEnwdefhuN/Cd9539W4x8sJTymZVQDf+wwue8c1NIyBUafGjiV497cuY29tbD19ZAaasmFw/j/h2BvaNlTi6+qzCuNuPzI6jqC5BuY/547z1v+1fcwRBb1dWVRr2Y91gb13rMHEi91z3P+CaKOuvQWQtle0DfoWhRsU/kZoCs/Y4stwPThFnszzrszUe+vpiwdAj06sOAzudYxvkJR6gvr2Bst6M/Vlw3Yc1G9bDZ/c3zYI9gZqfcfDwEOjf0emh03EG9R7B1p3ZrBsoqBw5eTktv3gNrfo1T2HdDzuIN6mBLX/gUbYvLDt9fHevsk1CgJN8MbPOh8Yxwf121Ymd47srLWfxc4GVrcpdm2CZKyPy9Sv/XT31dW/8XN45zfw7wu6NnZDpB0K6kVSlbeuftuq6AC9XmPdQNz0TPjKgy4Yv+rj2KAwXkxN/XKXWfaW3ow9zwW0o051g33ba7SMPBmO/hHk94Qhx7Yts/HOG59b6uqmvfs66x5XLx/x0V3w2k8SZ+p3JL6uPr7XxBiY4Fn+4tWfwCs/jAZ38asDQ3SRMG9d/YIXoxnDtEw3RzhAZl5sb8C0v7Xd3+x/09poiljyhsuwxs+64/PFZuo3zHaP+T+XtD+f/copboB0eyUkoRAsfdsFF4+eCfce4coYYjL1A9zaChHJZOqbqqP16Rl57rX3ZurbW+CsTabe2xBY5XncQfjnWfDaj+GfX3a9JRExQf2BsfX83vnqA82uB+XDO13tfGS2mLRMONCzHkRngvpEmfrVH7tjvfcH+G1PeOFq9/i9GrfD1Fvd7/6Gzs/EEj9INqKjEpxN88MN97CNc5ObJcgrvoQFXLZ+V0tUu59srwi4qWzjGx/B5mhP564U9MPCl93vNuTGLcmuU7nYrTD++f/GOqQK6kVSVb8Jba8bcAhc8LgL6CE8heOotuUn8XJLoxnoQCN8fHf0H3ZaZufGB0SkpcO+58ReF1+LHy+rAL7+n9gpK6ffD/Oei/5dkkRQn18OPUbF7jfeAd+IPmd/vRt4F3HIFbFBOSQO6r2Z/XFfjV0E7KDLo7MarZwCz1zqAvHZT7l/7J8/4dl5uGHTtN0N6qyJC+ohtqZ+3rNufv75z8ELV7TNsM54xM1a9NGdbhakha/E3h5ohn9/FZ74iisDWDnFZXxfuNqVbkQUD4idGWnbqo7rr71Z+sI+7hz0Zuo/uc8tGBZfHtImqPe8z9tWR59jxfRotr9qebQXpH5rtPGXluVWVvVm6ldNhTWfuEbAM9+G13/qeoL+5gn8++wf+7navCC20dDuc94QXVQuLROKwlPGBhphyp9h8i2uBOjzx2NngwL49EFo8QzIXRT3XnXEm6n3nvMdzYDz7s20aVR613xIRnymHnZPSUlXSp3WfhZ9vBvn0Oa5w+4pwamY7noHIxa+rHUFktW4zfViJivohyfOg7n/cd+TXZmmNsUoqBdJVQW93QDT7GJXtnLpW3Dpm7FlC53RyzPLiXeg6ohJHQfj7Rl3fuzf3hlr2pORDec9CqNOj17X2Uw9xJbgxJffgKuP/9ZLsaVH4BaZOup6OOk30cGWuWXR372DZSOLaqVnu0HSXsUDYhtD855xQdPz34E7Dohmn7OL4MBvRO+36L9xmfpwQ6G9npYV78dmRz+5H16+ltagJdDoViP++G8uOx8MwLOXwtI32+7LXx87pqBooBuA7F3Q69lv73gO/tq40htw4ysiq/naIEz+Izx4AtSG5/pvaYjtBSgb7l7zyCJjLbXR9ROWxM108uk/3OUGT5a+936uYVs6FHrtF34dmty0o09+PTZwDniChP4Hu/MiMjVtoMk1HDrizRz3PxiGnxD9e8qfY+/7wW3RDHlLg+sx8No0b8fzzK/6AP44xM2aVL81NlM//qLo7zsK6td+Fvs+Ryx4MTxlasCV7wSa294nomZDdK2G9Ozo9as/Sm6xr0S2Lod7j4SHT2t/hid/U+IehVUfuCDO3+h6SLzB3/wX3Pl2/3Gw7O3Y0hvvd9LumL1n2Vuxf29d5rLJ7VnyBvyul3vsHS2W1xlN1e51SGZl7e4QDOzcwnmLX4O/jobbxyU/xmbuf2L/d3Q0hfEXgIJ6kVQ26Xfw09Wutjx+RdvOOukmyO/d9vqx53V9n/0mxJZRJJq7P5G0dDjjtsTBeDKZenCZeBP+iks0BgDc4OHvTHZZ9ohjf+p6LrKL4JJX4ZifwoXPRXs/2jwHA+fcH1vCFHHU9bHzrUfUeAa67neuG08RsejVuGx3OMAsHRp9PhBdMAxcja6/0WXAX/PObhQpbbKuZvq2feDhU6Pd/+CmIz3se20fI7iGCbgxAhELX3b7aO8fdE3cIFlwc/x/5/3YcpgNs12A7W+C12+IZqvzyt1rbEziuvrFceUdy99xdejeWYYi402MgfMeiZaBNdfEjm/wvoYQnUaz99jodclk91ZNif4+5CgYekz79wXXI7LoVZj5T2jY0vb2SM9K5RKY9UR0IHcoCC9d46aYXT8T3vx5tAbfpMVOLbt5QXQ7cNngDXNc4+7Fq6PX73tO9H0JBdwCavcdBX871JVk1bWzNoE3S9/3wOhrboMucPY3dn7e+ld/DJvmuulzX/p+4gz22k+jM3yVDY8upNdS5xq3fz/KjU946sLo9h/eHn1s798SWzZ00GWx+/a+Zu0J+tsG2IFmmPlYx1NjLn277XWLXnb7fPFquPvgaK9DKAj//ZFrXK6f6RbL2hktDe4Yt+4LtwyE+4+Bv4xyvTbJPO+uCvrdd8Zt+7g1ASLvS12l+3vyn3Z8rjTXwss/cK9D/WZXNteRUND1QnrNeXrXPs+9gIJ6EXH6TXCz9ow8NXpddpGrk+8qY+Dkm11wPuSYtrPm7EhBLzfo1suX0f7UmfH6T4SrP4UrPoSRk9q/X1a+G6z7rVdc6c+hV0ZvKxkEx/3MzWoUET9O4KSbYgf4evUeC9fMggv+BWf9zQX5mXGlQAd+w81UFGnAVK9xZTMRkeebXw4n/tq9T5N+5/YbGStQtRzumuAy4K3P/yD4/gyXOY6o3RBbN3zY9+Dsv7v9RrLxXpHrjvlpdB0AcAHdfUdFAxRrXZa1bnPb8puI0iFw8X9h0s3Rxsm6z+AfJ7ngNuL4n0fHWcQE9StdBntLgqzmM5fE1oeP8ZRv9RgB33yxbW/ThEvgh/PhwIvc4+k5Gkae4m6LCeo95UiVi13pTnwQ5C3bGHI0DPaMC4k45ZZor1CgEZ78mmvMRHhXDV70ihsLcf8x8OJV8PhXXJAy//nYngPvuIweI91YlsgYCBt0DRJrXe/P/ce69+yNn0UHlZo0N1PTwZdH9zn/uegUmVuXuuA4UcbeG9T3OSD62oHrCbq5twscvQ3IHdkwJ3b2p6VvJl6Izlt6M/goGHps9O9nvh2ddnXZ266UbeO82LEWaz+Nndd+xKToKtMhv3u975roXq9EGfSqlfC3w9xzi/QSWevGt7z0PfjHJKhINEUq7rOxKUEjceHLrkdn1uPu/H7uu65RtPRN930Q8emD0XKwysWulK8zg3sjx/AmFYLNbgrfew5xjcmOSoH8TW6AejJlaRErp0S/dz77hys9bKl35X+f/QPeuxne/nX720+9NXYhwDlPdbyGxIIX2o6baK6JTn+6/nP3uqfiSsg7kGA0mIj8z8rrAV/7t/vSXfyaWwgrI2fn9jn6dLhhtQvUOjsr0EGXuscSyawVDwRf2o638eoxvOP7gHtcQxJMwZnI0GMgEntMuBgOv2bH9y/q534iDrnS1VgveAn2PdtlOI1xswdFAlPvwlbeHoAjrnU/Ecf81AVpEDsjzpCjXUMiq8CVGL3zWxcANlZF7zP+W65xYIwbBH3Y92KDzJyS6GxJaelu5eAeI1w2NRRwM/888RU3/mHDnGjw4S1n8A7uBTfg9/DvueO99hN3nTdo3u9c97givEH9oldiV/HN6xl9nbxB2wEXxgZ64ErLLnreDaxt2u4y2qff6h7PWXe755aRGz23vEH9/BfcAOhAIzxyhssUgguUz7rb9RBEavzTc1yjKz3L7SOS5R96nBunMex414hpqo5E2qrVAAAgAElEQVR9fLllrkfhjnFu8GTFdBdMRwYcr/kIPr4nHMS3I1I+13d8tL7/sbPdoO36zYm3OeIa9xkpGeR66eoSlLxUTHPZ1LPujv38erPdffZ36xm8//vYbf0NbpDiJa8mHgPk5V3rIeK1G9xrV9Arep23RCbymY00CoNx4z0+ujtxz15kClCT5kq1Bh8Vbeh457x/5AzXEO0ZbihVrXTXRYLi125wiYqtS6PlTKGAC+6/OyW6inXEMk+Wvu94d36E/K6B5J0Fp26jy8ovi8vqV1e48rPiga7xEGhyn4uvPtb2Ocar2RBb6uXLcOdd5D2vXgNPfcP1ap76p9jvnYrp7v3ZOCc8O5aFgYe5mdAysunQ4ldj/371x640xtsw/Phul4jY98ux961a6c59r5Y6t+DfxARruoAL1Kd4pkguGhj9fpr5qDsvI4v9HfY9l3j6gjBWAzR2yBgzY/z48eNnzNBqcyJ7xLqZLhAKBVxJzZcTzCSzu62Z5gKzEZM631Bpz+LX3eDViIxcNybh1D9HS3/iBf0uaxjJThqfC/SP/lHbxk8w4GbqWf6uG48x8dux92mph9v2iwb+vcfBFQkGDq76wA36TRQAxvvqE7FZ8whr4b/XxZYTlA6D706OHdS8Zho85OkpKugbrdk//VaXwfNmHYsGhteISFC2Ba4GvXqNyyzv6H2r3+Jei0i9ffm+LmDsaPGtfc+B88IDruc+4zLWxQNdL1AkuNxe4YKrz5+ITlt68h/gsKtcwNjuIkiG1qx8enbslI4AJ/wKjroOpj/QdnXiiPRsN2Xn4CNdw6+HZw2Cafe6wcPgSu5KBseOB5h4KRz7M9cbAHDrPtGG5JUfQ/kYF8Anms0lvzdc9rZrCK6f6UpXAs2usTj6THd+33mga9CAm8I1UpY06nS3orbP57LXtwyMBu/XL3Hn8J8TlL5FXrOsgtiBqV699nPny4KX4OmLEt8nvxeceYc75hs/b7t4Wvm+7rFGxhdEHHMDHHdj7HVPXQQLw4tmnfRbl8GOr7GPyC5q2/gD14hoqIo9Fy99q+Pyy5evjdaU9x4Ll7/nGjWzHnNZcm+DPy3TNUCHn+g+g+3N0HPo1XDK7xPfFmGtO1e8Y23ak1kA33kvel4GWuA/F0cbTN7zvvc413BK9Dme9YTrcQE3A9d33nOlZKEEJT4mDa6aFm247QUmTJjAzJkzZ1prO2gJt6WgvgMK6kX2Ams+cZnB/b/WfsD2RTDrCffPetARLnuYTBasYjr8+2tuHMCZd8Kgw7p+/Pf+4HoRwK26214GsK7SrbjrLZfIzG+7CNJ3p0KfcYn3EfS7gasr3ncNmG+/kfi+z14OcxOsbHz9YlfD/N7vwlcYuPiVtguPddWCl1xA4V0RGVzAg2m7cvOISa6Myztta0OVu3+i9SH8jbD0LReUjjrNBSfT/h7bWwKu58MbcIHrHardGPu6fOMZ19vTXOfGKqz6IPrYM3JdI+7wa2Kz3l7WuiAzt8xlr62FF66C2Z6xCuk5rvfsgG/AvYdFr/vZWheggwvY0zJdwP/A8dGGS3uMz40XiZRKDDnalan901PStv/X3Bib/14fnTWqxyj4Xrik4/7jolN4Hn6Ny7rHDwSPLIK22pPpP+BC+PI9Lnh84lyXOR5+ops16a1fuYHjiaRlufcrvmHly4gGjr50F7g3Vrn3ut94VxceaWBcNc19dl/29PRl5LnvN+9AeXAZ7HUz256LEUOOhm/toMxpy1JXXhPZ/sLnYgdzN1S5WZk+e5iEswLFMLH3id9XvHUz4YHj3O+ZBa5x5P3sTLgEVrwXHTOTluka3QW9YMXk2AbZN54Jl4OFX/fL3oX+cXHvrMfD4zHCDcTDv+96JJ/+livJSWTEyfCNBN8xTTWuUdhdiZskKajfhRTUi8heL/I9vrP/fJpq4F/nu/r48x+FQYe3f99QyE3RuHW5K0kadKSref/4LleaMuRoN4vRjh5T0O+65sv3bb9UqnYT3H0QNHuyln0PdANv67e4gK56jVtZ+NgbEu+jq2Y/5RovrUGMcZn40mHw9Ddd2U1umStX2O8rO//6b6+A2/eL/j3mTNfzcv+x0WAxPRuuneMCo7smRgOk6xbFjmEIhVxA3bDVDbbOzOv84/E3uQWSVrwXd4MnsOt/kMvCJ7JiMjx+TnQNjWREgsRXf+yms43wZu/BjQU4JlzCtX6WKzEbdBgceb0buPzPuHEuR//YlQD92zOY+LS/xI4n8Fr9ETx+btvAPi0LvvYv2LKsbQPs3Ifhk7/HrnabSGF/+OE8d/7+dWQ0AD3zDvdZfiVu4b4Ln3MlRu0FpQDffCnxAO3qtW5wdaQBPuRod99E5+r6Wa6kKNHj3+fLrrHVc5Q79yMzbuX3gsvfdbNzNde5Rte8Z914nDNvd2U7kR6f/b/myo4ig/lHnOzKPTfNd72x8Y0kr3EXwDn3uYZmpGG3z1lwyh/deKPNC10viHdcUdlw14uRW+p6KB87O3pb3wPDsyCFz+OLnne9E+Deg/nPu56rk34TO1HAbqCgfhdSUC8i/3Os3e3ZqR2KLynxBvBNNS7Y68xqzJ3x2cNuYTKsCyAOvcJd729yM530Htu9vUdPXegG8JUMccFSbqkLit4N90gcepUbAwBuQZ3Jf3SzNx33s+57DF6hkGt4Tb4l8UxAB10Gp/+17fURMx51QaoNufKpgYe6MSbp2W5qTW+DwVtSEQq67byDqCMOuNAFjGkZiY9prZsFxzso9ZpZUDwY7p4YHWz83SluPEB7Kqa7DHYw4N7j/HJXhtT3APe6/PNL0XKpkae4Fbe3LIG/H9m2vt9rwsUugAc3Y9UHt8P4b8Jpf3YN3bsnRFdLLh3mVt+umOZmkIkYe557DWeFe9P6H+SC/4xcV/u+aiosedPNJOTNrF/+bsfjG7Yuh2XvuEDYBt10vd4Gfl0l3Ht47FiNHqNcSZ63ZGjMl1wPTGTg9fmPuYbq/Ofc+JgJF0fHbC172828FBkPElE8EMae7xpw6VmwdgY8eHzsfXzpbRuOvcfChc9Hy8VCIXj8bNczOPoMtzDjqz9ymX1wg6XPvMM1Ht+4MTpLVm6Zm3Ahr2zHr1k3UlC/CymoFxHZw0JBV8qx4XP3D/yKD6F89O47fuUSlyn3ruWwq7Q0uMZC3wOjZTuhEMx4yM3/fvg17Y+x2JWsdcH9+7fEDm4++37Y/6vtbwcuW2xDLnsb31hc8b6b0rB2g9vXgINij/nmL9wgyohjf+bq1TtqdM5+MtzLgitluzg8TejaGW4djkGHw/G/2PE+OlK70ZV6YOBLd0VLmxa96hazyy6KrquxZpqbeSevJ1zy39hB4KGQGzMQERmPAW6V7QPD03M+drZrBPUY5XpHmqrhrvE7bkB4jf+me5zdYenbbqB8stKy4CcrEpeiedVVulmxata7cQK99ot9r611aw7saOrQAYfC15+CnOLY60Mh1xCJLOhXuxHuHN9+mRW48SAX/Kttmc8upKB+F1JQLyKyF2iocrNg9JsAo0/b04/mf5e1bmasmY+6APX0W3dtI8NaN8Bz0SsuuN337A43AVwA997v3NSgp/4x+fUtdqX44H1HVrzvsvYjTope11LvatT7TXCLwkHbMqV4xudmgxr3VZfxTvb4yVjwkpt+d/2saKa8ZIibOSd+5p6Rp7hAuztsX+O+C9Z/7kp3WmpdL1C/8a68aPw3k5+17YPb2plO07jxIyf8X3Tq4N1EQf0upKBeRERE9kqN2+GFK11g3VTjss75vV1wO+QoV7fe3uDo7tLS4I7vS3frg4SCrlRo3WfR+5x5J0z4Vvv76KpQyA3Q72oJXCjkyoGWv+fWNaha7noHzrht5xd07KKdCeo1T72IiIhIKsopdoNNIzrTG9BdMnNh8BHRv31pbqD9fUe7gdoZuTDq1Pa33xk+386NafH5YOy57gdcb0hG7t41pqgTFNSLiIiIfBHs7oC+PUX93TS1nz7oViXPL9/Tjyg5XZklai+ioF5EREREulePEW48g+w2e0mTTkREREREukpBvYiIiIhIilNQLyIiIiKS4hTUi4iIiIikOAX1IiIiIiIpTkG9iIiIiEiKU1AvIiIiIpLiFNSLiIiIiKQ4BfUiIiIiIilOQb2IiIiISIpTUC8iIiIikuIU1IuIiIiIpDgF9SIiIiIiKU5BvYiIiIhIilNQLyIiIiKS4hTUi4iIiIikuG4L6o0x/Y0xDxlj1htjmo0xq4wxtxtjSjqxj5OMMX81xrxjjNlqjLHGmA+S2G4fY8zTxpjNxpgmY8xiY8xNxpicnXtWIiIiIiJ7v/Tu2IkxZhjwEVAOvAgsAg4GrgVOMcYcYa3dmsSurgbOApqAZUBpEsc+BHgXyACeASqA44H/A04wxpxgrW3u9JMSEREREUkR3ZWp/xsuoL/GWvtla+1PrbXHA7cBo4Cbk9zPH4H9gHzgzI7ubIxJAx4GcoFzrbVft9beABwCPAscAfyws09GRERERCSV7HRQH87STwJWAffE3fwroB64yBiT19G+rLUfW2vnW2uDSR7+GGAMMMVa+5JnPyHgJ+E/rzDGmCT3JyIiIiKScrojU39c+PLNcDDdylpbC3yIy6Qf2g3Hind8+PL1+BustSuAJcAgYOguOLaIiIiIyF6hO2rqR4Uvl7Rz+1JcJn8k8E43HK+zxx4Z/lm+ox0ZY2a0c9Porj00EREREZHdozsy9UXhy+p2bo9cX9wNx9qbji0iIiIislfoltlvvgistRMSXR/O4I/fzQ9HRERERCRp3ZGpj2TDi9q5PXL99m441t50bBERERGRvUJ3BPWLw5cj27l9RPiyvbr3VD22iIiIiMheoTuC+vfCl5OMMTH7M8YU4OaKbwCmdcOx4r0bvjwl/gZjzFBcsL8aWLELji0iIiIislfY6aDeWrsceBMYjFsR1usmIA94zFpbH7nSGDPaGNMds8pMBhYCRxtjvuTZvw+3kBXA3621thuOJSIiIiKyV+qugbJXAR8BdxpjTsAF2ofg5rBfAvw87v4Lw5cxi0IZY44ELgv/mR++HGGMeSRyH2vtxZ7fg8aYS3AZ+2eMMc8Aa4ATgIm4OfJv28nnJiIiIiKyV+uWoN5au9wYMxH4Da4U5jRgA3AHcJO1dluSuxoOfCvuuvK46y6OO/YnxpiDcL0Ck4ACXMnNb4BbrLXNnXs2IiIiIiKppdumtLTWVgCXJHlf0871jwCPdOHYC4DzOrudiIiIiMgXQXcMlBURERERkT1IQb2IiIiISIpTUC8iIiIikuIU1IuIiIiIpDgF9SIiIiIiKU5BvYiIiIhIilNQLyIiIiKS4hTUi4iIiIikOAX1IiIiIiIpTkG9iIiIiEiKU1AvIiIiIpLiFNSLiIiIiKQ4BfUiIiIiIilOQb2IiIiISIpTUC8iIiIikuIU1IuIiIiIpDgF9SIiIiIiKU5BvYiIiIhIilNQLyIiIiKS4hTUi4iIiIikOAX1IiIiIiIpTkG9iIiIiEiKU1AvIiIiIpLiFNSLiIiIiKQ4BfUiIiIiIilOQb2IiIiISIpTUC8iIiIikuIU1IuIiIiIpDgF9SIiIiIiKU5BvYiIiIhIilNQLyIiIiKS4hTUi4iIiIikOAX1IiIiIiIpTkG9iIiIiEiKU1AvIiIiIpLiFNSLiIiIiKQ4BfUiIiIiIilOQb2IiIiISIpTUC8iIiIikuK6Lag3xvQ3xjxkjFlvjGk2xqwyxtxujCnp5H5Kw9utCu9nfXi//du5/ypjjG3nZ2P3PDsRERERkb1XenfsxBgzDPgIKAdeBBYBBwPXAqcYY46w1m5NYj9l4f2MBN4FngRGA5cApxtjDrPWrkiwaTVwe4Lr67rwdEREREREUkq3BPXA33AB/TXW2rsiVxpjbgV+CNwMXJHEfn6PC+hvtdZe79nPNcAd4eOckmC77dbaX3f50YuIiIiIpLCdLr8JZ+knAauAe+Ju/hVQD1xkjMnrYD/5wEXh+/867ua7gdXAycaYoTv7mEVEREREvki6o6b+uPDlm9bakPcGa20t8CGQCxzawX4OBXKAD8PbefcTAt6IO55XljHmQmPMjcaYa40xxxlj0jr7REREREREUlF3lN+MCl8uaef2pbhM/kjgnZ3cD+H9xOsNPBZ33UpjzCXW2sk7OGYrY8yMdm4ancz2IiIiIiJ7Sndk6ovCl9Xt3B65vngX7edh4ARcYJ8HjAXuAwYDrxlj9u/guCIiIiIiKa27BsruMdbam+KumgdcYYypA67H1eefncR+JiS6PpzBH7+TD1NEREREZJfpjkx9JINe1M7tkeu376b9RPw9fHl0kvcXEREREUlJ3RHULw5fJqp1BxgRvmyvVr679xNRGb7c4aw7IiIiIiKprjuC+vfCl5OMMTH7M8YUAEcADcC0DvYzDWgEjghv592PDzfY1nu8jkRm20m0WJWIiIiIyBfGTgf11trlwJu4galXx918Ey5T/pi1tj5ypTFmtDEmZlYZa20dbgabPNrOU/+98P7f8K4oa4wZk2j+e2PMYNzc9gCPd/IpiYiIiIiklO4aKHsV8BFwpzHmBGAhcAhuTvklwM/j7r8wfGnirr8ROBa4zhhzADAdGAOcBWymbaPhq8D1xpgpuMWpaoFhwOlANvAq8JedfG4iIiIiInu1bgnqrbXLjTETgd8ApwCnARuAO4CbrLXbktzPVmPMYbiVaL8MHAVsxU1b+X/W2rVxm7yHm9/+QFyZTx5uIO0HuKz/Y9Zau5NPT0RERERkr9ZtU1paayuAS5K8b3yG3ntbFXBt+Kej/UwGklpcSkRERETki6o7BsqKiIiIiMgepKBeRERERCTFKagXEREREUlxCupFRERERFKcgnoRERERkRSnoF5EREREJMUpqBcRERERSXEK6kVEREREUpyCehERERGRFKegXkREREQkxSmoFxERERFJcQrqRURERERSnIJ6EREREZEUp6BeRERERCTFKagXEREREUlxCupFRERERFKcgnoRERERkRSnoF5EREREJMUpqBcRERERSXEK6kVEREREUpyCehERERGRFKegXkREREQkxSmoFxERERFJcQrqRURERERSnIJ6EREREZEUp6BeRERERCTFKagXEREREUlxCupFRERERFKcgnoRERERkRSnoF5EREREJMUpqBcRERERSXEK6kVEREREUpyCehERERGRFKegXkREREQkxSmoFxERERFJcQrqRURERERSnIJ6ERGRFNcSCFFZ27ynH4aI7EEK6qXb1DUHaAmE9vTDEBH5n9LkD3LB/R9z0M1vc+tbS/b0wxGRPURB/V6qpsnPz5+fy+bapk5td897yzjqT+9yz3vLsNbuokfX1jMz1jLu129wyu1T2FDduNuOKyLyv+75WeuYuWY7AHe/u5Rlm2uT3va9RZt5+tMKAsGuJ2Sstbv1/42IJNZtQb0xpr8x5iFjzHpjTLMxZpUx5nZjTEkn91Ma3m5VeD/rw/vtv6uPvTf50+uLeOKTNZz418k8/WlFUl+YL36+jj+/sZiKqkb+/MZifvPKgt3yRbuppolfvTiPkIUVW+q58vGZNAeCu/y4EYFgiCb/7jueiOw6Tf4gP3hyFqfdMZWPl2/d0w9nrxcMWR6YsqL175Al6Wz9i5+v45JHPuUnz87h58/P69LxH5y6gpG/eI3LHv2Mxpbkv4efnL6GSx6ezpQllV06bqprbAl2+v9zQ0uA1+dtYEudyqySZa1lyabaTp2bqaxbgnpjzDBgBnAJMB24DVgBXAt8bIwpS3I/ZcDH4e2Wh/czPbzfGcaYobvq2HuTlVvqeeKTNQDUNAX4ybNz+MaDn3DPe8u49/3l/OODlTw/ay3vL95MRVUDAMs21/Gz5+bG7OfhD1fxyxfnEQol/8Uxa802zv/7x3zl3o94f/HmpLb5w6sLqfd8YD6v2M5vXl6Q9DEBapv8/PmNRXz7kU+ZujT5L/kpSyo57JZ3Oeh3b/P0pxWdOmZEY0uQ6SurqGnyx1y/ZmsDsyu2J/ziDYUsj01bzdVPzOTVuRt2W5bKWsuDU1dwxC3v8r1/zaSqvqXT+1hRWceVj8/g9Dun8uLn67r02DfXNPGPD1Z2OuiasqSSHzw5i/98VkGwE+elV31zgLrmQKe321zTtNsbf03+IHPWbmfxxtounyMVVQ08MGUFn1ds7+ZHt+u9OncDVzw2g9fnbUjq/qGQ5YdPfc4Ln69nwYYaLn30U+as3T3POxiy/PXNxRxxy7vc8tqiLp+fDS2BNt8lu9JbCzaxYkt9zHWvzt3I3LXVO9xua10zv35pfuvfT31WwZvzN3bq2JOXVPK7/y7EH7S8s2gzVz0xA38HGX9rLbe9tYSfPjeX9xZXcumjn3Y5sK9p8tPQ0vnvgojpK6v4/asLmbYi+e+xiqoGbnhmDhfc/zEfLtvS6WOGQpZb31rCAb95k1Nun8qquPeuPZW1zZx6x1SueHwmx//l/aT/P8erbw5Q3bD7zk9wpbnXPfU5B9/8NvdPWb7b/l+GQpbrn57NpNumcOKtk1m2uW63HHdPMt3x4hpj3gAmAddYa+/yXH8r8EPgPmvtFUns5z7gO8Ct1trrPddfA9wBvGGtPWVXHHsHj2nG+PHjx8+YMaOru+iSD5Zu4WfPz6GiquNSlrH9iqhrDrAy/OWQkWbwB6Pva+/CbE4b24eDh5QQDIE/GMIfDBEIWayFXoVZ9C/J5YXP13Hf5OV4/5edOKacIT3y+HDZVlZvrWds/yK+dvBATtmvN1npaUxfWcX5932c8HFdeOhADhxQQr+SHIIhS3MgSF5mOmP6FlKYnQG4wV2vzdvA7/67sHWQlzHw/eOGc+2JI1m2uY5pK7bSEgiRn51OQXY6I3sVMKxnPs/MqODG5+fF/PO99oQRXHbUEN5asImPl28lGLKkpxlaAiFWbm1gZWUdPp/hoMGlHDKklMUba3l17gbqW4KU5GZw01n7MWmfXtz29hIemLKCkIXRvQu4/KihnLl/XzLTfWyqaeL6p2fzgecLffzAYn5w4kj6leSQnZHG0k21fLKyisUbaxlYmsuxo3py6NAysjPSAPePbeGGWj5ctoX0NMOhQ8sY1asAY2BjTROba5opyE6nNC+TwuwMfD5DXXOAG56Zw3/nRgOkfsU53HvheAIhyzsLN9HsD3HauD6MH1iCtZZPVlbx+ryNZKX7GFaeT0VVA/dNXkGL5x/vcaN68p2jh7G5tol12xtJM4aC7AwKstPpX5LDoLI8SnIzMMYQCIZ4bNpqbn1zCbXhwPrSI4fw01NHk5HmcgSBYIiapgA1jX5K8zNb3+tX5qznmn/Paj2/xvQp5MbTRnP4sB6k+Uyb86c5EGRLXQu9CrJIT/PRHAjywJQV3P3eMoIhy9cPHsj3TxhBj/ysHX08WLWlnlteW8Tr8zdSlJPBjaeN5vyJAzDGtL4XW+tbWLetEWNgeHk+uZnprdv7gyFXovBZBUs319G/JIdRvQrZf0ARk/bpTU5mWszxapr8PPTBSl6ft5Glm+taz88Dw+fI0SN6tB57RyINx1teW0RjuDHynaOHcv2kkWSlp7W5v7WW5ZV1zF9fQ3FuJiN75dO7MDupY3k1+YPMWL2Nj5ZvYdaa7fQsyOK7Rw9jn76FndrPczPXct3Ts1v//s7RQ7nhlNGk+QxV9S1kpfvIy0qP2eb3ry7kfk/WGaBHfibPXnk4g8rykj72tvoW5qyrpk9RNiN7FXR4f9c78Dmve4La08f14bbzDyAzPbncV3WDn79NXsYjH64iZC2XHzWUa04Y0fqZ9x5re4OfXoVZre9NfXOA6SuryM9OZ/zAkoSfh0SstZxz70fMCpfeZGf4aPK7z/YRw8v45mGDmbeumsLsDI4a2SP8HeP2fc2/Z/HS7PUx+yvLy+SNHx7d4WcKYGN1E6fdObVNYuHsA/vx1/P2x5fgOVhr+cubi7nnveUx1+dkpPHE5YdQlpfJrDXbCVnLkB55DO2ZT1FORpv9+IMh7npnKfdOXk5Gmo9fnrEPFxw0IOlzPRAMcfvbS7nn/WVEQqCT9+3FjaeNafc8q6xt5oGpK3jkw1Wt359pPsP/nbEP3zxsUFLHbvIH+dF/ZvPKnOh3eO/CbP51+SEM6ZHHzDXb2FjdzIRBJfQuym69T0NLgAvun8YcT0PNGPjRpFFM2qcX2RlplOZlxnyequpbWLihhr7FOQwuy6W60c+97y/nkY9WEQhZvn3EYH5w4sg2n8FE5q2r5p2Fm8nNTOOU/XozoDS3w20iNtc0cckjnzJ/fU3rdZceOYRfnD6mw9ds/vpqbn97KWu2NrBP30ImDCph4uASRpYXtJ5fgWCIqvoWinMz23xW//rmYu56d1nr3z3yM3n8skMY3btz32W724QJE5g5c+ZMa+2Ezm6700F9OFO+DFgFDLPWhjy3FQAbAAOUW2vbbZIaY/KBzUAI6GOtrfXc5sNl3weFj7GiO4/dwfPbI0E9uAzy7W8v4YGpK0g2aZSV7uPZKw/ngakrePHz9R1v0EWF2ens07eQtdsaWbvNNTxOG9ubNJ+Pl2d3fNwBpTkEg5aNNU3tPreCrPTWwDFebmYaDe10p6X5TJezbADFuRlsT5DJSPMZyguyqGsKtPu4diQz3Uffomx6FmSxobqp9XXzHrfZH2oN4CJ8BkpyMwlZy7YkMyyHDi2l0R9idjdld7MzfORlpmMhYe/A+IHF9C/J5fOK7awJ9x4BpPsMXz6wH/v3L+LXLy9I+L7kZKQxuk8BfYqy8QctTf4gFVUNrKlqIGQhPyudiYNLWFPVwIrK2I9xbmYap43tw+CyXIpzM1mwoYaZq7exsaaJ8oIsehZkMX1lVUwjF+DgwaX0L8lh0cZaVmypaw2EwP2zHFyWR2FOBv5AiE01TWxtp0ekODeDCw4ayFbn0ekAABnFSURBVOHDygiGLIs21nLflOUJz5+IktwMcjLS3PlQnMOQHnkMKsulJDeTopwMmgIhlm2u44Olla110l6jexdwyJBSjDH4gyFqmwJUN/qZt666zeMsyE5n//7FjB9UwtAeedQ2u8bWqi31LNlcR0VVA+UFWezbt4g+RdnMWL2NGWu2tRnwbgycP2EAx4zqSV1TgIaWAOlpPjLTfWSGLzPSfPQuzGZ0nwI+WVHFxQ9PJxD3fu/bt5CaJj8VVY2k+wwnjCnnvAkDCIQsU5ZW8q9wDyW48z6yeXlBFgcNLqVnQRYbq5tYsqmW9dWNDC7LY//+xQztmUejP0hNY4BZFduYXbG9ddtRvQr40gF9GV6eT3FOBo3+IDPXbGfWmm00tgTpVZjNmqoG5q5rm9k+ZIg75vz1NWyta8ZasLjG9MFDSjlgQDHbGlpYuaWel2evp6Yp9nthUFkulx05hNK8LAKhEG8u2MS7CzfT6A/SpyibE8aU0+wP8d+5G1q/z3rkZ3HSPuU0tARZuqmO7Q0tjOxdwLj+xRRmp7Nscx0rKuspzEmnf0kuj3y0CoDMNB+PXHIQF/7jk3a/U/sUZXPQ4FLKC7J48IOVrdcXZqe3PvYjhpdx8eFDGFiay7LNdXy2uoqKqgZK8zLpXZhNaV4mGek+npu5jhmrt7ljp/tizpmCrHTKC7Moy8siJzON7AwfW+taWF5ZF/Md5n2PjYFE4UheZhpl+e6zPKI8n5G9Cnh+1ro279cZ4/pw9XHDCYYs9c0BKrY1sqaqgar6ZnzG4AsHkD5jmL12e+tjj9cjP5PeRdn0Lsymd1E2xTmZfLJyK5+t3pbw8UWOPXFQCT0LsqlvDrCxpokN1U1sCl/WNPrJyUyjoTnA+uq24+R65GeRk+lrTeIZAwcNKuXokT0oy8/i9XkbmdxBb4Yx7lw/YEAxKyrr+Wx1VetrW5KbQSBo2/zf6luUzdcOHogx0OQPsWJLHUs21VFV38KAkhyG9sxn6eZa5q2ridlu/MBiDhtWxsheBQwqyyNkLS0BlyxsCbif7Y1+ttQ28+SnFazb3jY5eca4PhwwoJhAyLK1rpn125uorG2mb3E2+/UrYu22Rv758aqE53JBdjrj+hdRVe9n+ea61kZWSW4GA0pzOWZkT3Iz0/nj64vabFucm8G1J4xg375FDCpzjZOWQIg5a6uZsqSST1dXUd8cIBiCYChEQXYGU35y3A5f++62p4P6y4AHgPuttd9NcHskk36itfadHeznROAt4E1r7ckJbo9k8S+z1v6jO4/dwfPbY0F9xMINNbw5fxNNgSAha2lqCVLV4GdzTROz1myPybr+4ZyxfO3ggQRDljveWcrj01Z3ukTj8GFl9CvO4ZmZa9v9EouXneHjneuPpSQ3g/Pv+7jNl0AyyguyGFSWy6erEn/ZtmefPoWU5WcydWnnu0Ij4v8pReyogWAMnDC6nClLtsS8B7vD6eP6MGVxZZcaFwAHDChmTJ9C/j19Tcd3bkdBdjq1TZ0/fr/iHKrqW9o0Xr6ohvTIY922xp06R7r6Wu8J2Rk+rIXm8OfJmz1O1kn79OLyo4Zy0T8+ad3P7jKuf1FMRjRVfHXiAP547jh+9J/ZPDNjbdLbnX1gP84Z34+L/jG9S8f1GXj8skN4efZ6/j09+RLIE0aXc/2kUXzjwWlJJyt2haE98tqUL3XkgAHFhKzdqfPk9LF9eHfR5k5/D/745FG8v3hzp/9P7mlpPsP+/YsSJip2tQmDSliysbZL/y8LstOZ++s2IekutTNBfcf9Lh0bFb5sb2TOUlxgPRLYUWCdzH4I76e7j40xpr2offSOttsdxvQpZEyfxN1F1Y1+3pi/kWnLt3LgoBIuOGgA4D5A1500kmuOH85Hy7fy2ryNVNY2kZHmMmrpaYYMnw+LZf32JtZUNZCV7uOiwwZx4SGD8PkMFx46iBc+X0eaMRw+vIwhPfJ5Zfb6hC3v7x8/gn7FOQA8d+URvDZvA0s21bKisp7K2mYywtm8ytpmlmyqbc3gGQN9CrM584C+fP/4EeRkpHHnO0u5892lWOs+UEeN6EGfohxqm/xsqWthztrq1oFCx43qyV1fH09Wuo+fPTe39Z/Z6N4FnLl/X3oXZuMPhjAGBpbmMbRnHrVNgdbSgpLcTL50QF+G9czjD68tas0U5mWmcePpYzh9bB+e+GQN//pkTcxz7lOUza3nH8Bhw8qoqGrgnveWMWdtNY3+IPXNAXoWZHHwkFLG9iti3roa3lu8ubU8KqIgK51jRvXEAh8t29L6j604N4M+RTnUNwfYVt8S80WUl5nGTWftx7kT+rOiso6rnpjJoo21lOZlcuKYcgJBy4uz17c2RDLTfXxlfD/KC7JZXllHfXOASfv25qsTB+DzGc4Z34+7313GppomBpbmMqA0F4Orgayqb2FNOGPu7RXJyUjje8cP59Ijh/DQhyv5yxuLY7IpxkBhdgbZGT421cQO6BpclstT3z0Ma+GOd5byzsJNbG5nbm1j3GvkzX4WZKXzo5NH0a84hz+/sZjFm5Kb5WPCoJLwP8NKHpi6ok1DrSArnX4lOfiDIVZuqW+THSovyOK8if2ZtE9vNtY0MX99Dc/NXNumtyViQGkOPzxxJCft04uC7Aw2VDfy9/eX8+SnFZ0KUtN8hiuOGcr3jx/Bk9PX8PvXFu1w2tji3AzGDyyhutHPkk21XW4IDOuZxxHDezB+YAkvfL6O9xcnX/PsDeD7FGXzzJWH869PVseUXGSm+dpt5Bw4sJg7LjiA3Mx07v76eK576vNO/zP2GRjdu7BNL0xHfnH6GC49cgi3vbWEOz3d9skaXJbLj04eRW1TgN+/urDd1z8r3dfmPBjWM4/qxkCXBkEaA5cfPQSAn5w8innrqlm5pZ79+hVx4IBiNlQ3MWVpZZvHU5qXyS/P2IfSvEwuPnxwa9a/M35w4kgOH9aDQ4aUkZuZzuPTVu/wHM/JSOO8if35xen7kJnu49FvH8w3H5rO9gY/BVnpjB9UQm5mGisq61m5tb7d8z0zzcd1k0aypqohpocnWT4DPzxxJFcdN5y566q55bWFTF9Z1W4vh8/AxMGlfOuwwZw2tjfNgRA3PDun0z3iGWmGn506hkuOGMwnK6v49iOftn6/FmanM6JXAbPWbEv4OK46dhhXHzecy48ayt/eX8bbCzfR2BKkyR9iQ3Vjm+/hMb0LWV/d2NpzOLRHHj86eRSNLUFufnVh0gm/rHQfp+zXm+pGP1OXbul0T3huZhp/+8Z4jhzeg589N5f/dKLRecTwMi4/aqjrNVq1jc9Wb2vzGSnKyaC2yZ/wNRvVq4BHLjmIVVsauOihT3bYi5rIzvT67wndkam/H7gcuNxa+2CC228GbgRutNb+YQf7uRG4GbjZWvuLBLdfDtyPJyvfXccO37fdoH78+PG5ezJTv7cJhSwV2xpYXum6gXvkZ3HWAX2TrmdsDgRZUVlPdkYafYuzE9YHr6iso7rRz379ilprtSOstWyobqKqvoV9+xbG1EbPX19DbmYaQ3vmd+m5zVi9jVlrtnHq2D6tjZSIJn+QytpmapsCjOyVT3pa58aZVzf4qaxrYnNtM+k+HwcMKG6tAQyFLGu3NVKQnU5JXmbMdq4rs4XtDX4GlubG1OgGQ5Y1VQ0MKMlpfTxrtzXw5PQKsjN8nH/QAMoLstkZ1lrqmgM0+UM0tgQpL8yKeQyfrarilTkbGFyWy4EDSxjTp7D1ec1YXcW976/g7YWbGNWrgIcuOajN61pZ28yCDTXUNPpbyznKC7MY2iOf7Awfa6oa+GRFFVUNLZxzYD/KC7Nbn/snK7ayvLKOtdsaqaxtZmjPPMYPLGFYeT6Vtc2s395IWX4W4wcWt54nizfW8s6iTRRkpTOqdyEje+VTnBt9zZv8QZaFu3Qz03xkZ6QxuCy3zfsdDI9jeH7WOqob/aSn+chO93HMqJ6cN2FAwlrs5kCQ6kY/LQH3Wq6pamDllnrWbmukptHP9kY/PgPDeuYzrDyfQ4eUMbAsWr9aUdXAlKWV+AMhLK68KT87nfysDAaW5jKiPL+11tRad07NXLONmau3saW+hcLsdAqzM+gdrjUfHO5FmLuumo3VjezTt5DDh/WgV2HsOfP+4s08O3Md/vDYlpyMNAIh29rd7g/PQLVkU11r47coJ4Onv3sYo3q7mvZpK7aydFMt+/UrYt++RaypauA/MyqYvLiSgnAt+fhBJZwwujzmta5u9LNoQw0ba1w5Q2leFqN6FdCnOJulm+qYs3Y7G2uayM9yY276FedyxPAyinMzqW8O8NaCTXy0fAtV9e4zFLKWsf2KGD+ohJ4FWVTWNlNZ28y4/sUcPKS09bgvz17PjNXbGF6ez9h+RQwszSUtzRAKue8ZN2amhh75WQzpkcfw8nyOGN6j9ftqc20T//6kgrXbGqhtCtAUCDKufzFnjOvD0B55fLZ6G5OXVGItnLJfb/bvX0TIusGbM1ZXUZqXFT43M5i/vobZFdU0BYLu3OiZR2VtM7MqtlNR1cCX9u/LeRMHtPncer+TA8EQ89bXMHftduauq2Z7g58rjx3GgQNLWs/nF2atY87a7SzdXMeaqgb6l+QwYVAJo3sXUt3oZ1NNE9sb/ARCIfxBy759C/nWYYNj6udDIcv2Rj+ba5vYVu+nyR+k0R8kPyud4eVunEd8vX11o5+tdc0MKsuLGU9graWmKdBanrFoYw0LNtSQl5nO1w8Z2JrsenXuBv7xwUqqG/1kpPnIznClbYNKc1vPZWstIQsha/EZwxHDe7Sem97XqLKu2ZXOVDexscaVhAwqy+XEMb0oixtrYK1l8pJKZldUs6nW3TcvM43eRTn0KcqmV2E2fYqyXWllIERDS5AhZXkU5UbHCSzeWMtLs9exX98ijh9TTlZ6GpW1zby5YCOrttRT3eintinA+IElXHbUkHb/z9Y1B5hdsZ1566opzcvkuNHl9MjPwlrLyi31NPqDjOpV0PrZ2lbfwjMz1rKlrpk0nyHdZ+hfmsvIXgWUF2Sxpsr9j89OT+PEMb1aH3NVfQtTl1ayaGMtSzfVsqG6ifQ0H1lpPjLSTWs5XmF2BmX5WZQXZHHKfr3pG/7eD4Usr83byPSVW0nzuQRjUU4G/YpzKM3LZPXWeuauq6a+Jcip+/Xm9LF9Yp6zte5/3sINNZTlZzGyvICi3AyC4TKemWu28eaCTXy4bAu9C7O55xvj6V/ivkMra5t58fN1LNhQw4L1NVTWNuPzGdKMoW9xNkeO6MnRI3rQrySHtPD16T5fzPu1O+zp8psvRFC/g8e1x8tvRFJdkz9IRpov6QGAkro21TSxeGMto/sU7HSDUkTkf82eLr+JFJUVtXN75PqOCqm6sp/uOraI7ELxM4DIF1evwuw2mX4REdn1umOe+sXhy5Ht3D4ifNnRahhd2U93HVtEREREJGV1R1D/XvhyUnjqyVbhaSWPABqAaR3sZxrQCBwR3s67Hx9uwKv3eN15bBERERGRlLXTQb21djnwJjAYuDru5puAPOAx7zzxxpjRxpiYWWWstXXAY+H7/zpuP98L7/+NyBz1XT22iIiIiMgXTXfU1ANcBXwE3GmMOQFYCBwCHIcrffl53P0Xhi/jR83dCBwLXGeMOQCYDowBzsItTBUfuHfl2CIiIiIiXyjdUX4TyZhPBB7BBdTXA8OAO4BDrbVbk9zPVuAw4E5geHg/hwAPAxPCx9klxxYRERERSVXdlanHWlsBXJLkfdud185aWwVcG/7p9mOLiIiIiHzRdEumXkRERERE9hwF9SIiIiIiKU5BvYiIiIhIilNQLyIiIiKS4hTUi4iIiIikOAX1IiIiIiIpTkG9iIiIiEiKU1AvIiIiIpLiFNSLiIiIiKQ4BfUiIiIiIinOWGv39GPYqxljtubk5JSOGTNmTz8UEREREfkCW7hwIY2NjVXW2rLObqugvgPGmJVAIbBqDxx+dPhy0R44tqQGnSOSDJ0nkgydJ5IMnSe71mCgxlo7pLMbKqjfixljZgBYayfs6ccieyedI5IMnSeSDJ0nkgydJ3sv1dSLiIiIiKQ4BfUiIiIiIilOQb2IiIiISIpTUC8iIiIikuIU1IuIiIiIpDjNfiMiIiIikuKUqRcRERERSXEK6kVEREREUpyCehERERGRFKegXkREREQkxSmoFxERERFJcQrq/7+9+42VoyrjOP79KVKkKt5UsWpNrkLxLxCRqLQJtDQiRFDQVvtCrAgJGKmgkpiACPVfTBRFKKFGAyXti5LUIMEUIYFCq1WMEjUNIq1yIdVC7a290l5bLDy+OGftuOxue7d3dmeW3yeZTObMnN2zd5979tnZM3PMzMzMzGrOSb2ZmZmZWc05qa8gSTMk3SLp75L2ShqRdL2koX63zXonv+/RZnmqTZ1ZktZI2iHp35L+KOlySS/tdftt8kiaL+lGSesl/SvHwMoD1JlwLEg6W9IDksYk7ZL0kKRFk/+KrAwTiRNJwx36l5C0qsPzLJL0mxwjYzlmzi7vldlkkTRN0kWS7pC0OfcNY5J+IelCSS3zQvcn9XBYvxtg/0/SMcAG4GjgTuBR4L3AZcCZkmZHxGgfm2i9NQZc36J8V3OBpI8APwH2ALcDO4BzgO8Ds4EF5TXTSvYV4ETS+74FeFung7uJBUmXAjcCo8BK4FlgPrBc0vERccVkvRgrzYTiJPsD8NMW5RtbHSzpu8CX8uP/CDgcWAjcJWlxRCztot3WOwuAm4GtwFrgSeB1wEeBHwNnSVoQhZlJ3Z/USER4qdAC3AMEsLip/Hu5fFm/2+ilZ7EwAowc5LGvArYBe4GTC+VHkL4kBrCw36/JS9exMBeYCQiYk9/PlZMVC8Aw6QN7FBgulA8Bm3OdU/r9d/AyqXEynPcvn8Djz8p1NgNDTY81mmNo+FBeg5fSY+R0UkL+kqby6aQEP4CPFcrdn9Ro8fCbCsln6c8gJXM3Ne2+BtgNnC9pao+bZtU3H3gtsCoiftsojIg9pLN3AJ/tR8Ps0EXE2ojYFPmT8QC6iYXPAFOApRExUqjzT+BbefOSLptvPTLBOOlGIwa+mWOj8bwjpM+sKcAFJT23TYKIuD8i7oqI55vKnwKW5c05hV3uT2rESX21zM3re1v8wz0D/BI4Enh/rxtmfTNF0iclXSnpMklz24xhPD2vf95i3zpgHJglaUppLbWq6CYWOtW5u+kYGyxvkHRx7mMulnRCh2MdJ4PtP3m9r1Dm/qRGPKa+Wt6a14+12b+JdCb/OOC+nrTI+m06sKKp7HFJF0TEg4WytrETEfskPQ68E3gL8KdSWmpV0U0sdKqzVdJuYIakIyNivIQ2W/98IC//I+kBYFFEPFkomwq8EdgVEVtbPM6mvD6upHZaiSQdBnwqbxaTcfcnNeIz9dVyVF6PtdnfKH91D9pi/XcrMI+U2E8Fjgd+SBqveLekEwvHOnasoZtYONg6R7XZb/UzDnwdeA9prPMQcBrp4sk5wH1NQz3dxwy2bwPvAtZExD2FcvcnNeKk3qyiImJJHv/4dESMR8TGiLiEdNH0y4Fr+9tCM6uriNgWEV+NiIcjYmde1pF+DX4IOBa4qL+ttF6Q9HnSHY0eBc7vc3PsEDipr5YDfXttlO/sQVusuhoXM51aKHPsWEM3sXCwddqdebMBERH7SLc2BPcxAy/fevIHwCPA3IjY0XSI+5MacVJfLX/O63ZjEmfmdbsx9/bi8I+8Lv403jZ28ljJN5MufvpruU2zCugmFjrVeT0p1rZ4/OuLxgv6mIjYDfwNeEWOiWb+fKoZSZeT7iW/kZTQt5rU0P1JjTipr5a1eX1G86xukl5JmuRhHPh1rxtmldK4+1GxE70/r89scfyppLsmbYiIvWU2zCqhm1joVOespmNs8LXqY8BxMjAkfZk0edTvSQn9tjaHuj+pESf1FRIRfwHuJV0I+bmm3UtI325X5DMmNsAkvb3VfASShoHGjI3F6d9XA9uBhZJOLhx/BPCNvHlzKY21qukmFm4lTS5zaY6xRp0h4Mq8uQwbGJJOaj55lMvnAV/Imyubdjdi4KocG406w6TPrL2kWLIKk3Q16cLY3wHzImJ7h8Pdn9SIypujwrqRJ6DaABwN3Em6RdT7SPewfwyYFRGj/Wuh9YKka0kXLq0DngCeAY4BPkSayW8NcF5EPFuocy6pA94DrCJN5f1h0u3FVgMfL3FSGitRfm/PzZvTgQ+SzqKuz2XbozDtejexIGkxcANpFsjb2T+t+wzguvC07pU3kTjJt62cSfq82ZL3n8D++4dfHRGNpK34HNcBX8x1VgOHA58AppFmQl/aXMeqQ9IiYDnwHGnoTatx7SMRsbxQx/1JTTipryBJbwK+RvrpahqwFbgDWFKcxc8Gl6TTSDPuvZv9t7TcSfqpdAXpF5sX/PNKmg1cBZxCSv43A7cAN0TEc71pvU22/CXvmg6HPBERw011JhwLks4BrgBOIv2S+whpVsjbDvElWA9MJE4kXQicR7qN4WuAlwFPA78ivefr2z2IpE+Tzsy/A3geeBj4TkT87JBfhJXqIGIE4MGImNNUz/1JDTipNzMzMzOrOY+pNzMzMzOrOSf1ZmZmZmY156TezMzMzKzmnNSbmZmZmdWck3ozMzMzs5pzUm9mZmZmVnNO6s3MzMzMas5JvZmZmZlZzTmpNzMzMzOrOSf1ZmZmZmY156TezMzMzKzmnNSbmZmZmdWck3ozMzMzs5pzUm9mZmZmVnNO6s3MzMzMas5JvZmZmZlZzTmpNzMzMzOruf8CY2S/GMBa298AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 378
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(valid_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "GITWwFbHh1Bg",
    "outputId": "83b56fa7-0a63-4f05-b665-3ef7f0deafae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f22d8aebc90>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 378
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(valid_acc, label='Validation Accuracy')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "tIJKKTIBopSW"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), home+'/FIGA/model_normal.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25mEV-rUXiEz",
    "outputId": "5a2f50dd-9712-4d1d-ae98-914fb0b38ed4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(home+\"/FIGA/model_normal.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "MFnVOnNeh3jX"
   },
   "outputs": [],
   "source": [
    "#To measure accuracy\n",
    "\n",
    "def measure_accuracy(test_loader):\n",
    "  model.eval() # Prep model for Evaluation\n",
    "  correct_total = 0.\n",
    "  total = 0.\n",
    "\n",
    "  mean_of = 5 # Mean of how many evaluations\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  for i in range(mean_of):\n",
    "    for data, target in test_loader:\n",
    "      # Move the data to device\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      # forward pass: compute predicted outputs by passing inputs to the model\n",
    "      output = model(data)\n",
    "      # calculate the loss\n",
    "      loss = criterion(output, target)\n",
    "      # update test loss \n",
    "      valid_loss += loss.item()*data.size(0)\n",
    "      # convert output probabilities to predicted class\n",
    "      _, pred = torch.max(output, 1)\n",
    "      # compare predictions to true label\n",
    "      correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "      for i in range(len(target)):\n",
    "        if len(target) == 1:\n",
    "          correct_total += correct.item()\n",
    "        else:\n",
    "          correct_total += correct[i].item()\n",
    "        total += 1\n",
    "\n",
    "  # calculate and print average test loss\n",
    "  valid_loss = valid_loss/(mean_of * len(test_loader.dataset))\n",
    "  print('Test Loss: {:.6f}\\n'.format(valid_loss))\n",
    "\n",
    "\n",
    "  #print total accuracy of the model\n",
    "  print('Test Accuracy (Overall): %0.2f%% (%2d/%2d)' % (\n",
    "      (correct_total/total)*100,\n",
    "      np.sum(correct_total), np.sum(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKm_Y9dCTrab",
    "outputId": "ad047db6-c070-4e12-b27f-f9c619d3f06b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.118334\n",
      "\n",
      "Test Accuracy (Overall): 95.74% (42440/44330)\n"
     ]
    }
   ],
   "source": [
    "#Normal Samples Accuracy (test_loader is the loader from original data)\n",
    "measure_accuracy(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "zneFoKysoXJ_"
   },
   "outputs": [],
   "source": [
    "#FIGA IMPLEMENTATION\n",
    "\n",
    "def check_sign(phis, legi, feature_no):\n",
    "    '''\n",
    "    phis - Phishing Site Data\n",
    "    legi - Legitimate Site Data\n",
    "    feature_no - The feature for which sign needs to be checked\n",
    "    '''\n",
    "    if (phis.iloc[:,feature_no].mean() - legi.iloc[:,feature_no].mean())>=0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def featureImportance(X, y, n):\n",
    "    '''\n",
    "    X - Data\n",
    "    y - Labels\n",
    "    n - The number of features to perturb\n",
    "    '''\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    dtc.fit(X, y)\n",
    "    importance = dtc.feature_importances_\n",
    "    df_impor = pd.DataFrame({'Feature':np.arange(0,importance.size), 'Importance':importance},columns=['Feature', 'Importance'])\n",
    "    df_impor.sort_values(by=['Importance'], ascending=False,inplace=True)\n",
    "    f = df_impor.iloc[:n,0]\n",
    "    phis = X[y==1]\n",
    "    legi = X[y==0]\n",
    "    d = [check_sign(phis, legi, x) for x in f]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X.values)\n",
    "    return f.to_numpy(), d, scaler\n",
    "    \n",
    "def perturbation(f, d, x, scaler, e = 0.05):\n",
    "    '''\n",
    "    X - Data\n",
    "    f - feature_importance; A ranked feature vector\n",
    "    d - Signed attack direction vector\n",
    "    x - Sample to be perturbated\n",
    "    scaler - Scaler used to transform values\n",
    "    e - The total percentage of the input to modify\n",
    "    '''\n",
    "    sample = x\n",
    "    sample = scaler.transform(sample.reshape(1, -1))\n",
    "    E = e*np.sum(sample)/f.size\n",
    "    for i in range(f.size):\n",
    "        temp = sample[0][f[i]] + (E*d[i])\n",
    "        if temp<0:\n",
    "            temp = 0\n",
    "        sample[0][f[i]] = temp\n",
    "    sample = scaler.inverse_transform(sample)\n",
    "    return np.round_(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "q1Yw5onpPAds"
   },
   "outputs": [],
   "source": [
    "#Perturbating Samples\n",
    "test_data_new = test_data\n",
    "X, y = test_data_new.drop('phishing',axis =1 ), test_data_new['phishing']\n",
    "f, d, scaler = featureImportance(X, y, 40)\n",
    "for i in range(test_data_new.shape[0]):\n",
    "  if test_data_new.iloc[i,111] == 1:\n",
    "    X.iloc[i,:] = perturbation( f, d, X.iloc[i,:].to_numpy(), scaler, e=0.5).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "D26YHB01Q5TI"
   },
   "outputs": [],
   "source": [
    "#Creating Dataloader of Perturbated samples \n",
    "train_target = torch.from_numpy(y.values)\n",
    "train = torch.from_numpy(X.values).float()\n",
    "train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "test_loader_new = data_utils.DataLoader(dataset = train_tensor, batch_size = 128, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YEsIKR4-Sekg",
    "outputId": "32cb29a9-1692-4828-81d0-f2aeecafc27e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 17.582987\n",
      "\n",
      "Test Accuracy (Overall): 65.53% (29050/44330)\n"
     ]
    }
   ],
   "source": [
    "#Perturbated Samples Accuracy (test_loader_new is the loader from Perturbated data)\n",
    "measure_accuracy(test_loader_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24CdMcksnbX1"
   },
   "source": [
    "##FIGA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ok_DJSz7nlti"
   },
   "source": [
    "###FIGA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hozpsgy3XthJ"
   },
   "outputs": [],
   "source": [
    "#FIGA IMPLEMENTATION\n",
    "\n",
    "def check_sign(phis, legi, feature_no):\n",
    "    '''\n",
    "    phis - Phishing Site Data\n",
    "    legi - Legitimate Site Data\n",
    "    feature_no - The feature for which sign needs to be checked\n",
    "    '''\n",
    "    if (phis.iloc[:,feature_no].mean() - legi.iloc[:,feature_no].mean())>=0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def featureImportance(X, y, n):\n",
    "    '''\n",
    "    X - Data\n",
    "    y - Labels\n",
    "    n - The number of features to perturb\n",
    "    '''\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    dtc.fit(X, y)\n",
    "    importance = dtc.feature_importances_\n",
    "    df_impor = pd.DataFrame({'Feature':np.arange(0,importance.size), 'Importance':importance},columns=['Feature', 'Importance'])\n",
    "    df_impor.sort_values(by=['Importance'], ascending=False,inplace=True)\n",
    "    f = df_impor.iloc[:n,0]\n",
    "    phis = X[y==1]\n",
    "    legi = X[y==0]\n",
    "    d = [check_sign(phis, legi, x) for x in f]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X.values)\n",
    "    return f.to_numpy(), d, scaler\n",
    "    \n",
    "def perturbation(f, d, x, scaler, e = 0.05):\n",
    "    '''\n",
    "    X - Data\n",
    "    f - feature_importance; A ranked feature vector\n",
    "    d - Signed attack direction vector\n",
    "    x - Sample to be perturbated\n",
    "    scaler - Scaler used to transform values\n",
    "    e - The total percentage of the input to modify\n",
    "    '''\n",
    "    sample = x\n",
    "    sample = scaler.transform(sample.reshape(1, -1))\n",
    "    E = e*np.sum(sample)/f.size\n",
    "    for i in range(f.size):\n",
    "        temp = sample[0][f[i]] + (E*d[i])\n",
    "        if temp<0:\n",
    "            temp = 0\n",
    "        sample[0][f[i]] = temp\n",
    "    sample = scaler.inverse_transform(sample)\n",
    "    return np.round_(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7REzneQVnpIz"
   },
   "source": [
    "###Training Model with perturbated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNfOLkQ2sfDO",
    "outputId": "4b004835-8aad-404b-ce8d-e78c56580484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/FIGA2\n"
     ]
    }
   ],
   "source": [
    "%cd $home/FIGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf6f7bZd4R9Z",
    "outputId": "fc884919-1a29-4668-af54-c6ab5f9d873e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26594/26594 [05:17<00:00, 83.78it/s]\n",
      "100%|██████████| 26594/26594 [05:35<00:00, 79.31it/s]\n",
      "100%|██████████| 26594/26594 [05:19<00:00, 83.26it/s]\n",
      "100%|██████████| 26594/26594 [05:13<00:00, 84.85it/s]\n",
      "100%|██████████| 26594/26594 [05:13<00:00, 84.96it/s] \n"
     ]
    }
   ],
   "source": [
    "#Creating new dataset with perturbated samples with different combinations of n and e\n",
    "from tqdm import tqdm\n",
    "data = pd.read_csv(home+\"/FIGA/dataset_full.csv\")\n",
    "total_data = data\n",
    "for n_f, e in [(20,0.2),(15, 0.7),(25,0.1),(5,0.05),(10,0.02)]:\n",
    "  data_to_perturb = data\n",
    "  data_to_perturb = data.sample(frac=1).reset_index(drop=True)\n",
    "  data_to_perturb = data_to_perturb.iloc[:int(data.shape[0]*0.3),:]\n",
    "  X, y = data_to_perturb.drop('phishing', axis = 1 ), data_to_perturb['phishing']\n",
    "  f, d, scaler = featureImportance(X, y , n_f)\n",
    "  for i in tqdm(range(X.shape[0])):\n",
    "    if y.iloc[i] == 1:\n",
    "      X.iloc[i,:] = perturbation( f, d, X.iloc[i,:].to_numpy(), scaler, e).ravel()\n",
    "  X['phishing'] = y\n",
    "  total_data = total_data.append(X)\n",
    "  del data_to_perturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "0NIVr_RL-QX0"
   },
   "outputs": [],
   "source": [
    "#Saving the new dataset\n",
    "total_data.to_csv(home+'/FIGA2/dataset_full_new.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "VxTbPfct714a"
   },
   "outputs": [],
   "source": [
    "total_data = pd.read_csv(home+\"/FIGA2/dataset_full_new.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "YBfgmc8F9lWV",
    "outputId": "a7f0b1c2-1076-4eee-a2fb-79cf7e21815d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-472978ea-de0e-41fe-bc90-a93c81271a8a\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qty_dot_url</th>\n",
       "      <th>qty_hyphen_url</th>\n",
       "      <th>qty_underline_url</th>\n",
       "      <th>qty_slash_url</th>\n",
       "      <th>qty_questionmark_url</th>\n",
       "      <th>qty_equal_url</th>\n",
       "      <th>qty_at_url</th>\n",
       "      <th>qty_and_url</th>\n",
       "      <th>qty_exclamation_url</th>\n",
       "      <th>qty_space_url</th>\n",
       "      <th>qty_tilde_url</th>\n",
       "      <th>qty_comma_url</th>\n",
       "      <th>qty_plus_url</th>\n",
       "      <th>qty_asterisk_url</th>\n",
       "      <th>qty_hashtag_url</th>\n",
       "      <th>qty_dollar_url</th>\n",
       "      <th>qty_percent_url</th>\n",
       "      <th>qty_tld_url</th>\n",
       "      <th>length_url</th>\n",
       "      <th>qty_dot_domain</th>\n",
       "      <th>qty_hyphen_domain</th>\n",
       "      <th>qty_underline_domain</th>\n",
       "      <th>qty_slash_domain</th>\n",
       "      <th>qty_questionmark_domain</th>\n",
       "      <th>qty_equal_domain</th>\n",
       "      <th>qty_at_domain</th>\n",
       "      <th>qty_and_domain</th>\n",
       "      <th>qty_exclamation_domain</th>\n",
       "      <th>qty_space_domain</th>\n",
       "      <th>qty_tilde_domain</th>\n",
       "      <th>qty_comma_domain</th>\n",
       "      <th>qty_plus_domain</th>\n",
       "      <th>qty_asterisk_domain</th>\n",
       "      <th>qty_hashtag_domain</th>\n",
       "      <th>qty_dollar_domain</th>\n",
       "      <th>qty_percent_domain</th>\n",
       "      <th>qty_vowels_domain</th>\n",
       "      <th>domain_length</th>\n",
       "      <th>domain_in_ip</th>\n",
       "      <th>server_client_domain</th>\n",
       "      <th>...</th>\n",
       "      <th>qty_hashtag_file</th>\n",
       "      <th>qty_dollar_file</th>\n",
       "      <th>qty_percent_file</th>\n",
       "      <th>file_length</th>\n",
       "      <th>qty_dot_params</th>\n",
       "      <th>qty_hyphen_params</th>\n",
       "      <th>qty_underline_params</th>\n",
       "      <th>qty_slash_params</th>\n",
       "      <th>qty_questionmark_params</th>\n",
       "      <th>qty_equal_params</th>\n",
       "      <th>qty_at_params</th>\n",
       "      <th>qty_and_params</th>\n",
       "      <th>qty_exclamation_params</th>\n",
       "      <th>qty_space_params</th>\n",
       "      <th>qty_tilde_params</th>\n",
       "      <th>qty_comma_params</th>\n",
       "      <th>qty_plus_params</th>\n",
       "      <th>qty_asterisk_params</th>\n",
       "      <th>qty_hashtag_params</th>\n",
       "      <th>qty_dollar_params</th>\n",
       "      <th>qty_percent_params</th>\n",
       "      <th>params_length</th>\n",
       "      <th>tld_present_params</th>\n",
       "      <th>qty_params</th>\n",
       "      <th>email_in_url</th>\n",
       "      <th>time_response</th>\n",
       "      <th>domain_spf</th>\n",
       "      <th>asn_ip</th>\n",
       "      <th>time_domain_activation</th>\n",
       "      <th>time_domain_expiration</th>\n",
       "      <th>qty_ip_resolved</th>\n",
       "      <th>qty_nameservers</th>\n",
       "      <th>qty_mx_servers</th>\n",
       "      <th>ttl_hostname</th>\n",
       "      <th>tls_ssl_certificate</th>\n",
       "      <th>qty_redirects</th>\n",
       "      <th>url_google_index</th>\n",
       "      <th>domain_google_index</th>\n",
       "      <th>url_shortened</th>\n",
       "      <th>phishing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.207316</td>\n",
       "      <td>0</td>\n",
       "      <td>60781</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>223</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.499566</td>\n",
       "      <td>-1</td>\n",
       "      <td>36024</td>\n",
       "      <td>579</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9540</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.935901</td>\n",
       "      <td>0</td>\n",
       "      <td>4766</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>589</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.410021</td>\n",
       "      <td>0</td>\n",
       "      <td>20454</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.410761</td>\n",
       "      <td>0</td>\n",
       "      <td>53831</td>\n",
       "      <td>6998</td>\n",
       "      <td>306</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3597</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221612</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>28733</td>\n",
       "      <td>3243</td>\n",
       "      <td>381</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>27553</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221613</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4057</td>\n",
       "      <td>333</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>16996</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221614</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.320962</td>\n",
       "      <td>0</td>\n",
       "      <td>60781</td>\n",
       "      <td>6368</td>\n",
       "      <td>205</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1689</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221615</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>704</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3598</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221616</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666315</td>\n",
       "      <td>0</td>\n",
       "      <td>29222</td>\n",
       "      <td>6790</td>\n",
       "      <td>1245</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3596</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221617 rows × 112 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-472978ea-de0e-41fe-bc90-a93c81271a8a')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-472978ea-de0e-41fe-bc90-a93c81271a8a button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-472978ea-de0e-41fe-bc90-a93c81271a8a');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "        qty_dot_url  qty_hyphen_url  ...  url_shortened  phishing\n",
       "0                 3               0  ...              0         1\n",
       "1                 5               0  ...              0         1\n",
       "2                 2               0  ...              0         0\n",
       "3                 4               0  ...              0         1\n",
       "4                 2               0  ...              0         0\n",
       "...             ...             ...  ...            ...       ...\n",
       "221612            4               1  ...              0         1\n",
       "221613            2               0  ...              0         1\n",
       "221614            2               0  ...              0         0\n",
       "221615            2               0  ...              0         0\n",
       "221616            2               0  ...              0         0\n",
       "\n",
       "[221617 rows x 112 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "-JfGZpywCuIH"
   },
   "outputs": [],
   "source": [
    "dataset_length = total_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "D0FHYsn9C0mY"
   },
   "outputs": [],
   "source": [
    "#Scaling the new dataset\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(total_data.drop('phishing', axis = 1)), columns=total_data.drop('phishing', axis = 1).columns)\n",
    "\n",
    "df_scaled['phishing'] = total_data['phishing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "yCk4G8eNBXXk"
   },
   "outputs": [],
   "source": [
    "#Saving Sampler Pickle File \n",
    "import pickle\n",
    "scalerfile = home+'/FIGA/scaler.pkl'\n",
    "pickle.dump(scaler, open(scalerfile, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "DHsMxKoDBw99"
   },
   "outputs": [],
   "source": [
    "#Saving Model pickle file\n",
    "import pickle\n",
    "modelfile = home+'/FIGA/model_FIGA_Trained.pkl'\n",
    "pickle.dump(model, open(modelfile, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "pBzrWuJpB7Ix"
   },
   "outputs": [],
   "source": [
    "#Loading Sampler and Model\n",
    "import pickle\n",
    "scalerfile = home+'/FIGA/scaler.pkl'\n",
    "scaler = pickle.load(open(scalerfile, 'rb'))\n",
    "modelfile = home+'/FIGA/model_FIGA_Trained.pkl'\n",
    "model = pickle.load(open(modelfile, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "Jzult7m3jIgC"
   },
   "outputs": [],
   "source": [
    "#Scaling dataset using recently loading sampler\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(total_data.drop('phishing', axis = 1)), columns=total_data.drop('phishing', axis = 1).columns)\n",
    "df_scaled['phishing'] = total_data['phishing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "BhAzEoSf9gmP",
    "outputId": "2f98c1b2-e136-4a3f-b196-cc243b91620e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-4bd68acc-bbab-4d55-bfbf-b69644e7122a\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qty_dot_url</th>\n",
       "      <th>qty_hyphen_url</th>\n",
       "      <th>qty_underline_url</th>\n",
       "      <th>qty_slash_url</th>\n",
       "      <th>qty_questionmark_url</th>\n",
       "      <th>qty_equal_url</th>\n",
       "      <th>qty_at_url</th>\n",
       "      <th>qty_and_url</th>\n",
       "      <th>qty_exclamation_url</th>\n",
       "      <th>qty_space_url</th>\n",
       "      <th>qty_tilde_url</th>\n",
       "      <th>qty_comma_url</th>\n",
       "      <th>qty_plus_url</th>\n",
       "      <th>qty_asterisk_url</th>\n",
       "      <th>qty_hashtag_url</th>\n",
       "      <th>qty_dollar_url</th>\n",
       "      <th>qty_percent_url</th>\n",
       "      <th>qty_tld_url</th>\n",
       "      <th>length_url</th>\n",
       "      <th>qty_dot_domain</th>\n",
       "      <th>qty_hyphen_domain</th>\n",
       "      <th>qty_underline_domain</th>\n",
       "      <th>qty_slash_domain</th>\n",
       "      <th>qty_questionmark_domain</th>\n",
       "      <th>qty_equal_domain</th>\n",
       "      <th>qty_at_domain</th>\n",
       "      <th>qty_and_domain</th>\n",
       "      <th>qty_exclamation_domain</th>\n",
       "      <th>qty_space_domain</th>\n",
       "      <th>qty_tilde_domain</th>\n",
       "      <th>qty_comma_domain</th>\n",
       "      <th>qty_plus_domain</th>\n",
       "      <th>qty_asterisk_domain</th>\n",
       "      <th>qty_hashtag_domain</th>\n",
       "      <th>qty_dollar_domain</th>\n",
       "      <th>qty_percent_domain</th>\n",
       "      <th>qty_vowels_domain</th>\n",
       "      <th>domain_length</th>\n",
       "      <th>domain_in_ip</th>\n",
       "      <th>server_client_domain</th>\n",
       "      <th>...</th>\n",
       "      <th>qty_hashtag_file</th>\n",
       "      <th>qty_dollar_file</th>\n",
       "      <th>qty_percent_file</th>\n",
       "      <th>file_length</th>\n",
       "      <th>qty_dot_params</th>\n",
       "      <th>qty_hyphen_params</th>\n",
       "      <th>qty_underline_params</th>\n",
       "      <th>qty_slash_params</th>\n",
       "      <th>qty_questionmark_params</th>\n",
       "      <th>qty_equal_params</th>\n",
       "      <th>qty_at_params</th>\n",
       "      <th>qty_and_params</th>\n",
       "      <th>qty_exclamation_params</th>\n",
       "      <th>qty_space_params</th>\n",
       "      <th>qty_tilde_params</th>\n",
       "      <th>qty_comma_params</th>\n",
       "      <th>qty_plus_params</th>\n",
       "      <th>qty_asterisk_params</th>\n",
       "      <th>qty_hashtag_params</th>\n",
       "      <th>qty_dollar_params</th>\n",
       "      <th>qty_percent_params</th>\n",
       "      <th>params_length</th>\n",
       "      <th>tld_present_params</th>\n",
       "      <th>qty_params</th>\n",
       "      <th>email_in_url</th>\n",
       "      <th>time_response</th>\n",
       "      <th>domain_spf</th>\n",
       "      <th>asn_ip</th>\n",
       "      <th>time_domain_activation</th>\n",
       "      <th>time_domain_expiration</th>\n",
       "      <th>qty_ip_resolved</th>\n",
       "      <th>qty_nameservers</th>\n",
       "      <th>qty_mx_servers</th>\n",
       "      <th>ttl_hostname</th>\n",
       "      <th>tls_ssl_certificate</th>\n",
       "      <th>qty_redirects</th>\n",
       "      <th>url_google_index</th>\n",
       "      <th>domain_google_index</th>\n",
       "      <th>url_shortened</th>\n",
       "      <th>phishing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.209351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.685511</td>\n",
       "      <td>-0.639835</td>\n",
       "      <td>-0.421846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.191844</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.692308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.187129</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.749943</td>\n",
       "      <td>-0.547566</td>\n",
       "      <td>-0.137476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.307692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.779083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.431298</td>\n",
       "      <td>-0.639835</td>\n",
       "      <td>-0.421846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.213333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.769231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.161552</td>\n",
       "      <td>-0.639835</td>\n",
       "      <td>-0.421846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.234397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.422871</td>\n",
       "      <td>0.473592</td>\n",
       "      <td>0.156309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221612</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.490607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474416</td>\n",
       "      <td>-0.123767</td>\n",
       "      <td>0.297552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.699007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221613</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.490607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.458091</td>\n",
       "      <td>-0.586701</td>\n",
       "      <td>-0.227872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.950284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221614</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.055174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.685511</td>\n",
       "      <td>0.373369</td>\n",
       "      <td>-0.033898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.135319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221615</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.230769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.847256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.611443</td>\n",
       "      <td>-0.527681</td>\n",
       "      <td>-0.372881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221616</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.492895</td>\n",
       "      <td>0.440503</td>\n",
       "      <td>1.924670</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221617 rows × 112 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4bd68acc-bbab-4d55-bfbf-b69644e7122a')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-4bd68acc-bbab-4d55-bfbf-b69644e7122a button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-4bd68acc-bbab-4d55-bfbf-b69644e7122a');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "        qty_dot_url  qty_hyphen_url  ...  url_shortened  phishing\n",
       "0               1.0             0.0  ...            0.0         1\n",
       "1               3.0             0.0  ...            0.0         1\n",
       "2               0.0             0.0  ...            0.0         0\n",
       "3               2.0             0.0  ...            0.0         1\n",
       "4               0.0             0.0  ...            0.0         0\n",
       "...             ...             ...  ...            ...       ...\n",
       "221612          2.0             1.0  ...            0.0         1\n",
       "221613          0.0             0.0  ...            0.0         1\n",
       "221614          0.0             0.0  ...            0.0         0\n",
       "221615          0.0             0.0  ...            0.0         0\n",
       "221616          0.0             0.0  ...            0.0         0\n",
       "\n",
       "[221617 rows x 112 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bv9BLGB8d19",
    "outputId": "ec6f2873-f469-41a6-ad80-40a1a3529065"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but RobustScaler was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(scaler.transform(np.array(total_data.iloc[4,:111]).reshape(1,-1)).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EO0KU_eI-2GB",
    "outputId": "8e707f98-5b02-43bb-a9f3-1d779ca64ce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "output = model(a.float().unsqueeze(0))\n",
    "_, pred = torch.max(output, 1)\n",
    "print(pred.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wr4-rj_ADetM",
    "outputId": "29fc6f52-bd52-44c2-d6ed-29c999669103"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221617, 112)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "cMhmLe1GCn0S"
   },
   "outputs": [],
   "source": [
    "#Creating DataLoader of the new dataset\n",
    "train_data = df_scaled.iloc[:int(dataset_length*0.7)]\n",
    "val_data = df_scaled.iloc[int(dataset_length*0.7):int(dataset_length*0.85)]\n",
    "test_data = df_scaled.iloc[int(dataset_length*0.85):]\n",
    "\n",
    "train_target = torch.from_numpy(train_data['phishing'].values)\n",
    "train = torch.from_numpy(train_data.drop('phishing', axis = 1).values,).float()\n",
    "train_tensor = data_utils.TensorDataset(train, train_target)\n",
    "train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = 512, shuffle = True)\n",
    "\n",
    "train_target = torch.from_numpy(val_data['phishing'].values)\n",
    "train = torch.from_numpy(val_data.drop('phishing', axis = 1).values).float()\n",
    "train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "valid_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = 512, shuffle = True)\n",
    "\n",
    "train_target = torch.from_numpy(test_data['phishing'].values)\n",
    "train = torch.from_numpy(test_data.drop('phishing', axis = 1).values).float()\n",
    "train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "test_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = 512, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "E4DO5RO5fm_F"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear_layers = nn.Sequential(\n",
    "                        nn.Linear(111,50),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.2),\n",
    "                        nn.Linear(50,200),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.2),\n",
    "                        nn.Linear(200,100),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.4),\n",
    "                        nn.Linear(100,20),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.8),\n",
    "                        nn.Linear(20,10),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.4),\n",
    "                        nn.Linear(10,2)\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "C3lT5Xp1DE5b"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer  = optim.SGD(model.parameters(),lr = 0.1)\n",
    "scheduler = StepLR(optimizer, step_size=8, gamma=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "D0boqGiaDK8T",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4cce3ded-e56c-439a-e67c-87a6c3c564dc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40.. Train loss: 0.069.. Validation loss: 0.676.. Validation accuracy: 0.659.. LR : [0.1]\n",
      "valid loss decreased (inf --> 0.675920).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.065.. Validation loss: 0.647.. Validation accuracy: 0.659.. LR : [0.1]\n",
      "valid loss decreased (0.675920 --> 0.646704).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.061.. Validation loss: 0.634.. Validation accuracy: 0.659.. LR : [0.1]\n",
      "valid loss decreased (0.646704 --> 0.633835).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.064.. Validation loss: 0.620.. Validation accuracy: 0.660.. LR : [0.1]\n",
      "valid loss decreased (0.633835 --> 0.620248).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.057.. Validation loss: 0.582.. Validation accuracy: 0.691.. LR : [0.1]\n",
      "valid loss decreased (0.620248 --> 0.581658).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.046.. Validation loss: 0.519.. Validation accuracy: 0.722.. LR : [0.1]\n",
      "valid loss decreased (0.581658 --> 0.518770).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.040.. Validation loss: 0.462.. Validation accuracy: 0.758.. LR : [0.1]\n",
      "valid loss decreased (0.518770 --> 0.462369).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.034.. Validation loss: 0.420.. Validation accuracy: 0.784.. LR : [0.1]\n",
      "valid loss decreased (0.462369 --> 0.420314).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.031.. Validation loss: 0.381.. Validation accuracy: 0.832.. LR : [0.1]\n",
      "valid loss decreased (0.420314 --> 0.381237).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.027.. Validation loss: 0.364.. Validation accuracy: 0.819.. LR : [0.1]\n",
      "valid loss decreased (0.381237 --> 0.363719).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.023.. Validation loss: 0.338.. Validation accuracy: 0.832.. LR : [0.1]\n",
      "valid loss decreased (0.363719 --> 0.338382).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.025.. Validation loss: 0.325.. Validation accuracy: 0.845.. LR : [0.1]\n",
      "valid loss decreased (0.338382 --> 0.325375).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.022.. Validation loss: 0.341.. Validation accuracy: 0.815.. LR : [0.1]\n",
      "Epoch 1/40.. Train loss: 0.021.. Validation loss: 0.356.. Validation accuracy: 0.801.. LR : [0.1]\n",
      "Epoch 1/40.. Train loss: 0.021.. Validation loss: 0.296.. Validation accuracy: 0.876.. LR : [0.1]\n",
      "valid loss decreased (0.325375 --> 0.295948).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.024.. Validation loss: 0.293.. Validation accuracy: 0.876.. LR : [0.1]\n",
      "valid loss decreased (0.295948 --> 0.292673).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.020.. Validation loss: 0.304.. Validation accuracy: 0.833.. LR : [0.1]\n",
      "Epoch 1/40.. Train loss: 0.022.. Validation loss: 0.325.. Validation accuracy: 0.824.. LR : [0.1]\n",
      "Epoch 1/40.. Train loss: 0.033.. Validation loss: 0.373.. Validation accuracy: 0.813.. LR : [0.1]\n",
      "Epoch 1/40.. Train loss: 0.021.. Validation loss: 0.338.. Validation accuracy: 0.819.. LR : [0.1]\n",
      "Epoch 1/40.. Train loss: 0.017.. Validation loss: 0.292.. Validation accuracy: 0.846.. LR : [0.1]\n",
      "valid loss decreased (0.292673 --> 0.291516).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.024.. Validation loss: 0.300.. Validation accuracy: 0.880.. LR : [0.1]\n",
      "Epoch 1/40.. Train loss: 0.023.. Validation loss: 0.311.. Validation accuracy: 0.836.. LR : [0.1]\n",
      "Epoch 1/40.. Train loss: 0.020.. Validation loss: 0.311.. Validation accuracy: 0.838.. LR : [0.1]\n",
      "Epoch 1/40.. Train loss: 0.017.. Validation loss: 0.272.. Validation accuracy: 0.866.. LR : [0.1]\n",
      "valid loss decreased (0.291516 --> 0.272465).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.018.. Validation loss: 0.278.. Validation accuracy: 0.859.. LR : [0.1]\n",
      "Epoch 1/40.. Train loss: 0.017.. Validation loss: 0.271.. Validation accuracy: 0.869.. LR : [0.1]\n",
      "valid loss decreased (0.272465 --> 0.271210).  Saving model ...\n",
      "Epoch 1/40.. Train loss: 0.016.. Validation loss: 0.299.. Validation accuracy: 0.849.. LR : [0.1]\n",
      "Epoch 1/40.. Train loss: 0.016.. Validation loss: 0.272.. Validation accuracy: 0.885.. LR : [0.1]\n",
      "Epoch 1/40.. Train loss: 0.015.. Validation loss: 0.269.. Validation accuracy: 0.871.. LR : [0.1]\n",
      "valid loss decreased (0.271210 --> 0.269063).  Saving model ...\n",
      "Epoch 2/40.. Train loss: 0.066.. Validation loss: 0.342.. Validation accuracy: 0.815.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.016.. Validation loss: 0.280.. Validation accuracy: 0.871.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.016.. Validation loss: 0.321.. Validation accuracy: 0.842.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.017.. Validation loss: 0.245.. Validation accuracy: 0.894.. LR : [0.1]\n",
      "valid loss decreased (0.269063 --> 0.244568).  Saving model ...\n",
      "Epoch 2/40.. Train loss: 0.016.. Validation loss: 0.236.. Validation accuracy: 0.897.. LR : [0.1]\n",
      "valid loss decreased (0.244568 --> 0.235617).  Saving model ...\n",
      "Epoch 2/40.. Train loss: 0.018.. Validation loss: 0.309.. Validation accuracy: 0.860.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.020.. Validation loss: 0.247.. Validation accuracy: 0.893.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.017.. Validation loss: 0.240.. Validation accuracy: 0.896.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.015.. Validation loss: 0.238.. Validation accuracy: 0.896.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.015.. Validation loss: 0.251.. Validation accuracy: 0.887.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.015.. Validation loss: 0.310.. Validation accuracy: 0.856.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.016.. Validation loss: 0.225.. Validation accuracy: 0.902.. LR : [0.1]\n",
      "valid loss decreased (0.235617 --> 0.225436).  Saving model ...\n",
      "Epoch 2/40.. Train loss: 0.026.. Validation loss: 0.356.. Validation accuracy: 0.883.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.016.. Validation loss: 0.234.. Validation accuracy: 0.898.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.016.. Validation loss: 0.290.. Validation accuracy: 0.867.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.014.. Validation loss: 0.248.. Validation accuracy: 0.889.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.017.. Validation loss: 0.267.. Validation accuracy: 0.882.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.013.. Validation loss: 0.233.. Validation accuracy: 0.898.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.014.. Validation loss: 0.300.. Validation accuracy: 0.869.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.019.. Validation loss: 0.329.. Validation accuracy: 0.868.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.017.. Validation loss: 0.232.. Validation accuracy: 0.900.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.014.. Validation loss: 0.256.. Validation accuracy: 0.887.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.032.. Validation loss: 0.453.. Validation accuracy: 0.815.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.016.. Validation loss: 0.315.. Validation accuracy: 0.867.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.014.. Validation loss: 0.286.. Validation accuracy: 0.874.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.017.. Validation loss: 0.231.. Validation accuracy: 0.901.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.016.. Validation loss: 0.240.. Validation accuracy: 0.894.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.016.. Validation loss: 0.286.. Validation accuracy: 0.874.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.017.. Validation loss: 0.331.. Validation accuracy: 0.855.. LR : [0.1]\n",
      "Epoch 2/40.. Train loss: 0.012.. Validation loss: 0.272.. Validation accuracy: 0.880.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.078.. Validation loss: 0.355.. Validation accuracy: 0.831.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.014.. Validation loss: 0.250.. Validation accuracy: 0.893.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.012.. Validation loss: 0.270.. Validation accuracy: 0.886.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.017.. Validation loss: 0.308.. Validation accuracy: 0.872.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.019.. Validation loss: 0.355.. Validation accuracy: 0.855.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.016.. Validation loss: 0.280.. Validation accuracy: 0.881.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.014.. Validation loss: 0.262.. Validation accuracy: 0.888.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.013.. Validation loss: 0.274.. Validation accuracy: 0.884.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.016.. Validation loss: 0.331.. Validation accuracy: 0.866.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.013.. Validation loss: 0.250.. Validation accuracy: 0.895.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.011.. Validation loss: 0.251.. Validation accuracy: 0.890.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.013.. Validation loss: 0.234.. Validation accuracy: 0.897.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.015.. Validation loss: 0.243.. Validation accuracy: 0.891.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.013.. Validation loss: 0.272.. Validation accuracy: 0.884.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.015.. Validation loss: 0.330.. Validation accuracy: 0.863.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.014.. Validation loss: 0.257.. Validation accuracy: 0.892.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.020.. Validation loss: 0.267.. Validation accuracy: 0.889.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.012.. Validation loss: 0.231.. Validation accuracy: 0.900.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.016.. Validation loss: 0.236.. Validation accuracy: 0.898.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.010.. Validation loss: 0.246.. Validation accuracy: 0.897.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.012.. Validation loss: 0.306.. Validation accuracy: 0.879.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.012.. Validation loss: 0.287.. Validation accuracy: 0.884.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.013.. Validation loss: 0.231.. Validation accuracy: 0.903.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.016.. Validation loss: 0.288.. Validation accuracy: 0.884.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.012.. Validation loss: 0.303.. Validation accuracy: 0.881.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.013.. Validation loss: 0.296.. Validation accuracy: 0.881.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.015.. Validation loss: 0.222.. Validation accuracy: 0.903.. LR : [0.1]\n",
      "valid loss decreased (0.225436 --> 0.221898).  Saving model ...\n",
      "Epoch 3/40.. Train loss: 0.011.. Validation loss: 0.341.. Validation accuracy: 0.861.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.013.. Validation loss: 0.233.. Validation accuracy: 0.902.. LR : [0.1]\n",
      "Epoch 3/40.. Train loss: 0.015.. Validation loss: 0.309.. Validation accuracy: 0.874.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.071.. Validation loss: 0.454.. Validation accuracy: 0.841.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.011.. Validation loss: 0.253.. Validation accuracy: 0.892.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.016.. Validation loss: 0.223.. Validation accuracy: 0.904.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.012.. Validation loss: 0.247.. Validation accuracy: 0.895.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.011.. Validation loss: 0.290.. Validation accuracy: 0.887.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.015.. Validation loss: 0.316.. Validation accuracy: 0.877.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.012.. Validation loss: 0.254.. Validation accuracy: 0.892.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.011.. Validation loss: 0.277.. Validation accuracy: 0.885.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.012.. Validation loss: 0.226.. Validation accuracy: 0.898.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.013.. Validation loss: 0.242.. Validation accuracy: 0.895.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.010.. Validation loss: 0.268.. Validation accuracy: 0.893.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.018.. Validation loss: 0.246.. Validation accuracy: 0.889.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.012.. Validation loss: 0.253.. Validation accuracy: 0.894.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.012.. Validation loss: 0.263.. Validation accuracy: 0.891.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.015.. Validation loss: 0.252.. Validation accuracy: 0.888.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.013.. Validation loss: 0.255.. Validation accuracy: 0.894.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.014.. Validation loss: 0.230.. Validation accuracy: 0.898.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.015.. Validation loss: 0.262.. Validation accuracy: 0.887.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.013.. Validation loss: 0.312.. Validation accuracy: 0.883.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.013.. Validation loss: 0.257.. Validation accuracy: 0.894.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.012.. Validation loss: 0.224.. Validation accuracy: 0.902.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.018.. Validation loss: 0.251.. Validation accuracy: 0.889.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.016.. Validation loss: 0.322.. Validation accuracy: 0.875.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.009.. Validation loss: 0.274.. Validation accuracy: 0.888.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.008.. Validation loss: 0.246.. Validation accuracy: 0.896.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.014.. Validation loss: 0.249.. Validation accuracy: 0.895.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.010.. Validation loss: 0.220.. Validation accuracy: 0.904.. LR : [0.1]\n",
      "valid loss decreased (0.221898 --> 0.220350).  Saving model ...\n",
      "Epoch 4/40.. Train loss: 0.015.. Validation loss: 0.264.. Validation accuracy: 0.894.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.016.. Validation loss: 0.413.. Validation accuracy: 0.865.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.010.. Validation loss: 0.288.. Validation accuracy: 0.889.. LR : [0.1]\n",
      "Epoch 4/40.. Train loss: 0.012.. Validation loss: 0.335.. Validation accuracy: 0.877.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.049.. Validation loss: 0.314.. Validation accuracy: 0.879.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.010.. Validation loss: 0.237.. Validation accuracy: 0.901.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.014.. Validation loss: 0.227.. Validation accuracy: 0.898.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.011.. Validation loss: 0.250.. Validation accuracy: 0.898.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.014.. Validation loss: 0.229.. Validation accuracy: 0.898.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.016.. Validation loss: 0.348.. Validation accuracy: 0.880.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.015.. Validation loss: 0.210.. Validation accuracy: 0.906.. LR : [0.1]\n",
      "valid loss decreased (0.220350 --> 0.210436).  Saving model ...\n",
      "Epoch 5/40.. Train loss: 0.016.. Validation loss: 0.244.. Validation accuracy: 0.901.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.015.. Validation loss: 0.265.. Validation accuracy: 0.900.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.010.. Validation loss: 0.251.. Validation accuracy: 0.897.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.011.. Validation loss: 0.235.. Validation accuracy: 0.903.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.011.. Validation loss: 0.323.. Validation accuracy: 0.875.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.010.. Validation loss: 0.307.. Validation accuracy: 0.885.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.009.. Validation loss: 0.323.. Validation accuracy: 0.887.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.009.. Validation loss: 0.264.. Validation accuracy: 0.895.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.010.. Validation loss: 0.227.. Validation accuracy: 0.905.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.014.. Validation loss: 0.324.. Validation accuracy: 0.885.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.012.. Validation loss: 0.312.. Validation accuracy: 0.887.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.015.. Validation loss: 0.358.. Validation accuracy: 0.875.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.015.. Validation loss: 0.247.. Validation accuracy: 0.900.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.016.. Validation loss: 0.418.. Validation accuracy: 0.867.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.011.. Validation loss: 0.221.. Validation accuracy: 0.904.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.008.. Validation loss: 0.267.. Validation accuracy: 0.889.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.011.. Validation loss: 0.245.. Validation accuracy: 0.897.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.014.. Validation loss: 0.234.. Validation accuracy: 0.903.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.012.. Validation loss: 0.287.. Validation accuracy: 0.895.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.015.. Validation loss: 0.310.. Validation accuracy: 0.887.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.009.. Validation loss: 0.288.. Validation accuracy: 0.892.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.010.. Validation loss: 0.211.. Validation accuracy: 0.914.. LR : [0.1]\n",
      "Epoch 5/40.. Train loss: 0.013.. Validation loss: 0.259.. Validation accuracy: 0.895.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.052.. Validation loss: 0.349.. Validation accuracy: 0.858.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.012.. Validation loss: 0.224.. Validation accuracy: 0.902.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.013.. Validation loss: 0.237.. Validation accuracy: 0.903.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.011.. Validation loss: 0.220.. Validation accuracy: 0.906.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.011.. Validation loss: 0.240.. Validation accuracy: 0.902.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.012.. Validation loss: 0.212.. Validation accuracy: 0.907.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.011.. Validation loss: 0.209.. Validation accuracy: 0.909.. LR : [0.1]\n",
      "valid loss decreased (0.210436 --> 0.209005).  Saving model ...\n",
      "Epoch 6/40.. Train loss: 0.014.. Validation loss: 0.215.. Validation accuracy: 0.905.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.013.. Validation loss: 0.300.. Validation accuracy: 0.889.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.010.. Validation loss: 0.230.. Validation accuracy: 0.905.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.014.. Validation loss: 0.221.. Validation accuracy: 0.903.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.009.. Validation loss: 0.232.. Validation accuracy: 0.909.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.010.. Validation loss: 0.263.. Validation accuracy: 0.900.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.016.. Validation loss: 0.299.. Validation accuracy: 0.893.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.015.. Validation loss: 0.252.. Validation accuracy: 0.895.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.011.. Validation loss: 0.211.. Validation accuracy: 0.912.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.011.. Validation loss: 0.234.. Validation accuracy: 0.906.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.012.. Validation loss: 0.223.. Validation accuracy: 0.911.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.009.. Validation loss: 0.251.. Validation accuracy: 0.904.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.013.. Validation loss: 0.272.. Validation accuracy: 0.897.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.013.. Validation loss: 0.235.. Validation accuracy: 0.907.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.010.. Validation loss: 0.241.. Validation accuracy: 0.905.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.011.. Validation loss: 0.220.. Validation accuracy: 0.912.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.008.. Validation loss: 0.209.. Validation accuracy: 0.912.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.009.. Validation loss: 0.243.. Validation accuracy: 0.904.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.014.. Validation loss: 0.215.. Validation accuracy: 0.904.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.009.. Validation loss: 0.261.. Validation accuracy: 0.902.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.010.. Validation loss: 0.294.. Validation accuracy: 0.894.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.015.. Validation loss: 0.253.. Validation accuracy: 0.900.. LR : [0.1]\n",
      "Epoch 6/40.. Train loss: 0.012.. Validation loss: 0.235.. Validation accuracy: 0.909.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.067.. Validation loss: 0.279.. Validation accuracy: 0.893.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.011.. Validation loss: 0.227.. Validation accuracy: 0.908.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.009.. Validation loss: 0.224.. Validation accuracy: 0.911.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.012.. Validation loss: 0.210.. Validation accuracy: 0.913.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.011.. Validation loss: 0.232.. Validation accuracy: 0.908.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.016.. Validation loss: 0.226.. Validation accuracy: 0.902.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.011.. Validation loss: 0.220.. Validation accuracy: 0.913.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.011.. Validation loss: 0.279.. Validation accuracy: 0.891.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.011.. Validation loss: 0.259.. Validation accuracy: 0.902.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.008.. Validation loss: 0.290.. Validation accuracy: 0.900.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.012.. Validation loss: 0.244.. Validation accuracy: 0.909.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.011.. Validation loss: 0.276.. Validation accuracy: 0.903.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.011.. Validation loss: 0.368.. Validation accuracy: 0.879.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.013.. Validation loss: 0.375.. Validation accuracy: 0.878.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.013.. Validation loss: 0.198.. Validation accuracy: 0.911.. LR : [0.1]\n",
      "valid loss decreased (0.209005 --> 0.197957).  Saving model ...\n",
      "Epoch 7/40.. Train loss: 0.010.. Validation loss: 0.233.. Validation accuracy: 0.906.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.011.. Validation loss: 0.227.. Validation accuracy: 0.907.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.012.. Validation loss: 0.280.. Validation accuracy: 0.895.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.013.. Validation loss: 0.256.. Validation accuracy: 0.900.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.009.. Validation loss: 0.224.. Validation accuracy: 0.904.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.010.. Validation loss: 0.265.. Validation accuracy: 0.898.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.010.. Validation loss: 0.219.. Validation accuracy: 0.910.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.014.. Validation loss: 0.261.. Validation accuracy: 0.906.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.012.. Validation loss: 0.350.. Validation accuracy: 0.883.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.011.. Validation loss: 0.310.. Validation accuracy: 0.896.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.018.. Validation loss: 0.416.. Validation accuracy: 0.869.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.008.. Validation loss: 0.223.. Validation accuracy: 0.913.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.015.. Validation loss: 0.216.. Validation accuracy: 0.905.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.010.. Validation loss: 0.229.. Validation accuracy: 0.910.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.009.. Validation loss: 0.236.. Validation accuracy: 0.906.. LR : [0.1]\n",
      "Epoch 7/40.. Train loss: 0.008.. Validation loss: 0.250.. Validation accuracy: 0.906.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.048.. Validation loss: 0.241.. Validation accuracy: 0.903.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.007.. Validation loss: 0.228.. Validation accuracy: 0.903.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.010.. Validation loss: 0.268.. Validation accuracy: 0.902.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.013.. Validation loss: 0.231.. Validation accuracy: 0.910.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.012.. Validation loss: 0.249.. Validation accuracy: 0.908.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.009.. Validation loss: 0.206.. Validation accuracy: 0.920.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.011.. Validation loss: 0.231.. Validation accuracy: 0.916.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.012.. Validation loss: 0.278.. Validation accuracy: 0.902.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.013.. Validation loss: 0.215.. Validation accuracy: 0.912.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.010.. Validation loss: 0.215.. Validation accuracy: 0.916.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.010.. Validation loss: 0.224.. Validation accuracy: 0.913.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.013.. Validation loss: 0.209.. Validation accuracy: 0.906.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.013.. Validation loss: 0.190.. Validation accuracy: 0.918.. LR : [0.1]\n",
      "valid loss decreased (0.197957 --> 0.189729).  Saving model ...\n",
      "Epoch 8/40.. Train loss: 0.013.. Validation loss: 0.226.. Validation accuracy: 0.912.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.011.. Validation loss: 0.207.. Validation accuracy: 0.916.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.013.. Validation loss: 0.273.. Validation accuracy: 0.897.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.011.. Validation loss: 0.213.. Validation accuracy: 0.913.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.008.. Validation loss: 0.230.. Validation accuracy: 0.909.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.010.. Validation loss: 0.263.. Validation accuracy: 0.903.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.011.. Validation loss: 0.233.. Validation accuracy: 0.911.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.010.. Validation loss: 0.227.. Validation accuracy: 0.909.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.010.. Validation loss: 0.230.. Validation accuracy: 0.912.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.011.. Validation loss: 0.277.. Validation accuracy: 0.906.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.010.. Validation loss: 0.237.. Validation accuracy: 0.913.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.010.. Validation loss: 0.256.. Validation accuracy: 0.906.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.012.. Validation loss: 0.238.. Validation accuracy: 0.914.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.009.. Validation loss: 0.287.. Validation accuracy: 0.897.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.008.. Validation loss: 0.217.. Validation accuracy: 0.912.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.009.. Validation loss: 0.214.. Validation accuracy: 0.910.. LR : [0.1]\n",
      "Epoch 8/40.. Train loss: 0.010.. Validation loss: 0.215.. Validation accuracy: 0.915.. LR : [0.1]\n",
      "Epoch 9/40.. Train loss: 0.049.. Validation loss: 0.250.. Validation accuracy: 0.909.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.009.. Validation loss: 0.228.. Validation accuracy: 0.912.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.011.. Validation loss: 0.225.. Validation accuracy: 0.914.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.011.. Validation loss: 0.217.. Validation accuracy: 0.915.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.008.. Validation loss: 0.217.. Validation accuracy: 0.916.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.012.. Validation loss: 0.230.. Validation accuracy: 0.912.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.009.. Validation loss: 0.224.. Validation accuracy: 0.915.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.010.. Validation loss: 0.214.. Validation accuracy: 0.917.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.010.. Validation loss: 0.240.. Validation accuracy: 0.911.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.011.. Validation loss: 0.214.. Validation accuracy: 0.918.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.009.. Validation loss: 0.236.. Validation accuracy: 0.913.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.008.. Validation loss: 0.222.. Validation accuracy: 0.915.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.015.. Validation loss: 0.228.. Validation accuracy: 0.914.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.009.. Validation loss: 0.237.. Validation accuracy: 0.912.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.007.. Validation loss: 0.219.. Validation accuracy: 0.916.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.010.. Validation loss: 0.211.. Validation accuracy: 0.919.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.011.. Validation loss: 0.223.. Validation accuracy: 0.914.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.010.. Validation loss: 0.219.. Validation accuracy: 0.917.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.011.. Validation loss: 0.210.. Validation accuracy: 0.920.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.009.. Validation loss: 0.221.. Validation accuracy: 0.917.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.009.. Validation loss: 0.238.. Validation accuracy: 0.911.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.007.. Validation loss: 0.231.. Validation accuracy: 0.914.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.010.. Validation loss: 0.225.. Validation accuracy: 0.915.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.010.. Validation loss: 0.212.. Validation accuracy: 0.918.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.008.. Validation loss: 0.248.. Validation accuracy: 0.909.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.007.. Validation loss: 0.241.. Validation accuracy: 0.912.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.010.. Validation loss: 0.230.. Validation accuracy: 0.915.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.009.. Validation loss: 0.223.. Validation accuracy: 0.917.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.011.. Validation loss: 0.209.. Validation accuracy: 0.920.. LR : [0.012249999999999999]\n",
      "Epoch 9/40.. Train loss: 0.012.. Validation loss: 0.222.. Validation accuracy: 0.917.. LR : [0.012249999999999999]\n",
      "Epoch 10/40.. Train loss: 0.051.. Validation loss: 0.286.. Validation accuracy: 0.902.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.012.. Validation loss: 0.221.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.007.. Validation loss: 0.225.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.008.. Validation loss: 0.216.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.011.. Validation loss: 0.226.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.012.. Validation loss: 0.225.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.007.. Validation loss: 0.223.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.011.. Validation loss: 0.241.. Validation accuracy: 0.910.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.011.. Validation loss: 0.228.. Validation accuracy: 0.915.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.009.. Validation loss: 0.228.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.008.. Validation loss: 0.213.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.010.. Validation loss: 0.223.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.009.. Validation loss: 0.217.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.008.. Validation loss: 0.223.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.010.. Validation loss: 0.214.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.011.. Validation loss: 0.233.. Validation accuracy: 0.915.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.007.. Validation loss: 0.217.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.010.. Validation loss: 0.211.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.011.. Validation loss: 0.228.. Validation accuracy: 0.915.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.009.. Validation loss: 0.226.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.008.. Validation loss: 0.253.. Validation accuracy: 0.905.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.009.. Validation loss: 0.227.. Validation accuracy: 0.915.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.006.. Validation loss: 0.220.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.011.. Validation loss: 0.247.. Validation accuracy: 0.912.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.010.. Validation loss: 0.211.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.012.. Validation loss: 0.217.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.009.. Validation loss: 0.219.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.008.. Validation loss: 0.207.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.007.. Validation loss: 0.223.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.011.. Validation loss: 0.226.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 10/40.. Train loss: 0.008.. Validation loss: 0.230.. Validation accuracy: 0.915.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.042.. Validation loss: 0.291.. Validation accuracy: 0.902.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.009.. Validation loss: 0.221.. Validation accuracy: 0.915.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.009.. Validation loss: 0.215.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.012.. Validation loss: 0.217.. Validation accuracy: 0.915.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.010.. Validation loss: 0.220.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.011.. Validation loss: 0.218.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.010.. Validation loss: 0.225.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.013.. Validation loss: 0.213.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.011.. Validation loss: 0.215.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.010.. Validation loss: 0.218.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.009.. Validation loss: 0.207.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.010.. Validation loss: 0.207.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.009.. Validation loss: 0.215.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.009.. Validation loss: 0.243.. Validation accuracy: 0.912.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.006.. Validation loss: 0.233.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.007.. Validation loss: 0.214.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.007.. Validation loss: 0.230.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.010.. Validation loss: 0.225.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.010.. Validation loss: 0.226.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.007.. Validation loss: 0.226.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.010.. Validation loss: 0.228.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.009.. Validation loss: 0.212.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.007.. Validation loss: 0.210.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.010.. Validation loss: 0.214.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.012.. Validation loss: 0.217.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.008.. Validation loss: 0.261.. Validation accuracy: 0.909.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.008.. Validation loss: 0.216.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.007.. Validation loss: 0.245.. Validation accuracy: 0.915.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.007.. Validation loss: 0.256.. Validation accuracy: 0.911.. LR : [0.034999999999999996]\n",
      "Epoch 11/40.. Train loss: 0.010.. Validation loss: 0.225.. Validation accuracy: 0.914.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.055.. Validation loss: 0.298.. Validation accuracy: 0.895.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.009.. Validation loss: 0.226.. Validation accuracy: 0.915.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.007.. Validation loss: 0.217.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.011.. Validation loss: 0.227.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.009.. Validation loss: 0.211.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.007.. Validation loss: 0.226.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.011.. Validation loss: 0.199.. Validation accuracy: 0.923.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.012.. Validation loss: 0.232.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.007.. Validation loss: 0.224.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.010.. Validation loss: 0.203.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.008.. Validation loss: 0.213.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.009.. Validation loss: 0.212.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.008.. Validation loss: 0.210.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.007.. Validation loss: 0.233.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.009.. Validation loss: 0.225.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.009.. Validation loss: 0.243.. Validation accuracy: 0.915.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.009.. Validation loss: 0.219.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.013.. Validation loss: 0.221.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.010.. Validation loss: 0.211.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.007.. Validation loss: 0.204.. Validation accuracy: 0.923.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.924.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.008.. Validation loss: 0.201.. Validation accuracy: 0.923.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.009.. Validation loss: 0.235.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.007.. Validation loss: 0.226.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.007.. Validation loss: 0.214.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.007.. Validation loss: 0.222.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.010.. Validation loss: 0.223.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.008.. Validation loss: 0.222.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.009.. Validation loss: 0.212.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 12/40.. Train loss: 0.010.. Validation loss: 0.241.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.045.. Validation loss: 0.242.. Validation accuracy: 0.912.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.010.. Validation loss: 0.235.. Validation accuracy: 0.912.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.010.. Validation loss: 0.233.. Validation accuracy: 0.914.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.011.. Validation loss: 0.235.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.010.. Validation loss: 0.211.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.010.. Validation loss: 0.237.. Validation accuracy: 0.915.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.006.. Validation loss: 0.221.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.011.. Validation loss: 0.233.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.006.. Validation loss: 0.217.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.007.. Validation loss: 0.226.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.008.. Validation loss: 0.209.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.010.. Validation loss: 0.227.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.009.. Validation loss: 0.216.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.008.. Validation loss: 0.229.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.008.. Validation loss: 0.225.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.009.. Validation loss: 0.234.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.007.. Validation loss: 0.199.. Validation accuracy: 0.925.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.010.. Validation loss: 0.214.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.007.. Validation loss: 0.199.. Validation accuracy: 0.925.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.009.. Validation loss: 0.222.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.008.. Validation loss: 0.206.. Validation accuracy: 0.924.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.009.. Validation loss: 0.215.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.013.. Validation loss: 0.222.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.010.. Validation loss: 0.238.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.009.. Validation loss: 0.208.. Validation accuracy: 0.924.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.009.. Validation loss: 0.205.. Validation accuracy: 0.923.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.009.. Validation loss: 0.231.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.008.. Validation loss: 0.247.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.010.. Validation loss: 0.228.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 13/40.. Train loss: 0.010.. Validation loss: 0.217.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.051.. Validation loss: 0.305.. Validation accuracy: 0.898.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.009.. Validation loss: 0.214.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.013.. Validation loss: 0.221.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.009.. Validation loss: 0.214.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.008.. Validation loss: 0.227.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.008.. Validation loss: 0.229.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.009.. Validation loss: 0.207.. Validation accuracy: 0.923.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.011.. Validation loss: 0.207.. Validation accuracy: 0.923.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.010.. Validation loss: 0.240.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.007.. Validation loss: 0.209.. Validation accuracy: 0.924.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.007.. Validation loss: 0.210.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.009.. Validation loss: 0.215.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.007.. Validation loss: 0.232.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.009.. Validation loss: 0.214.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.009.. Validation loss: 0.202.. Validation accuracy: 0.923.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.008.. Validation loss: 0.212.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.010.. Validation loss: 0.203.. Validation accuracy: 0.926.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.008.. Validation loss: 0.233.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.008.. Validation loss: 0.240.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.008.. Validation loss: 0.204.. Validation accuracy: 0.924.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.007.. Validation loss: 0.211.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.006.. Validation loss: 0.199.. Validation accuracy: 0.924.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.010.. Validation loss: 0.215.. Validation accuracy: 0.923.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.008.. Validation loss: 0.240.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.007.. Validation loss: 0.219.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.008.. Validation loss: 0.213.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.008.. Validation loss: 0.219.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.009.. Validation loss: 0.227.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.008.. Validation loss: 0.250.. Validation accuracy: 0.915.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.011.. Validation loss: 0.201.. Validation accuracy: 0.925.. LR : [0.034999999999999996]\n",
      "Epoch 14/40.. Train loss: 0.008.. Validation loss: 0.218.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.052.. Validation loss: 0.290.. Validation accuracy: 0.901.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.010.. Validation loss: 0.214.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.010.. Validation loss: 0.214.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.010.. Validation loss: 0.204.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.010.. Validation loss: 0.217.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.009.. Validation loss: 0.215.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.009.. Validation loss: 0.234.. Validation accuracy: 0.915.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.010.. Validation loss: 0.214.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.008.. Validation loss: 0.232.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.008.. Validation loss: 0.216.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.009.. Validation loss: 0.221.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.011.. Validation loss: 0.232.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.008.. Validation loss: 0.238.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.011.. Validation loss: 0.220.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.008.. Validation loss: 0.226.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.007.. Validation loss: 0.216.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.010.. Validation loss: 0.229.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.008.. Validation loss: 0.227.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.007.. Validation loss: 0.229.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.007.. Validation loss: 0.241.. Validation accuracy: 0.917.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.009.. Validation loss: 0.222.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.009.. Validation loss: 0.235.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.009.. Validation loss: 0.226.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.927.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.009.. Validation loss: 0.220.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.008.. Validation loss: 0.220.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.009.. Validation loss: 0.213.. Validation accuracy: 0.924.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.007.. Validation loss: 0.218.. Validation accuracy: 0.923.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.010.. Validation loss: 0.224.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 15/40.. Train loss: 0.007.. Validation loss: 0.199.. Validation accuracy: 0.925.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.046.. Validation loss: 0.221.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.010.. Validation loss: 0.195.. Validation accuracy: 0.926.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.009.. Validation loss: 0.192.. Validation accuracy: 0.927.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.009.. Validation loss: 0.229.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.010.. Validation loss: 0.220.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.008.. Validation loss: 0.218.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.008.. Validation loss: 0.240.. Validation accuracy: 0.916.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.007.. Validation loss: 0.236.. Validation accuracy: 0.918.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.011.. Validation loss: 0.208.. Validation accuracy: 0.925.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.007.. Validation loss: 0.214.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.009.. Validation loss: 0.217.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.009.. Validation loss: 0.208.. Validation accuracy: 0.924.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.010.. Validation loss: 0.205.. Validation accuracy: 0.924.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.008.. Validation loss: 0.208.. Validation accuracy: 0.924.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.011.. Validation loss: 0.221.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.010.. Validation loss: 0.213.. Validation accuracy: 0.923.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.010.. Validation loss: 0.206.. Validation accuracy: 0.925.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.011.. Validation loss: 0.245.. Validation accuracy: 0.912.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.012.. Validation loss: 0.202.. Validation accuracy: 0.925.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.009.. Validation loss: 0.228.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.013.. Validation loss: 0.224.. Validation accuracy: 0.922.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.011.. Validation loss: 0.231.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.012.. Validation loss: 0.204.. Validation accuracy: 0.926.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.009.. Validation loss: 0.240.. Validation accuracy: 0.919.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.008.. Validation loss: 0.214.. Validation accuracy: 0.924.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.012.. Validation loss: 0.213.. Validation accuracy: 0.923.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.011.. Validation loss: 0.229.. Validation accuracy: 0.920.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.005.. Validation loss: 0.218.. Validation accuracy: 0.923.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.009.. Validation loss: 0.207.. Validation accuracy: 0.926.. LR : [0.034999999999999996]\n",
      "Epoch 16/40.. Train loss: 0.007.. Validation loss: 0.222.. Validation accuracy: 0.921.. LR : [0.034999999999999996]\n",
      "Epoch 17/40.. Train loss: 0.041.. Validation loss: 0.217.. Validation accuracy: 0.925.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.010.. Validation loss: 0.205.. Validation accuracy: 0.926.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.010.. Validation loss: 0.208.. Validation accuracy: 0.925.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.010.. Validation loss: 0.209.. Validation accuracy: 0.925.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.007.. Validation loss: 0.209.. Validation accuracy: 0.925.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.010.. Validation loss: 0.205.. Validation accuracy: 0.926.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.010.. Validation loss: 0.219.. Validation accuracy: 0.923.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.015.. Validation loss: 0.215.. Validation accuracy: 0.924.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.009.. Validation loss: 0.217.. Validation accuracy: 0.923.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.009.. Validation loss: 0.219.. Validation accuracy: 0.922.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.008.. Validation loss: 0.214.. Validation accuracy: 0.923.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.008.. Validation loss: 0.229.. Validation accuracy: 0.920.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.007.. Validation loss: 0.217.. Validation accuracy: 0.923.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.008.. Validation loss: 0.210.. Validation accuracy: 0.926.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.009.. Validation loss: 0.222.. Validation accuracy: 0.922.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.009.. Validation loss: 0.211.. Validation accuracy: 0.925.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.010.. Validation loss: 0.214.. Validation accuracy: 0.924.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.007.. Validation loss: 0.213.. Validation accuracy: 0.923.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.007.. Validation loss: 0.217.. Validation accuracy: 0.924.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.008.. Validation loss: 0.215.. Validation accuracy: 0.924.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.009.. Validation loss: 0.208.. Validation accuracy: 0.925.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.005.. Validation loss: 0.213.. Validation accuracy: 0.924.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.008.. Validation loss: 0.216.. Validation accuracy: 0.924.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.007.. Validation loss: 0.213.. Validation accuracy: 0.925.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.009.. Validation loss: 0.202.. Validation accuracy: 0.926.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.007.. Validation loss: 0.206.. Validation accuracy: 0.926.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.007.. Validation loss: 0.209.. Validation accuracy: 0.926.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.009.. Validation loss: 0.215.. Validation accuracy: 0.924.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.007.. Validation loss: 0.219.. Validation accuracy: 0.923.. LR : [0.0042875]\n",
      "Epoch 17/40.. Train loss: 0.010.. Validation loss: 0.213.. Validation accuracy: 0.924.. LR : [0.0042875]\n",
      "Epoch 18/40.. Train loss: 0.053.. Validation loss: 0.265.. Validation accuracy: 0.909.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.008.. Validation loss: 0.219.. Validation accuracy: 0.921.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.009.. Validation loss: 0.199.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.007.. Validation loss: 0.210.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.007.. Validation loss: 0.203.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.011.. Validation loss: 0.206.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.010.. Validation loss: 0.200.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.009.. Validation loss: 0.206.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.007.. Validation loss: 0.215.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.008.. Validation loss: 0.201.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.009.. Validation loss: 0.209.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.008.. Validation loss: 0.207.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.009.. Validation loss: 0.207.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.008.. Validation loss: 0.214.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.010.. Validation loss: 0.201.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.007.. Validation loss: 0.207.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.005.. Validation loss: 0.213.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.009.. Validation loss: 0.211.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.012.. Validation loss: 0.206.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.007.. Validation loss: 0.206.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.008.. Validation loss: 0.221.. Validation accuracy: 0.923.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.008.. Validation loss: 0.219.. Validation accuracy: 0.923.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.010.. Validation loss: 0.203.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.008.. Validation loss: 0.221.. Validation accuracy: 0.923.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.007.. Validation loss: 0.212.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.006.. Validation loss: 0.223.. Validation accuracy: 0.923.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.007.. Validation loss: 0.208.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.008.. Validation loss: 0.209.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.006.. Validation loss: 0.205.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 18/40.. Train loss: 0.006.. Validation loss: 0.214.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.056.. Validation loss: 0.214.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.007.. Validation loss: 0.210.. Validation accuracy: 0.923.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.010.. Validation loss: 0.214.. Validation accuracy: 0.922.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.011.. Validation loss: 0.205.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.011.. Validation loss: 0.202.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.010.. Validation loss: 0.198.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.007.. Validation loss: 0.207.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.011.. Validation loss: 0.211.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.011.. Validation loss: 0.222.. Validation accuracy: 0.922.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.007.. Validation loss: 0.207.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.008.. Validation loss: 0.206.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.007.. Validation loss: 0.203.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.010.. Validation loss: 0.206.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.009.. Validation loss: 0.196.. Validation accuracy: 0.929.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.008.. Validation loss: 0.205.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.012.. Validation loss: 0.208.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.011.. Validation loss: 0.209.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.009.. Validation loss: 0.205.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.008.. Validation loss: 0.202.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.009.. Validation loss: 0.212.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.010.. Validation loss: 0.217.. Validation accuracy: 0.922.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.006.. Validation loss: 0.223.. Validation accuracy: 0.922.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.008.. Validation loss: 0.208.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.007.. Validation loss: 0.208.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.009.. Validation loss: 0.211.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.007.. Validation loss: 0.212.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.011.. Validation loss: 0.213.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.010.. Validation loss: 0.211.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 19/40.. Train loss: 0.009.. Validation loss: 0.208.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.048.. Validation loss: 0.225.. Validation accuracy: 0.921.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.007.. Validation loss: 0.198.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.011.. Validation loss: 0.205.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.006.. Validation loss: 0.215.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.008.. Validation loss: 0.217.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.011.. Validation loss: 0.211.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.010.. Validation loss: 0.206.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.008.. Validation loss: 0.215.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.008.. Validation loss: 0.215.. Validation accuracy: 0.923.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.005.. Validation loss: 0.214.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.008.. Validation loss: 0.209.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.007.. Validation loss: 0.211.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.012.. Validation loss: 0.211.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.008.. Validation loss: 0.215.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.010.. Validation loss: 0.207.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.007.. Validation loss: 0.217.. Validation accuracy: 0.923.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.010.. Validation loss: 0.210.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.009.. Validation loss: 0.209.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.008.. Validation loss: 0.204.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.007.. Validation loss: 0.222.. Validation accuracy: 0.923.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.009.. Validation loss: 0.207.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.010.. Validation loss: 0.221.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.009.. Validation loss: 0.219.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.009.. Validation loss: 0.218.. Validation accuracy: 0.923.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.005.. Validation loss: 0.208.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.008.. Validation loss: 0.206.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.008.. Validation loss: 0.211.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.009.. Validation loss: 0.206.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.005.. Validation loss: 0.209.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.009.. Validation loss: 0.201.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 20/40.. Train loss: 0.011.. Validation loss: 0.197.. Validation accuracy: 0.929.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.052.. Validation loss: 0.240.. Validation accuracy: 0.917.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.010.. Validation loss: 0.204.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.010.. Validation loss: 0.208.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.008.. Validation loss: 0.215.. Validation accuracy: 0.922.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.007.. Validation loss: 0.201.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.011.. Validation loss: 0.216.. Validation accuracy: 0.922.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.009.. Validation loss: 0.204.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.008.. Validation loss: 0.206.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.007.. Validation loss: 0.198.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.007.. Validation loss: 0.207.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.008.. Validation loss: 0.213.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.006.. Validation loss: 0.200.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.009.. Validation loss: 0.208.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.010.. Validation loss: 0.208.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.009.. Validation loss: 0.203.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.009.. Validation loss: 0.199.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.006.. Validation loss: 0.221.. Validation accuracy: 0.923.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.007.. Validation loss: 0.214.. Validation accuracy: 0.923.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.007.. Validation loss: 0.208.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.007.. Validation loss: 0.205.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.009.. Validation loss: 0.224.. Validation accuracy: 0.922.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.007.. Validation loss: 0.215.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.007.. Validation loss: 0.209.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.007.. Validation loss: 0.207.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.008.. Validation loss: 0.208.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.010.. Validation loss: 0.207.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.010.. Validation loss: 0.207.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.010.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.012.. Validation loss: 0.208.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 21/40.. Train loss: 0.008.. Validation loss: 0.196.. Validation accuracy: 0.929.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.043.. Validation loss: 0.261.. Validation accuracy: 0.913.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.011.. Validation loss: 0.210.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.007.. Validation loss: 0.199.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.007.. Validation loss: 0.203.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.008.. Validation loss: 0.207.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.007.. Validation loss: 0.211.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.010.. Validation loss: 0.207.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.006.. Validation loss: 0.211.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.008.. Validation loss: 0.196.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.010.. Validation loss: 0.199.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.006.. Validation loss: 0.208.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.009.. Validation loss: 0.209.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.009.. Validation loss: 0.195.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.008.. Validation loss: 0.197.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.010.. Validation loss: 0.205.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.010.. Validation loss: 0.212.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.009.. Validation loss: 0.211.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.011.. Validation loss: 0.200.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.008.. Validation loss: 0.209.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.005.. Validation loss: 0.213.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.007.. Validation loss: 0.202.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.007.. Validation loss: 0.207.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.010.. Validation loss: 0.212.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.007.. Validation loss: 0.192.. Validation accuracy: 0.931.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.008.. Validation loss: 0.204.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.008.. Validation loss: 0.212.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.009.. Validation loss: 0.212.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 22/40.. Train loss: 0.012.. Validation loss: 0.208.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.049.. Validation loss: 0.272.. Validation accuracy: 0.913.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.007.. Validation loss: 0.205.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.007.. Validation loss: 0.206.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.010.. Validation loss: 0.202.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.009.. Validation loss: 0.209.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.005.. Validation loss: 0.208.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.008.. Validation loss: 0.210.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.008.. Validation loss: 0.213.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.010.. Validation loss: 0.210.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.010.. Validation loss: 0.194.. Validation accuracy: 0.929.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.007.. Validation loss: 0.220.. Validation accuracy: 0.923.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.007.. Validation loss: 0.211.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.012.. Validation loss: 0.210.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.010.. Validation loss: 0.204.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.007.. Validation loss: 0.200.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.008.. Validation loss: 0.207.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.009.. Validation loss: 0.223.. Validation accuracy: 0.923.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.010.. Validation loss: 0.206.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.009.. Validation loss: 0.211.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.010.. Validation loss: 0.206.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.009.. Validation loss: 0.202.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.009.. Validation loss: 0.214.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.008.. Validation loss: 0.202.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.009.. Validation loss: 0.195.. Validation accuracy: 0.930.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.007.. Validation loss: 0.212.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.010.. Validation loss: 0.194.. Validation accuracy: 0.929.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.008.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.010.. Validation loss: 0.214.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.011.. Validation loss: 0.206.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 23/40.. Train loss: 0.008.. Validation loss: 0.214.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.047.. Validation loss: 0.219.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.009.. Validation loss: 0.209.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.010.. Validation loss: 0.208.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.008.. Validation loss: 0.208.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.007.. Validation loss: 0.211.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.007.. Validation loss: 0.206.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.007.. Validation loss: 0.215.. Validation accuracy: 0.925.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.007.. Validation loss: 0.208.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.010.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.009.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.008.. Validation loss: 0.210.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.005.. Validation loss: 0.201.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.008.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.008.. Validation loss: 0.204.. Validation accuracy: 0.929.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.008.. Validation loss: 0.194.. Validation accuracy: 0.931.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.009.. Validation loss: 0.195.. Validation accuracy: 0.931.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.009.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.010.. Validation loss: 0.207.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.007.. Validation loss: 0.207.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.010.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.009.. Validation loss: 0.201.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.007.. Validation loss: 0.197.. Validation accuracy: 0.929.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.012.. Validation loss: 0.208.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.008.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.011.. Validation loss: 0.212.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.008.. Validation loss: 0.210.. Validation accuracy: 0.927.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.011.. Validation loss: 0.210.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.005.. Validation loss: 0.215.. Validation accuracy: 0.926.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.009.. Validation loss: 0.214.. Validation accuracy: 0.924.. LR : [0.012249999999999999]\n",
      "Epoch 24/40.. Train loss: 0.008.. Validation loss: 0.204.. Validation accuracy: 0.928.. LR : [0.012249999999999999]\n",
      "Epoch 25/40.. Train loss: 0.052.. Validation loss: 0.213.. Validation accuracy: 0.926.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.009.. Validation loss: 0.206.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.008.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.006.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.007.. Validation loss: 0.202.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.006.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.009.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.009.. Validation loss: 0.209.. Validation accuracy: 0.927.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.009.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.008.. Validation loss: 0.207.. Validation accuracy: 0.927.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.006.. Validation loss: 0.208.. Validation accuracy: 0.927.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.009.. Validation loss: 0.209.. Validation accuracy: 0.927.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.009.. Validation loss: 0.207.. Validation accuracy: 0.927.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.009.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.010.. Validation loss: 0.201.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.010.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.008.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.006.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.007.. Validation loss: 0.195.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.005.. Validation loss: 0.205.. Validation accuracy: 0.927.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.008.. Validation loss: 0.207.. Validation accuracy: 0.927.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.007.. Validation loss: 0.205.. Validation accuracy: 0.927.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.011.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.008.. Validation loss: 0.207.. Validation accuracy: 0.927.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.008.. Validation loss: 0.210.. Validation accuracy: 0.927.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.008.. Validation loss: 0.206.. Validation accuracy: 0.927.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.011.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.009.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.009.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 25/40.. Train loss: 0.008.. Validation loss: 0.207.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 26/40.. Train loss: 0.061.. Validation loss: 0.224.. Validation accuracy: 0.924.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.006.. Validation loss: 0.218.. Validation accuracy: 0.925.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.007.. Validation loss: 0.207.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.007.. Validation loss: 0.207.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.010.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.009.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.011.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.005.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.008.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.008.. Validation loss: 0.203.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.010.. Validation loss: 0.200.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.010.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.007.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.011.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.010.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.008.. Validation loss: 0.194.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.006.. Validation loss: 0.195.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.010.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.010.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.010.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.011.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.008.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.010.. Validation loss: 0.197.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.009.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.006.. Validation loss: 0.206.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.007.. Validation loss: 0.209.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.007.. Validation loss: 0.206.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.009.. Validation loss: 0.204.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 26/40.. Train loss: 0.008.. Validation loss: 0.203.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.046.. Validation loss: 0.207.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.008.. Validation loss: 0.207.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.012.. Validation loss: 0.208.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.011.. Validation loss: 0.209.. Validation accuracy: 0.926.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.009.. Validation loss: 0.204.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.009.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.011.. Validation loss: 0.204.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.007.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.009.. Validation loss: 0.201.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.008.. Validation loss: 0.205.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.007.. Validation loss: 0.209.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.007.. Validation loss: 0.202.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.010.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.011.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.011.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.007.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.006.. Validation loss: 0.200.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.012.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.008.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.010.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.009.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.009.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.007.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.011.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.008.. Validation loss: 0.210.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.008.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.009.. Validation loss: 0.203.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.010.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.010.. Validation loss: 0.195.. Validation accuracy: 0.931.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.009.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 27/40.. Train loss: 0.007.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.049.. Validation loss: 0.204.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.008.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.009.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.008.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.012.. Validation loss: 0.202.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.008.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.009.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.007.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.012.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.008.. Validation loss: 0.197.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.005.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.011.. Validation loss: 0.203.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.009.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.007.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.009.. Validation loss: 0.204.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.009.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.010.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.007.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.007.. Validation loss: 0.195.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.007.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.005.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.005.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.010.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.007.. Validation loss: 0.203.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.008.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 28/40.. Train loss: 0.010.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.053.. Validation loss: 0.210.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.010.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.010.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.006.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.008.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.007.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.008.. Validation loss: 0.203.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.007.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.010.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.011.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.009.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.010.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.008.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.009.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.009.. Validation loss: 0.195.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.009.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.005.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.010.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.006.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.007.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.009.. Validation loss: 0.206.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.007.. Validation loss: 0.203.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.011.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.009.. Validation loss: 0.203.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.006.. Validation loss: 0.208.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.009.. Validation loss: 0.211.. Validation accuracy: 0.926.. LR : [0.0042875]\n",
      "Epoch 29/40.. Train loss: 0.009.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.056.. Validation loss: 0.205.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.007.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.010.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.006.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.006.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.008.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.010.. Validation loss: 0.196.. Validation accuracy: 0.931.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.011.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.008.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.007.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.010.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.009.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.006.. Validation loss: 0.202.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.009.. Validation loss: 0.202.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.010.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.009.. Validation loss: 0.197.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.007.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.011.. Validation loss: 0.197.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.006.. Validation loss: 0.201.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.008.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.009.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.011.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.009.. Validation loss: 0.195.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.007.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.005.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.011.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 30/40.. Train loss: 0.009.. Validation loss: 0.204.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.049.. Validation loss: 0.237.. Validation accuracy: 0.921.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.008.. Validation loss: 0.212.. Validation accuracy: 0.926.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.007.. Validation loss: 0.194.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.005.. Validation loss: 0.194.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.008.. Validation loss: 0.194.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.010.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.008.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.007.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.007.. Validation loss: 0.205.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.007.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.007.. Validation loss: 0.205.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.013.. Validation loss: 0.202.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.010.. Validation loss: 0.202.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.010.. Validation loss: 0.207.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.006.. Validation loss: 0.206.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.008.. Validation loss: 0.206.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.006.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.008.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.008.. Validation loss: 0.195.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.008.. Validation loss: 0.196.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.008.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.007.. Validation loss: 0.197.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.011.. Validation loss: 0.193.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.008.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.005.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.007.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.007.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.007.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 31/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.071.. Validation loss: 0.225.. Validation accuracy: 0.924.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.010.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.010.. Validation loss: 0.197.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.007.. Validation loss: 0.195.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.007.. Validation loss: 0.192.. Validation accuracy: 0.931.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.007.. Validation loss: 0.196.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.011.. Validation loss: 0.205.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.010.. Validation loss: 0.206.. Validation accuracy: 0.927.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.008.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.007.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.009.. Validation loss: 0.195.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.007.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.009.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.007.. Validation loss: 0.196.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.007.. Validation loss: 0.197.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.008.. Validation loss: 0.194.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.010.. Validation loss: 0.197.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.005.. Validation loss: 0.196.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.010.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.009.. Validation loss: 0.195.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.007.. Validation loss: 0.192.. Validation accuracy: 0.931.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.008.. Validation loss: 0.195.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.009.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.012.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.010.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0042875]\n",
      "Epoch 32/40.. Train loss: 0.006.. Validation loss: 0.203.. Validation accuracy: 0.929.. LR : [0.0042875]\n",
      "Epoch 33/40.. Train loss: 0.049.. Validation loss: 0.211.. Validation accuracy: 0.927.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.005.. Validation loss: 0.209.. Validation accuracy: 0.927.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.007.. Validation loss: 0.206.. Validation accuracy: 0.928.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.006.. Validation loss: 0.207.. Validation accuracy: 0.928.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.010.. Validation loss: 0.206.. Validation accuracy: 0.928.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.011.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.010.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.007.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.011.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.006.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.009.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.930.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.007.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.006.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.005.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.006.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.005.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.007.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.005.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.008.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.930.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.009.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.006.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.007.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.010.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.007.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.007.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.007.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0005252187499999999]\n",
      "Epoch 33/40.. Train loss: 0.007.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0005252187499999999]\n",
      "Epoch 34/40.. Train loss: 0.049.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.008.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.006.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.009.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.008.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.005.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.007.. Validation loss: 0.203.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.009.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.007.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.009.. Validation loss: 0.203.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.012.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.004.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.008.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.009.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.010.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.008.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.009.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.009.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.009.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.010.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.010.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.005.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.010.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.010.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.007.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.007.. Validation loss: 0.200.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.007.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 34/40.. Train loss: 0.009.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.054.. Validation loss: 0.209.. Validation accuracy: 0.927.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.008.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.007.. Validation loss: 0.204.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.008.. Validation loss: 0.204.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.006.. Validation loss: 0.204.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.008.. Validation loss: 0.204.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.007.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.008.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.009.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.007.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.007.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.007.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.010.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.010.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.008.. Validation loss: 0.195.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.009.. Validation loss: 0.195.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.010.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.009.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.007.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.006.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.010.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 35/40.. Train loss: 0.009.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.054.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.006.. Validation loss: 0.203.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.007.. Validation loss: 0.200.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.008.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.006.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.007.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.009.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.007.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.007.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.008.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.007.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.004.. Validation loss: 0.200.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.008.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.007.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.010.. Validation loss: 0.203.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.010.. Validation loss: 0.200.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.007.. Validation loss: 0.201.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.007.. Validation loss: 0.200.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.008.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.009.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.007.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.007.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.010.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.008.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.009.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.010.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 36/40.. Train loss: 0.010.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.046.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.008.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.006.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.009.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.009.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.007.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.008.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.007.. Validation loss: 0.200.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.007.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.008.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.013.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.012.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.007.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.009.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.011.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.007.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.006.. Validation loss: 0.201.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.007.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.011.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.007.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.013.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.008.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.007.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.009.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.008.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 37/40.. Train loss: 0.012.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.042.. Validation loss: 0.208.. Validation accuracy: 0.927.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.007.. Validation loss: 0.205.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.008.. Validation loss: 0.204.. Validation accuracy: 0.928.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.009.. Validation loss: 0.203.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.009.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.009.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.008.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.006.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.007.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.005.. Validation loss: 0.195.. Validation accuracy: 0.931.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.007.. Validation loss: 0.195.. Validation accuracy: 0.931.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.009.. Validation loss: 0.194.. Validation accuracy: 0.931.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.006.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.010.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.009.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.009.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.008.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.007.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.008.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.009.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.009.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.006.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.011.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.008.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.007.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.008.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.007.. Validation loss: 0.201.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 38/40.. Train loss: 0.010.. Validation loss: 0.200.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.039.. Validation loss: 0.205.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.008.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.007.. Validation loss: 0.195.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.007.. Validation loss: 0.195.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.008.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.005.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.005.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.007.. Validation loss: 0.196.. Validation accuracy: 0.931.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.008.. Validation loss: 0.195.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.007.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.007.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.008.. Validation loss: 0.200.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.007.. Validation loss: 0.200.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.007.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.009.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.011.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.013.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.007.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.007.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.010.. Validation loss: 0.199.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.010.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.011.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.010.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.012.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.010.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 39/40.. Train loss: 0.010.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.052.. Validation loss: 0.200.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.007.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.007.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.008.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.007.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.007.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.008.. Validation loss: 0.197.. Validation accuracy: 0.931.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.008.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.005.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.008.. Validation loss: 0.201.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.010.. Validation loss: 0.200.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.007.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.007.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.006.. Validation loss: 0.202.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.009.. Validation loss: 0.203.. Validation accuracy: 0.929.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.007.. Validation loss: 0.199.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.009.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.008.. Validation loss: 0.195.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.010.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.010.. Validation loss: 0.196.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.007.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.006.. Validation loss: 0.196.. Validation accuracy: 0.931.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.010.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.009.. Validation loss: 0.194.. Validation accuracy: 0.931.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.008.. Validation loss: 0.194.. Validation accuracy: 0.931.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.008.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.006.. Validation loss: 0.198.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.007.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n",
      "Epoch 40/40.. Train loss: 0.008.. Validation loss: 0.197.. Validation accuracy: 0.930.. LR : [0.0015006249999999998]\n"
     ]
    }
   ],
   "source": [
    "#TRAINING MODEL ON NEW DATASET\n",
    "\n",
    "epochs = 40 # Number of epochs\n",
    "steps = 0\n",
    "print_every = 10 \n",
    "train_losses, valid_losses,valid_acc = [], [], [] # List keeping track of losses and accuracy to plot later\n",
    "valid_loss_min = np.Inf # It will be used to save model whenever Vallidation loss decreases\n",
    "valid_acc_min = 0.0 \n",
    "\n",
    "for e in range(epochs):\n",
    "  \n",
    "  train_loss = 0 \n",
    "  model.train()\n",
    "  #train the model\n",
    "  for data, labels in train_loader:\n",
    "    steps+=1\n",
    "    # Move tensor to device('cuda' in case of GPU or 'cpu' in case of CPU)\n",
    "    data, labels = data.to(device), labels.to(device)\n",
    "    # Clearing all the previous gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Forward Pass\n",
    "    logits = model(data)\n",
    "    # Loss calculation\n",
    "    loss = criterion(logits,labels)\n",
    "    # Backward Pass\n",
    "    loss.backward()\n",
    "    # Update the parameters\n",
    "    optimizer.step()\n",
    "    # Updating the losses list\n",
    "    train_loss += loss.item()\n",
    "\n",
    "    # Evaluating after specific amount of steps\n",
    "    if steps % print_every == 0:\n",
    "      valid_loss = 0\n",
    "      accuracy = 0\n",
    "      # Setting Model to Evaluation Mode\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        # Getting Validation loss\n",
    "        for data, labels in valid_loader:\n",
    "          data, labels = data.to(device), labels.to(device)\n",
    "          logits = model(data)\n",
    "          batch_loss = criterion(logits,labels)\n",
    "          valid_loss += batch_loss.item()\n",
    "          \n",
    "          # Calculating Accuracy\n",
    "          output = F.softmax(logits,dim=1)\n",
    "          top_p,top_class = output.topk(1,dim = 1)\n",
    "          equals = top_class == labels.view(*top_class.shape)\n",
    "          accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "      # Printing stats    \n",
    "      print(f\"Epoch {e+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {train_loss/print_every:.3f}.. \"\n",
    "                  f\"Validation loss: {valid_loss/len(valid_loader):.3f}.. \"\n",
    "                  f\"Validation accuracy: {accuracy/len(valid_loader):.3f}.. \"\n",
    "                  f\"LR : {scheduler.get_lr():}\"\n",
    "                  )\n",
    "      valid_loss = valid_loss/len(valid_loader)\n",
    "      train_losses.append(train_loss/print_every)\n",
    "      valid_losses.append(valid_loss)\n",
    "      valid_acc.append(accuracy/len(valid_loader))\n",
    "      \n",
    "      # Checking if Validation loss decreased\n",
    "      if valid_loss <= valid_loss_min:\n",
    "        \n",
    "        # if decreased, it will save the model\n",
    "        print('valid loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "      \n",
    "    train_loss = 0    \n",
    "  # Scheduler performing a step to change learning rate of Optimizer    \n",
    "  scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-8BMVuYY624f",
    "outputId": "a6feef01-daeb-4e12-dccb-f6e27e0974c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.226101\n",
      "\n",
      "Test Accuracy (Overall): 91.15% (151500/166215)\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), home+'/FIGA/model_FIGA_trained.pt')\n",
    "measure_accuracy(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMy5B8Orn5Bv"
   },
   "source": [
    "###Testing Model trained with perturbated dataset on new n and e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nlmo12t1ho0P",
    "outputId": "cd28a06b-f9b7-4caa-d00d-c1809abc24fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 20 0.2 \n",
      "\n",
      "Test Loss: 1.528593\n",
      "\n",
      "Test Accuracy (Overall): 65.27% (28935/44330)\n",
      "Test Loss: 0.251680\n",
      "\n",
      "Test Accuracy (Overall): 90.18% (39975/44330)\n",
      "---------------\n",
      "\n",
      " 30 0.4 \n",
      "\n",
      "Test Loss: 1.731688\n",
      "\n",
      "Test Accuracy (Overall): 63.43% (28120/44330)\n",
      "Test Loss: 0.192264\n",
      "\n",
      "Test Accuracy (Overall): 92.82% (41145/44330)\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "#Creating a new test dataset with original perturbated values of original data with new n and e (n and e not seen by model)\n",
    "for n_f, e in [(20,0.2),(30,0.4)]:\n",
    "  print(\"\\n\",n_f,e,\"\\n\")\n",
    "  test_data_new = test_data.drop('phishing',axis = 1)\n",
    "  test_data_labels = test_data['phishing']\n",
    "  f, d, scaler = featureImportance(test_data_new, test_data_labels , n_f)\n",
    "  for i in range(test_data_new.shape[0]):\n",
    "    if test_data_labels.iloc[i]==1:\n",
    "      test_data_new.iloc[i,:] = perturbation( f, d, test_data_new.iloc[i,:].to_numpy(), scaler, e).ravel()\n",
    "    \n",
    "  #making a testloader out of it\n",
    "  train_target = torch.from_numpy(test_data_labels.values)\n",
    "  train = torch.from_numpy(test_data_new.values).float()\n",
    "  train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "  test_loader_new = data_utils.DataLoader(dataset = train_tensor, batch_size = 128, shuffle = True)\n",
    "    \n",
    "  #Testing perturbation with new n and e on normal model\n",
    "  model.load_state_dict(torch.load(home+\"/FIGA/model_normal.pt\",map_location=torch.device('cpu')))\n",
    "  measure_accuracy(test_loader_new)\n",
    "\n",
    "  #Testing perturbation with new n and e on Adversarially trained model\n",
    "  model.load_state_dict(torch.load(home+\"/FIGA/model_FIGA_trained.pt\"))\n",
    "  measure_accuracy(test_loader_new)\n",
    "  print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk1bsvmFC7bG"
   },
   "source": [
    "##Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NN-83UXwFJed"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AE70zM9FD25v"
   },
   "outputs": [],
   "source": [
    "a = np.array([ 2.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,\n",
    "        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,\n",
    "        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,\n",
    "        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,\n",
    "        0.00000e+00,  1.00000e+00,  1.40000e+01,  2.00000e+00,\n",
    "        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,\n",
    "        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,\n",
    "        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,\n",
    "        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,\n",
    "        2.00000e+00,  1.40000e+01,  0.00000e+00,  0.00000e+00,\n",
    "       -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
    "       -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
    "       -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
    "       -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
    "       -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
    "       -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
    "       -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
    "       -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
    "       -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
    "       -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
    "       -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
    "       -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
    "       -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
    "       -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
    "        0.00000e+00,  6.91094e-01,  1.00000e+00,  1.52440e+04,\n",
    "        7.09900e+03,  2.05000e+02,  1.00000e+00,  2.00000e+00,\n",
    "        2.00000e+00,  3.58900e+03,  0.00000e+00,  0.00000e+00,\n",
    "        0.00000e+00,  0.00000e+00,  0.00000e+00])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0y1G0QqZC9G4"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear_layers = nn.Sequential(\n",
    "                        nn.Linear(111,50),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.2),\n",
    "                        nn.Linear(50,200),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.2),\n",
    "                        nn.Linear(200,100),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.4),\n",
    "                        nn.Linear(100,20),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.8),\n",
    "                        nn.Linear(20,10),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.4),\n",
    "                        nn.Linear(10,2)\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "scalerfile = home+'/FIGA/scaler.pkl'\n",
    "scaler = pickle.load(open(scalerfile, 'rb'))\n",
    "modelfile = home+'/FIGA/model_FIGA_Trained.pkl'\n",
    "model = pickle.load(open(modelfile, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Z8C61K2E7of",
    "outputId": "42a7a47b-3057-4688-9864-2f9d7086e952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RobustScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(scaler.transform(a.reshape(1,-1)).ravel())\n",
    "\n",
    "output = model(a.float().unsqueeze(0))\n",
    "_, pred = torch.max(output, 1)\n",
    "print(pred.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoxCBI5TFSn4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Ok_DJSz7nlti",
    "sk1bsvmFC7bG"
   ],
   "name": "FIGA_Model (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
